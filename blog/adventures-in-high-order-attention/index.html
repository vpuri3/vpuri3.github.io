<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing | Vedant Puri</title><meta name=keywords content><meta name=description content="A practical tour of linear attention, its bottlenecks, and why multilinear / simplex-style attention might help"><meta name=author content><link rel=canonical href=https://vpuri3.github.io/blog/adventures-in-high-order-attention/><link crossorigin=anonymous href=/assets/css/stylesheet.72a547024d24c325f3bae9b2bb62cea7a569c0893d3ff900db35fa11b5fe740d.css integrity="sha256-cqVHAk0kwyXzuumyu2LOp6VpwIk9P/kA2zX6EbX+dA0=" rel="preload stylesheet" as=style><link rel=icon href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://github.githubassets.com/favicons/favicon.png><link rel=apple-touch-icon href=https://github.com/vpuri3.png><link rel=mask-icon href=https://github.githubassets.com/favicons/favicon.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://vpuri3.github.io/blog/adventures-in-high-order-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://vpuri3.github.io/blog/adventures-in-high-order-attention/"><meta property="og:site_name" content="Vedant Puri"><meta property="og:title" content="Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing"><meta property="og:description" content="A practical tour of linear attention, its bottlenecks, and why multilinear / simplex-style attention might help"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-16T00:00:00-05:00"><meta property="article:modified_time" content="2026-02-16T00:00:00-05:00"><meta property="og:image" content="https://github.com/vpuri3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/vpuri3.png"><meta name=twitter:title content="Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing"><meta name=twitter:description content="A practical tour of linear attention, its bottlenecks, and why multilinear / simplex-style attention might help"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://vpuri3.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing","item":"https://vpuri3.github.io/blog/adventures-in-high-order-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing","name":"Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing","description":"A practical tour of linear attention, its bottlenecks, and why multilinear / simplex-style attention might help","keywords":[],"articleBody":"Beyond pairwise attention Softmax attention is an extremely expressive token-mixing primitive, but it is expensive. For a sequence of length $N$, the core interaction matrix $Q \\cdot K^\\top$ is $N \\times N$, which drives both runtime and memory. Linear transformers try to keep global context while avoiding the quadratic scaling. The catch is that the simplest linear attention formulations often lose a key ingredient that makes softmax attention work so well: token-specific routing.\nThis post has two goals. First, I want to explain why vanilla linear attention underperforms in many settings, not as a matter of “bad approximation,” but as a structural consequence of how the computation is arranged. Second, I want to sketch a set of experimental ideas I’ve been exploring around enhanced attention mechanisms. To be explicit: these are research notes. Some of these ideas are promising on paper, but they have not reliably panned out in my experiments so far. I’m writing them up anyway because the framing has been useful for me, and it may help others reason about the design space.\nOne additional constraint threads through the post: permutation equivariance. Many strong long-sequence models exploit 1D sequence structure (chunking, convolution, hierarchical pooling, scan recurrences). That is great for language, but it is brittle for unstructured grids and point sets where there is no canonical ordering and where I would like the model to be insensitive to permutations.\nPreliminaries Vanilla softmax attention (SDPA) Let $X \\in \\mathbb{R}^{N \\times C}$ be a sequence of $N$ tokens with $C$ features. We form queries/keys/values by linear projection:\n$$Q = XW^q,\\quad K = XW^k,\\quad V = XW^v$$where $W^q,W^k,W^v \\in \\mathbb{R}^{C \\times C}$. In multi-head attention we split features into $H$ heads of dimension $d = C/H$, writing\n$$Q=[Q_1,\\dots,Q_H],\\quad K=[K_1,\\dots,K_H],\\quad V=[V_1,\\dots,V_H].$$Scaled dot-product attention (SDPA) in head $h$ is\n$$Y_h = \\mathrm{SDPA}(Q_h,K_h,V_h;s) = \\mathrm{softmax}\\!\\left(\\frac{Q_hK_h^\\top}{s}\\right)V_h$$with $s \\approx \\sqrt{d}$. In vector form,\n$$y_i = \\sum_{j=1}^N \\frac{\\exp(q_i^\\top k_j)}{\\sum_{\\ell=1}^N \\exp(q_i^\\top k_\\ell)}\\cdot v_j.$$The quadratic cost comes from the pairwise interaction matrix $QK^\\top \\in \\mathbb{R}^{N\\times N}$. A useful mental model is: each query $q_i$ produces its own distribution over keys, so it can route information in a token-specific way. That “personalized routing” is what many linear-time mechanisms struggle to preserve.\nimport torch def sdpa_naive(q, k, v): \"\"\" Scaled dot-product attention. Args: q: [B, H, N, D] queries (already split across heads) k: [B, H, N, D] keys v: [B, H, N, D] values scale: float attention scale (default 1.0) Returns: y: [B, H, N, D] output \"\"\" scale = q.size(-1) ** -0.5 S = (q @ k.mT) * scale # [B, H, N, N] attn logits A = S.softmax(dim=-1) # [B, H, N, N] attn weights y = A @ v # [B, H, N, D] return y Linear attention The basic idea Linear attention replaces the softmax kernel with a feature map $\\phi(\\cdot)$ (often constrained to be positive) so that similarities become dot products in feature space. Define\n$$\\tilde{Q}=\\phi(Q),\\quad \\tilde{K}=\\phi(K).$$A typical (row-normalized) update is\n$$Y = \\mathrm{rownorm}(\\tilde Q\\tilde K^\\top)V,$$which in vector form is\n$$y_i = \\sum_{j=1}^N \\frac{(\\tilde q_i^\\top \\tilde k_j)}{\\sum_{\\ell=1}^N \\tilde q_i^\\top \\tilde k_\\ell}\\cdot v_j.$$The efficiency comes from associativity:\n$$y_i = \\frac{\\tilde q_i^\\top\\left(\\sum_{j=1}^N \\tilde k_j v_j^\\top\\right)}{\\tilde q_i^\\top\\left(\\sum_{\\ell=1}^N \\tilde k_\\ell\\right)}.$$If we define a pooled “memory” state $S=\\tilde K^\\top V \\in \\mathbb{R}^{D\\times D}$ and $z=\\tilde K^\\top\\mathbf{1}\\in\\mathbb{R}^D$, we can write $Y = \\dfrac{\\tilde Q \\cdot S}{\\tilde Q \\cdot z}$. This is linear in $N$ (up to constants) because the expensive terms are reductions over the sequence. At a systems level, linear attention is appealing: you can stream over tokens, maintain a running state, and avoid materializing $N\\times N$ matrices.\nimport torch def linear_attn(q, k, v, kernel=None): \"\"\" Linear attention: linear-time global mixing via associative state reduction. Args: q: [B, H, N, D] queries k: [B, H, N, D] keys v: [B, H, N, D] values kernel: callable feature map φ applied to q and k (e.g. elu + 1) Returns: y: [B, H, N, D] output \"\"\" if kernel is not None: q = kernel(q) k = kernel(k) state = k.mT @ v # [B, H, D, D] k_sum = k.sum(dim=2).unsqueeze(-1) # [B, H, D, 1] num = q @ state den = q @ k_sum return num / (den + 1e-6) Why vanilla linear attention often underperforms It helps to look at the structural form of the update and ignore normalization for intuition. Linear attention can be written as\n$$y_i = \\tilde q_i^\\top\\left(\\sum_{j=1}^N \\tilde k_j v_j^\\top\\right),$$i.e., $y_i = \\tilde q_i^\\top S$ where $S\\in\\mathbb{R}^{D\\times D}$.\nThe critical observation is simple: $\\tilde q_i$ changes with $i$, but the memory $S$ is shared across all tokens. Every token is querying the same pooled summary of key/value content. That makes the model efficient, but it also changes the nature of token mixing.\nToken-specific routing is weakened. In softmax attention, each query produces its own distribution over keys. In basic linear attention, each query projects against the same global state. You can still get token-dependent outputs because $\\tilde q_i$ varies, but the mechanism no longer has “choose a few values and ignore the rest” behavior in the same way.\nSmoothing is hard to avoid. The associativity trick forces compression before interaction. Fine-grained information has to survive the bottleneck $S$ to influence outputs. This tends to smear distinctions unless the feature map and projections work very hard to preserve them.\nThe bottleneck lives in feature space. Softmax attention carries $O(N^2)$ interaction capacity through an $N\\times N$ attention map. Linear attention collapses this into a feature-space quadratic form: $S \\in \\mathbb{R}^{D\\times D}$. Unless $\\phi$ is carefully chosen (e.g., kernelized approximations of $\\exp$), this can be structurally less expressive, not merely a “worse approximation.”\nThis is why the linear-transformer literature has so many “patches”: better feature maps (Performer/FAVOR+ style), gating and recurrence (RetNet/RWKV-like ideas), low-rank bottlenecks (Nyström, latent tokens), and various normalization tricks (including the practical observation that removing row-normalization and stabilizing with RMSNorm at the end can sometimes help).\nHigher-order attention: why pairwise may not be enough Softmax attention is pairwise: it scores $(q_i,k_j)$ and uses that to weight $v_j$. Many tasks can be solved with pairwise interactions plus depth, but there are reasons to explore explicit multi-way mixing.\nOn algorithmic tasks, it is natural to point at interactions that look like (digit1, digit2, carry). On PDE-like data, nonlinearities often couple multiple factors, and it is tempting to ask whether explicitly modeling multi-way interactions could reduce the burden on depth or help linear-time models recover some selectivity.\nOne concrete formalization is 2-simplex (3-way) attention, which replaces bilinear dot products with trilinear forms. I do not view this as “the answer,” but it is a clean starting point for thinking about higher-order token mixing.\n2-simplex attention (3-way interactions) Standard attention is built on the bilinear form\n$$\\langle x_1,x_2\\rangle = \\sum_{d=1}^D x_1[d]x_2[d].$$2-simplex attention generalizes this to a trilinear form\n$$\\langle x_1,x_2,x_3\\rangle = \\sum_{d=1}^D x_1[d]x_2[d]x_3[d].$$A naive 3-way attention mechanism defines a score tensor over triples. Let\n$$Q,K^1,K^2,V^1,V^2 \\in \\mathbb{R}^{N\\times D},$$and define\n$$S_{ijk}=\\sum_{d'=1}^D Q_{id'}K^1_{jd'}K^2_{kd'}.$$Then $A=\\mathrm{softmax}(S)$ (softmax over $(j,k)$) and\n$$Y_{id}=\\sum_{j,k=1}^N A_{ijk}V^1_{jd}V^2_{kd}.$$This is expressive, but it requires forming $N\\times N\\times N$ tensors, which is intractable for large $N$. So the question that keeps coming up is: can we capture some higher-order behavior without paying cubic cost?\nProposal: multilinear kernelized attention (linear-time simplex-style mixing) This is where the “research notes” part begins. The idea below is not a claim that I have a working model. It is a proposal that falls out of the same associativity trick that makes linear attention fast. It is conceptually clean, but in my experiments it has been finicky to stabilize and has not consistently beaten simpler baselines.\nThe key trick in linear attention is: avoid instantiating $N\\times N$ by collapsing into a $D\\times D$ state. We can do something similar for 2-simplex attention by dropping softmax and restructuring the computation so the triple interaction factorizes into feature-space memories.\nStart from the unnormalized 3-way update:\n$$A_{ijk}=\\sum_{d'=1}^D Q_{id'}K^1_{jd'}K^2_{kd'},$$and\n$$Y_{id}=\\sum_{j,k=1}^N A_{ijk}V^1_{jd}V^2_{kd}.$$Rearranging sums gives\n$$Y_{id}=\\sum_{d'=1}^D Q_{id'}\\left(\\sum_{j=1}^N K^1_{jd'}V^1_{jd}\\right)\\left(\\sum_{k=1}^N K^2_{kd'}V^2_{kd}\\right).$$Define feature-space memories\n$$S^1=(K^1)^\\top V^1 \\in \\mathbb{R}^{D\\times D}$$and\n$$S^2=(K^2)^\\top V^2 \\in \\mathbb{R}^{D\\times D}.$$Then the output becomes $Y = Q\\left(S^1 \\odot S^2\\right)$ where $\\odot$ is the elementwise Hadamard product. This is linear in $N$ because forming each $S^\\ell$ is a single reduction pass over tokens.\nGeneralization to L-way multilinear attention With $L$ key/value pairs\n$$\\{(K^\\ell,V^\\ell)\\}_{\\ell=1}^L,$$define\n$$S^\\ell=(K^\\ell)^\\top V^\\ell \\in \\mathbb{R}^{D\\times D}$$and compute\n$$Y = Q\\left(S^1 \\odot S^2 \\odot \\cdots \\odot S^L\\right).$$You can view this as a generalized linear-time $L$-simplicial attention.\nOne motivation I find useful: “mixture of memories” mechanisms tend to combine multiple memories additively (a router picks or mixes). This proposal combines them multiplicatively, which behaves like feature-wise gating: all memories have to agree, and features can be amplified or suppressed by products.\nIn practice, the open questions dominate:\nnormalization (row-norm vs no row-norm + RMSNorm, or explicit gating), whether RoPE-like positional structure still helps or becomes awkward under multiplicative state composition, and whether the mechanism learns something meaningfully different than “more projections + gating.” So far, my results have not been clean enough to recommend this as a drop-in replacement. The main value for me has been as a lens: if the bottleneck is the shared memory $S$, one way to increase expressivity is to increase the structure of the memory interaction without blowing up the $N$ dependence.\nimport torch def multilinear_attn(q, ks, vs): \"\"\" Multilinear attention: elementwise product of L feature-space memories. Each (ks[l], vs[l]) pair contributes one D×D memory state; the states are multiplied elementwise before contracting with q. Args: q: [B, H, N, D] queries ks: [L, B, H, N, D] L key projections vs: [L, B, H, N, D] L value projections Returns: y: [B, H, N, D] output \"\"\" states = ks.mT @ vs # [L, B, H, D, D] state = states.prod(dim=0) # [B, H, D, D] return q @ state Strassen-style linearized mixing can be viewed as another structured memory composition:\nimport torch def strassen_linear_attn(q, k1, v1, k2, v2, g1, g2, g3, g4, scale=None): \"\"\" Strassen-style linearized mixing: structured combination of two memories. Args: q: [B, H, N, D] queries k1: [B, H, N, D] first key projection v1: [B, H, N, D] first value projection k2: [B, H, N, D] second key projection v2: [B, H, N, D] second value projection g1-g4: [B, H, N, D] learned gate tensors scale: float normalization scale Returns: y: [B, H, N, D] output \"\"\" if scale is None: N = q.size(-2) scale = N ** 05 S1 = (k1.mT / scale) @ (v1 / scale) # [B, H, D, D] S2 = (k2.mT / scale) @ (v2 / scale) # [B, H, D, D] v1_sum = v1.sum(dim=2, keepdim=True) # [B, H, 1, D] v2_sum = v2.sum(dim=2, keepdim=True) # [B, H, 1, D] y1 = (q @ S1) * v2_sum y2 = (S1 * S2).sum(dim=-2, keepdim=True).expand_as(q) y3 = (q @ S2) * v1_sum y4 = q @ (S1 * S2) return y1 * g1 + y2 * g2 + y3 * g3 + y4 * g4 Proposal: higher-order memory states (triple / quad attention) A second idea is to increase the capacity of the pooled state itself. Standard linear attention compresses everything into $S\\in\\mathbb{R}^{D\\times D}$. If you believe tasks are bottlenecked by this memory, you can increase its order.\nTriple attention uses a $D\\times D\\times D$ state:\n$$S_{ijk} = \\sum_{n=1}^N K^1_{ni}V_{nj}K^2_{nk},$$and\n$$Y_{nj} = \\sum_{i,k=1}^D Q^1_{ni}S_{ijk}Q^2_{nk}.$$Quad attention similarly uses a $D\\times D\\times D\\times D$ state:\n$$S_{ijk\\ell}=\\sum_{n=1}^N K^1_{ni}V_{nj}K^2_{nk}K^3_{n\\ell},$$and\n$$Y_{nj}=\\sum_{i,k,\\ell=1}^D Q^1_{ni}S_{ijk\\ell}Q^2_{nk}Q^3_{n\\ell}.$$These preserve linear scaling in $N$ but increase polynomial cost in $D$. That makes them plausible only when $D$ is small and kernels are highly optimized. In my own experiments, stability and memory traffic become the main hurdles quickly, so I view these as “maybe useful for specific bottlenecked settings,” not a general recipe.\nimport torch def triple_attn(q1, q2, k1, k2, v): \"\"\" Triple attention: third-order feature-space memory, linear in N. Args: q1: [B, H, N, D] first query projection q2: [B, H, N, D] second query projection k1: [B, H, N, D] first key projection k2: [B, H, N, D] second key projection v: [B, H, N, D] values Returns: y: [B, H, N, D] output \"\"\" state = torch.einsum('bhni,bhnj,bhnk-\u003ebhijk', k1, v, k2) # [B, H, D, D, D] return torch.einsum('bhni,bhijk,bhnk-\u003ebhnj', q1, state, q2) def quad_attn(q1, q2, q3, k1, k2, k3, v): \"\"\" Quad attention: fourth-order feature-space memory, linear in N. Args: q1-q3: [B, H, N, D] query projections k1-k3: [B, H, N, D] key projections v: [B, H, N, D] values Returns: y: [B, H, N, D] output \"\"\" state = torch.einsum('bhni,bhnj,bhnk,bhnl-\u003ebhijkl', k1, v, k2, k3) # [B, H, D, D, D, D] return torch.einsum('bhni,bhijkl,bhnk,bhnl-\u003ebhnj', q1, state, q2, q3) Where low-rank bottlenecks fit (and why linearizing FLARE collapses) Low-rank methods like FLARE reduce cost by routing through an intermediate set of latent tokens ($M\\ll N$). A subtle point is that they rely on a nonlinearity (softmax) that prevents full associativity collapse. If you fully linearize a two-stage gather–scatter mechanism, you typically recover something like $Y \\approx K\\,(Q^\\top Q)\\,(K^\\top V)$, which is still governed by a feature-space state and no longer depends on $M$ in an interesting way.\nI like keeping this sanity check in mind: if a design’s efficiency comes purely from associativity, then without an additional mechanism (nonlinearity, gating, recurrence, or structured bottlenecks) it tends to inherit the same shared-memory limitations.\nFLARE gather-scatter:\nimport torch.nn.functional as F def flare_mixer(q, k, v, scale=1.0): \"\"\" FLARE gather–scatter: low-rank global mixing via two SDPA calls. Args: q: [H, M, D] latent queries (per head, shared across batch) k: [B, H, N, D] token keys v: [B, H, N, D] token values scale: float attention scale Returns: y: [B, H, N, D] mixed token outputs \"\"\" qb = q.unsqueeze(0) # [1, H, M, D] z = F.scaled_dot_product_attention(qb, k, v, scale=scale) # [B, H, M, D] y = F.scaled_dot_product_attention(k, qb, z, scale=scale) # [B, H, N, D] return y Practical notes (from my experiments) A few things that have mattered more than I expected:\nRMSNorm has been more stable than LayerNorm in mixed precision for these variants. Normalization placement can dominate behavior. A recurring trick in linear attention is to simplify or remove row-normalization and stabilize later (e.g., RMSNorm after the mixer). Hybrid stacks are worth trying. Interleaving low-rank blocks with multilinear or higher-order blocks could be a reasonable way to trade off routing flexibility and stability, even if the standalone higher-order block is finicky. Benchmarks I care about I’m mainly interested in whether these mechanisms work beyond toy settings:\nPDE surrogates (steady and transient), where permutation equivariance matters and token ordering tricks are brittle. Long Range Arena, as a stress test for long-context sequence modeling. Comprehensive Attention Benchmark, to compare operators across a broader task suite. Takeaways Vanilla linear attention is efficient because of associativity, but that same compression creates a shared-memory bottleneck that weakens token-specific routing. Higher-order attention is a principled direction if you believe pairwise mixing is the wrong inductive bias for certain tasks, but naive formulations are intractable. Multilinear “memories” provide a clean linear-time way to inject higher-order structure, but in my experiments so far they have been hard to stabilize and have not consistently outperformed simpler baselines. If there is a practical path forward, I suspect it will involve careful normalization and gating, and likely hybrid designs that combine low-rank routing with higher-order feature-space interactions. If I were building the next round of experiments, my default plan would be:\nstart with a strong $L=1$ linear baseline (with modern normalization), then test $L=2$ multilinear as the cheapest higher-order extension, and only pursue triple/quad-state attention if there is clear evidence that the $D\\times D$ memory is the dominant bottleneck. References Katharopoulos, A. et al. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML (2020). https://arxiv.org/abs/2006.16236 Choromanski, K. et al. Rethinking Attention with Performers. ICLR (2021). https://arxiv.org/abs/2009.14794 Xiong, R. et al. Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention. AAAI (2021). https://arxiv.org/abs/2102.03902 Sun, Y. et al. Retentive Network: A Successor to Transformer for Large Language Models. arXiv (2023). https://arxiv.org/abs/2307.08621 Zhang, B., and Sennrich, R. Root Mean Square Layer Normalization. NeurIPS (2019). https://arxiv.org/abs/1910.07467 Tay, Y. et al. Long Range Arena: A Benchmark for Efficient Transformers. ICLR (2021). https://arxiv.org/abs/2011.04006 Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Kozachinskiy, A. et al. Strassen Attention, Split VC Dimension and Compositionality in Transformers. arXiv (2025). https://arxiv.org/abs/2501.19215 Roy, A. et al. Fast and Simplex: 2-Simplicial Attention in Triton. arXiv (2025). https://arxiv.org/abs/2507.02754 Qin, Z. et al. The Devil in Linear Transformer. arXiv (2022). https://arxiv.org/abs/2210.10340 ","wordCount":"2719","inLanguage":"en","image":"https://github.com/vpuri3.png","datePublished":"2026-02-16T00:00:00-05:00","dateModified":"2026-02-16T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://vpuri3.github.io/blog/adventures-in-high-order-attention/"},"publisher":{"@type":"Organization","name":"Vedant Puri","logo":{"@type":"ImageObject","url":"https://github.githubassets.com/favicons/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://vpuri3.github.io/ accesskey=h title="Vedant Puri (Alt + H)">Vedant Puri</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://vpuri3.github.io/#featured-work title=Featured><span>Featured</span></a></li><li><a href=https://vpuri3.github.io/#research-themes title=Research><span>Research</span></a></li><li><a href=https://vpuri3.github.io/#open-source title="Open Source"><span>Open Source</span></a></li><li><a href=https://vpuri3.github.io/work/ title=Work><span>Work</span></a></li><li><a href=https://vpuri3.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://vpuri3.github.io/photography/ title=Photography><span>Photography</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://vpuri3.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://vpuri3.github.io/blog/>Blog</a></div><h1 class="post-title entry-hint-parent">Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing</h1><div class=post-description>A practical tour of linear attention, its bottlenecks, and why multilinear / simplex-style attention might help</div><div class=post-meta><span title='2026-02-16 00:00:00 -0500 -0500'>February 16, 2026</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#beyond-pairwise-attention aria-label="Beyond pairwise attention">Beyond pairwise attention</a></li><li><a href=#preliminaries aria-label=Preliminaries>Preliminaries</a><ul><li><a href=#vanilla-softmax-attention-sdpa aria-label="Vanilla softmax attention (SDPA)">Vanilla softmax attention (SDPA)</a></li></ul></li><li><a href=#linear-attention aria-label="Linear attention">Linear attention</a><ul><li><a href=#the-basic-idea aria-label="The basic idea">The basic idea</a></li></ul></li><li><a href=#why-vanilla-linear-attention-often-underperforms aria-label="Why vanilla linear attention often underperforms">Why vanilla linear attention often underperforms</a></li><li><a href=#higher-order-attention-why-pairwise-may-not-be-enough aria-label="Higher-order attention: why pairwise may not be enough">Higher-order attention: why pairwise may not be enough</a></li><li><a href=#2-simplex-attention-3-way-interactions aria-label="2-simplex attention (3-way interactions)">2-simplex attention (3-way interactions)</a></li><li><a href=#proposal-multilinear-kernelized-attention-linear-time-simplex-style-mixing aria-label="Proposal: multilinear kernelized attention (linear-time simplex-style mixing)">Proposal: multilinear kernelized attention (linear-time simplex-style mixing)</a><ul><li><a href=#generalization-to-l-way-multilinear-attention aria-label="Generalization to L-way multilinear attention">Generalization to L-way multilinear attention</a></li></ul></li><li><a href=#proposal-higher-order-memory-states-triple--quad-attention aria-label="Proposal: higher-order memory states (triple / quad attention)">Proposal: higher-order memory states (triple / quad attention)</a></li><li><a href=#where-low-rank-bottlenecks-fit-and-why-linearizing-flare-collapses aria-label="Where low-rank bottlenecks fit (and why linearizing FLARE collapses)">Where low-rank bottlenecks fit (and why linearizing FLARE collapses)</a></li><li><a href=#practical-notes-from-my-experiments aria-label="Practical notes (from my experiments)">Practical notes (from my experiments)</a></li><li><a href=#benchmarks-i-care-about aria-label="Benchmarks I care about">Benchmarks I care about</a></li><li><a href=#takeaways aria-label=Takeaways>Takeaways</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=beyond-pairwise-attention>Beyond pairwise attention<a hidden class=anchor aria-hidden=true href=#beyond-pairwise-attention>#</a></h2><p>Softmax attention is an extremely expressive token-mixing primitive, but it is expensive. For a sequence of length $N$, the core interaction matrix $Q \cdot K^\top$ is $N \times N$, which drives both runtime and memory. Linear transformers try to keep global context while avoiding the quadratic scaling. The catch is that the simplest linear attention formulations often lose a key ingredient that makes softmax attention work so well: <strong>token-specific routing</strong>.</p><p>This post has two goals. First, I want to explain why vanilla linear attention underperforms in many settings, not as a matter of “bad approximation,” but as a structural consequence of how the computation is arranged. Second, I want to sketch a set of experimental ideas I’ve been exploring around <strong>enhanced attention mechanisms</strong>. To be explicit: these are research notes. Some of these ideas are promising on paper, but they have <em>not</em> reliably panned out in my experiments so far. I’m writing them up anyway because the framing has been useful for me, and it may help others reason about the design space.</p><p>One additional constraint threads through the post: <strong>permutation equivariance</strong>. Many strong long-sequence models exploit 1D sequence structure (chunking, convolution, hierarchical pooling, scan recurrences). That is great for language, but it is brittle for unstructured grids and point sets where there is no canonical ordering and where I would like the model to be insensitive to permutations.</p><hr><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><h3 id=vanilla-softmax-attention-sdpa>Vanilla softmax attention (SDPA)<a hidden class=anchor aria-hidden=true href=#vanilla-softmax-attention-sdpa>#</a></h3><p>Let $X \in \mathbb{R}^{N \times C}$ be a sequence of $N$ tokens with $C$ features. We form queries/keys/values by linear projection:</p>$$Q = XW^q,\quad K = XW^k,\quad V = XW^v$$<p>where $W^q,W^k,W^v \in \mathbb{R}^{C \times C}$. In multi-head attention we split features into $H$ heads of dimension $d = C/H$, writing</p>$$Q=[Q_1,\dots,Q_H],\quad K=[K_1,\dots,K_H],\quad V=[V_1,\dots,V_H].$$<p>Scaled dot-product attention (SDPA) in head $h$ is</p>$$Y_h = \mathrm{SDPA}(Q_h,K_h,V_h;s) = \mathrm{softmax}\!\left(\frac{Q_hK_h^\top}{s}\right)V_h$$<p>with $s \approx \sqrt{d}$.
In vector form,</p>$$y_i = \sum_{j=1}^N \frac{\exp(q_i^\top k_j)}{\sum_{\ell=1}^N \exp(q_i^\top k_\ell)}\cdot v_j.$$<p>The quadratic cost comes from the pairwise interaction matrix $QK^\top \in \mathbb{R}^{N\times N}$.
A useful mental model is: each query $q_i$ produces its own distribution over keys, so it can route information in a token-specific way. That “personalized routing” is what many linear-time mechanisms struggle to preserve.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sdpa_naive</span>(q, k, v):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Scaled dot-product attention.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q: [B, H, N, D]    queries (already split across heads)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k: [B, H, N, D]    keys
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v: [B, H, N, D]    values
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        scale: float       attention scale (default 1.0)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]    output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    scale <span style=color:#f92672>=</span> q<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    S <span style=color:#f92672>=</span> (q <span style=color:#f92672>@</span> k<span style=color:#f92672>.</span>mT) <span style=color:#f92672>*</span> scale <span style=color:#75715e># [B, H, N, N] attn logits</span>
</span></span><span style=display:flex><span>    A <span style=color:#f92672>=</span> S<span style=color:#f92672>.</span>softmax(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># [B, H, N, N] attn weights</span>
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> A <span style=color:#f92672>@</span> v              <span style=color:#75715e># [B, H, N, D]</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y
</span></span></code></pre></div><hr><h2 id=linear-attention>Linear attention<a hidden class=anchor aria-hidden=true href=#linear-attention>#</a></h2><h3 id=the-basic-idea>The basic idea<a hidden class=anchor aria-hidden=true href=#the-basic-idea>#</a></h3><p>Linear attention replaces the softmax kernel with a feature map $\phi(\cdot)$ (often constrained to be positive) so that similarities become dot products in feature space.
Define</p>$$\tilde{Q}=\phi(Q),\quad \tilde{K}=\phi(K).$$<p>A typical (row-normalized) update is</p>$$Y = \mathrm{rownorm}(\tilde Q\tilde K^\top)V,$$<p>which in vector form is</p>$$y_i = \sum_{j=1}^N \frac{(\tilde q_i^\top \tilde k_j)}{\sum_{\ell=1}^N \tilde q_i^\top \tilde k_\ell}\cdot v_j.$$<p>The efficiency comes from associativity:</p>$$y_i = \frac{\tilde q_i^\top\left(\sum_{j=1}^N \tilde k_j v_j^\top\right)}{\tilde q_i^\top\left(\sum_{\ell=1}^N \tilde k_\ell\right)}.$$<p>If we define a pooled “memory” state $S=\tilde K^\top V \in \mathbb{R}^{D\times D}$ and $z=\tilde K^\top\mathbf{1}\in\mathbb{R}^D$, we can write $Y = \dfrac{\tilde Q \cdot S}{\tilde Q \cdot z}$. This is linear in $N$ (up to constants) because the expensive terms are reductions over the sequence.
At a systems level, linear attention is appealing: you can stream over tokens, maintain a running state, and avoid materializing $N\times N$ matrices.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linear_attn</span>(q, k, v, kernel<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Linear attention: linear-time global mixing via associative state reduction.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q:      [B, H, N, D]    queries
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k:      [B, H, N, D]    keys
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v:      [B, H, N, D]    values
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        kernel: callable        feature map φ applied to q and k (e.g. elu + 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]    output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> kernel <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        q <span style=color:#f92672>=</span> kernel(q)
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> kernel(k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> k<span style=color:#f92672>.</span>mT <span style=color:#f92672>@</span> v                    <span style=color:#75715e># [B, H, D, D]</span>
</span></span><span style=display:flex><span>    k_sum <span style=color:#f92672>=</span> k<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># [B, H, D, 1]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    num <span style=color:#f92672>=</span> q <span style=color:#f92672>@</span> state
</span></span><span style=display:flex><span>    den <span style=color:#f92672>=</span> q <span style=color:#f92672>@</span> k_sum
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> num <span style=color:#f92672>/</span> (den <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-6</span>)
</span></span></code></pre></div><hr><h2 id=why-vanilla-linear-attention-often-underperforms>Why vanilla linear attention often underperforms<a hidden class=anchor aria-hidden=true href=#why-vanilla-linear-attention-often-underperforms>#</a></h2><p>It helps to look at the structural form of the update and ignore normalization for intuition. Linear attention can be written as</p>$$y_i = \tilde q_i^\top\left(\sum_{j=1}^N \tilde k_j v_j^\top\right),$$<p>i.e., $y_i = \tilde q_i^\top S$ where $S\in\mathbb{R}^{D\times D}$.</p><p>The critical observation is simple: $\tilde q_i$ changes with $i$, but the memory $S$ is shared across all tokens. Every token is querying the same pooled summary of key/value content. That makes the model efficient, but it also changes the nature of token mixing.</p><p><strong>Token-specific routing is weakened.</strong> In softmax attention, each query produces its own distribution over keys. In basic linear attention, each query projects against the same global state. You can still get token-dependent outputs because $\tilde q_i$ varies, but the mechanism no longer has “choose a few values and ignore the rest” behavior in the same way.</p><p><strong>Smoothing is hard to avoid.</strong> The associativity trick forces compression before interaction. Fine-grained information has to survive the bottleneck $S$ to influence outputs. This tends to smear distinctions unless the feature map and projections work very hard to preserve them.</p><p><strong>The bottleneck lives in feature space.</strong> Softmax attention carries $O(N^2)$ interaction capacity through an $N\times N$ attention map. Linear attention collapses this into a feature-space quadratic form: $S \in \mathbb{R}^{D\times D}$. Unless $\phi$ is carefully chosen (e.g., kernelized approximations of $\exp$), this can be structurally less expressive, not merely a “worse approximation.”</p><p>This is why the linear-transformer literature has so many “patches”: better feature maps (Performer/FAVOR+ style), gating and recurrence (RetNet/RWKV-like ideas), low-rank bottlenecks (Nyström, latent tokens), and various normalization tricks (including the practical observation that removing row-normalization and stabilizing with RMSNorm at the end can sometimes help).</p><hr><h2 id=higher-order-attention-why-pairwise-may-not-be-enough>Higher-order attention: why pairwise may not be enough<a hidden class=anchor aria-hidden=true href=#higher-order-attention-why-pairwise-may-not-be-enough>#</a></h2><p>Softmax attention is pairwise: it scores $(q_i,k_j)$ and uses that to weight $v_j$. Many tasks can be solved with pairwise interactions plus depth, but there are reasons to explore <strong>explicit multi-way mixing</strong>.</p><p>On algorithmic tasks, it is natural to point at interactions that look like (digit1, digit2, carry). On PDE-like data, nonlinearities often couple multiple factors, and it is tempting to ask whether explicitly modeling multi-way interactions could reduce the burden on depth or help linear-time models recover some selectivity.</p><p>One concrete formalization is <strong>2-simplex (3-way) attention</strong>, which replaces bilinear dot products with trilinear forms. I do not view this as “the answer,” but it is a clean starting point for thinking about higher-order token mixing.</p><hr><h2 id=2-simplex-attention-3-way-interactions>2-simplex attention (3-way interactions)<a hidden class=anchor aria-hidden=true href=#2-simplex-attention-3-way-interactions>#</a></h2><p>Standard attention is built on the bilinear form</p>$$\langle x_1,x_2\rangle = \sum_{d=1}^D x_1[d]x_2[d].$$<p>2-simplex attention generalizes this to a trilinear form</p>$$\langle x_1,x_2,x_3\rangle = \sum_{d=1}^D x_1[d]x_2[d]x_3[d].$$<p>A naive 3-way attention mechanism defines a score tensor over triples. Let</p>$$Q,K^1,K^2,V^1,V^2 \in \mathbb{R}^{N\times D},$$<p>and define</p>$$S_{ijk}=\sum_{d'=1}^D Q_{id'}K^1_{jd'}K^2_{kd'}.$$<p>Then $A=\mathrm{softmax}(S)$ (softmax over $(j,k)$) and</p>$$Y_{id}=\sum_{j,k=1}^N A_{ijk}V^1_{jd}V^2_{kd}.$$<p>This is expressive, but it requires forming $N\times N\times N$ tensors, which is intractable for large $N$.
So the question that keeps coming up is: can we capture some higher-order behavior without paying cubic cost?</p><hr><h2 id=proposal-multilinear-kernelized-attention-linear-time-simplex-style-mixing>Proposal: multilinear kernelized attention (linear-time simplex-style mixing)<a hidden class=anchor aria-hidden=true href=#proposal-multilinear-kernelized-attention-linear-time-simplex-style-mixing>#</a></h2><p>This is where the “research notes” part begins. The idea below is not a claim that I have a working model. It is a proposal that falls out of the same associativity trick that makes linear attention fast. It is conceptually clean, but in my experiments it has been finicky to stabilize and has not consistently beaten simpler baselines.</p><p>The key trick in linear attention is: avoid instantiating $N\times N$ by collapsing into a $D\times D$ state. We can do something similar for 2-simplex attention by dropping softmax and restructuring the computation so the triple interaction factorizes into feature-space memories.</p><p>Start from the unnormalized 3-way update:</p>$$A_{ijk}=\sum_{d'=1}^D Q_{id'}K^1_{jd'}K^2_{kd'},$$<p>and</p>$$Y_{id}=\sum_{j,k=1}^N A_{ijk}V^1_{jd}V^2_{kd}.$$<p>Rearranging sums gives</p>$$Y_{id}=\sum_{d'=1}^D Q_{id'}\left(\sum_{j=1}^N K^1_{jd'}V^1_{jd}\right)\left(\sum_{k=1}^N K^2_{kd'}V^2_{kd}\right).$$<p>Define feature-space memories</p>$$S^1=(K^1)^\top V^1 \in \mathbb{R}^{D\times D}$$<p>and</p>$$S^2=(K^2)^\top V^2 \in \mathbb{R}^{D\times D}.$$<p>Then the output becomes $Y = Q\left(S^1 \odot S^2\right)$ where $\odot$ is the elementwise Hadamard product. This is linear in $N$ because forming each $S^\ell$ is a single reduction pass over tokens.</p><h3 id=generalization-to-l-way-multilinear-attention>Generalization to L-way multilinear attention<a hidden class=anchor aria-hidden=true href=#generalization-to-l-way-multilinear-attention>#</a></h3><p>With $L$ key/value pairs</p>$$\{(K^\ell,V^\ell)\}_{\ell=1}^L,$$<p>define</p>$$S^\ell=(K^\ell)^\top V^\ell \in \mathbb{R}^{D\times D}$$<p>and compute</p>$$Y = Q\left(S^1 \odot S^2 \odot \cdots \odot S^L\right).$$<p>You can view this as a generalized linear-time $L$-simplicial attention.</p><p>One motivation I find useful: “mixture of memories” mechanisms tend to combine multiple memories additively (a router picks or mixes). This proposal combines them multiplicatively, which behaves like feature-wise gating: all memories have to agree, and features can be amplified or suppressed by products.</p><p>In practice, the open questions dominate:</p><ul><li>normalization (row-norm vs no row-norm + RMSNorm, or explicit gating),</li><li>whether RoPE-like positional structure still helps or becomes awkward under multiplicative state composition,</li><li>and whether the mechanism learns something meaningfully different than “more projections + gating.”</li></ul><p>So far, my results have not been clean enough to recommend this as a drop-in replacement. The main value for me has been as a lens: if the bottleneck is the shared memory $S$, one way to increase expressivity is to increase the <em>structure</em> of the memory interaction without blowing up the $N$ dependence.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>multilinear_attn</span>(q, ks, vs):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Multilinear attention: elementwise product of L feature-space memories.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Each (ks[l], vs[l]) pair contributes one D×D memory state; the states
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    are multiplied elementwise before contracting with q.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q:  [B, H, N, D]       queries
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        ks: [L, B, H, N, D]    L key projections
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        vs: [L, B, H, N, D]    L value projections
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]    output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    states <span style=color:#f92672>=</span> ks<span style=color:#f92672>.</span>mT <span style=color:#f92672>@</span> vs          <span style=color:#75715e># [L, B, H, D, D]</span>
</span></span><span style=display:flex><span>    state  <span style=color:#f92672>=</span> states<span style=color:#f92672>.</span>prod(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># [B, H, D, D]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> q <span style=color:#f92672>@</span> state
</span></span></code></pre></div><p>Strassen-style linearized mixing can be viewed as another structured memory composition:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>strassen_linear_attn</span>(q, k1, v1, k2, v2, g1, g2, g3, g4, scale<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Strassen-style linearized mixing: structured combination of two memories.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q:     [B, H, N, D]    queries
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k1:    [B, H, N, D]    first key projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v1:    [B, H, N, D]    first value projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k2:    [B, H, N, D]    second key projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v2:    [B, H, N, D]    second value projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        g1-g4: [B, H, N, D]   learned gate tensors
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        scale: float           normalization scale
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]    output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> scale <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        N <span style=color:#f92672>=</span> q<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        scale <span style=color:#f92672>=</span> N <span style=color:#f92672>**</span> <span style=color:#ae81ff>05</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    S1 <span style=color:#f92672>=</span> (k1<span style=color:#f92672>.</span>mT <span style=color:#f92672>/</span> scale) <span style=color:#f92672>@</span> (v1 <span style=color:#f92672>/</span> scale)      <span style=color:#75715e># [B, H, D, D]</span>
</span></span><span style=display:flex><span>    S2 <span style=color:#f92672>=</span> (k2<span style=color:#f92672>.</span>mT <span style=color:#f92672>/</span> scale) <span style=color:#f92672>@</span> (v2 <span style=color:#f92672>/</span> scale)      <span style=color:#75715e># [B, H, D, D]</span>
</span></span><span style=display:flex><span>    v1_sum <span style=color:#f92672>=</span> v1<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)      <span style=color:#75715e># [B, H, 1, D]</span>
</span></span><span style=display:flex><span>    v2_sum <span style=color:#f92672>=</span> v2<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)      <span style=color:#75715e># [B, H, 1, D]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    y1 <span style=color:#f92672>=</span> (q <span style=color:#f92672>@</span> S1) <span style=color:#f92672>*</span> v2_sum
</span></span><span style=display:flex><span>    y2 <span style=color:#f92672>=</span> (S1 <span style=color:#f92672>*</span> S2)<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>2</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>expand_as(q)
</span></span><span style=display:flex><span>    y3 <span style=color:#f92672>=</span> (q <span style=color:#f92672>@</span> S2) <span style=color:#f92672>*</span> v1_sum
</span></span><span style=display:flex><span>    y4 <span style=color:#f92672>=</span> q <span style=color:#f92672>@</span> (S1 <span style=color:#f92672>*</span> S2)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y1 <span style=color:#f92672>*</span> g1 <span style=color:#f92672>+</span> y2 <span style=color:#f92672>*</span> g2 <span style=color:#f92672>+</span> y3 <span style=color:#f92672>*</span> g3 <span style=color:#f92672>+</span> y4 <span style=color:#f92672>*</span> g4
</span></span></code></pre></div><hr><h2 id=proposal-higher-order-memory-states-triple--quad-attention>Proposal: higher-order memory states (triple / quad attention)<a hidden class=anchor aria-hidden=true href=#proposal-higher-order-memory-states-triple--quad-attention>#</a></h2><p>A second idea is to increase the capacity of the pooled state itself. Standard linear attention compresses everything into $S\in\mathbb{R}^{D\times D}$. If you believe tasks are bottlenecked by this memory, you can increase its order.</p><p>Triple attention uses a $D\times D\times D$ state:</p>$$S_{ijk} = \sum_{n=1}^N K^1_{ni}V_{nj}K^2_{nk},$$<p>and</p>$$Y_{nj} = \sum_{i,k=1}^D Q^1_{ni}S_{ijk}Q^2_{nk}.$$<p>Quad attention similarly uses a $D\times D\times D\times D$ state:</p>$$S_{ijk\ell}=\sum_{n=1}^N K^1_{ni}V_{nj}K^2_{nk}K^3_{n\ell},$$<p>and</p>$$Y_{nj}=\sum_{i,k,\ell=1}^D Q^1_{ni}S_{ijk\ell}Q^2_{nk}Q^3_{n\ell}.$$<p>These preserve linear scaling in $N$ but increase polynomial cost in $D$. That makes them plausible only when $D$ is small and kernels are highly optimized. In my own experiments, stability and memory traffic become the main hurdles quickly, so I view these as “maybe useful for specific bottlenecked settings,” not a general recipe.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>triple_attn</span>(q1, q2, k1, k2, v):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Triple attention: third-order feature-space memory, linear in N.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q1: [B, H, N, D]    first query projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q2: [B, H, N, D]    second query projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k1: [B, H, N, D]    first key projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k2: [B, H, N, D]    second key projection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v:  [B, H, N, D]    values
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]    output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;bhni,bhnj,bhnk-&gt;bhijk&#39;</span>, k1, v, k2)   <span style=color:#75715e># [B, H, D, D, D]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;bhni,bhijk,bhnk-&gt;bhnj&#39;</span>, q1, state, q2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>quad_attn</span>(q1, q2, q3, k1, k2, k3, v):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Quad attention: fourth-order feature-space memory, linear in N.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q1-q3: [B, H, N, D]    query projections
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k1-k3: [B, H, N, D]    key projections
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v:     [B, H, N, D]    values
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]    output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;bhni,bhnj,bhnk,bhnl-&gt;bhijkl&#39;</span>, k1, v, k2, k3)  <span style=color:#75715e># [B, H, D, D, D, D]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;bhni,bhijkl,bhnk,bhnl-&gt;bhnj&#39;</span>, q1, state, q2, q3)
</span></span></code></pre></div><hr><h2 id=where-low-rank-bottlenecks-fit-and-why-linearizing-flare-collapses>Where low-rank bottlenecks fit (and why linearizing FLARE collapses)<a hidden class=anchor aria-hidden=true href=#where-low-rank-bottlenecks-fit-and-why-linearizing-flare-collapses>#</a></h2><p>Low-rank methods like FLARE reduce cost by routing through an intermediate set of latent tokens ($M\ll N$). A subtle point is that they rely on a nonlinearity (softmax) that prevents full associativity collapse. If you fully linearize a two-stage gather–scatter mechanism, you typically recover something like $Y \approx K\,(Q^\top Q)\,(K^\top V)$, which is still governed by a feature-space state and no longer depends on $M$ in an interesting way.</p><p>I like keeping this sanity check in mind: if a design’s efficiency comes purely from associativity, then without an additional mechanism (nonlinearity, gating, recurrence, or structured bottlenecks) it tends to inherit the same shared-memory limitations.</p><p>FLARE gather-scatter:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>flare_mixer</span>(q, k, v, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    FLARE gather–scatter: low-rank global mixing via two SDPA calls.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        q: [H, M, D]      latent queries (per head, shared across batch)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k: [B, H, N, D]   token keys
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        v: [B, H, N, D]   token values
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        scale: float      attention scale
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        y: [B, H, N, D]   mixed token outputs
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    qb <span style=color:#f92672>=</span> q<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)                                          <span style=color:#75715e># [1, H, M, D]</span>
</span></span><span style=display:flex><span>    z  <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>scaled_dot_product_attention(qb, k, v, scale<span style=color:#f92672>=</span>scale)  <span style=color:#75715e># [B, H, M, D]</span>
</span></span><span style=display:flex><span>    y  <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>scaled_dot_product_attention(k, qb, z, scale<span style=color:#f92672>=</span>scale)  <span style=color:#75715e># [B, H, N, D]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y
</span></span></code></pre></div><hr><h2 id=practical-notes-from-my-experiments>Practical notes (from my experiments)<a hidden class=anchor aria-hidden=true href=#practical-notes-from-my-experiments>#</a></h2><p>A few things that have mattered more than I expected:</p><ul><li>RMSNorm has been more stable than LayerNorm in mixed precision for these variants.</li><li>Normalization placement can dominate behavior. A recurring trick in linear attention is to simplify or remove row-normalization and stabilize later (e.g., RMSNorm after the mixer).</li><li>Hybrid stacks are worth trying. Interleaving low-rank blocks with multilinear or higher-order blocks could be a reasonable way to trade off routing flexibility and stability, even if the standalone higher-order block is finicky.</li></ul><hr><h2 id=benchmarks-i-care-about>Benchmarks I care about<a hidden class=anchor aria-hidden=true href=#benchmarks-i-care-about>#</a></h2><p>I’m mainly interested in whether these mechanisms work beyond toy settings:</p><ul><li>PDE surrogates (steady and transient), where permutation equivariance matters and token ordering tricks are brittle.</li><li>Long Range Arena, as a stress test for long-context sequence modeling.</li><li>Comprehensive Attention Benchmark, to compare operators across a broader task suite.</li></ul><hr><h2 id=takeaways>Takeaways<a hidden class=anchor aria-hidden=true href=#takeaways>#</a></h2><ol><li>Vanilla linear attention is efficient because of associativity, but that same compression creates a shared-memory bottleneck that weakens token-specific routing.</li><li>Higher-order attention is a principled direction if you believe pairwise mixing is the wrong inductive bias for certain tasks, but naive formulations are intractable.</li><li>Multilinear “memories” provide a clean linear-time way to inject higher-order structure, but in my experiments so far they have been hard to stabilize and have not consistently outperformed simpler baselines.</li><li>If there is a practical path forward, I suspect it will involve careful normalization and gating, and likely hybrid designs that combine low-rank routing with higher-order feature-space interactions.</li></ol><p>If I were building the next round of experiments, my default plan would be:</p><ul><li>start with a strong $L=1$ linear baseline (with modern normalization),</li><li>then test $L=2$ multilinear as the cheapest higher-order extension,</li><li>and only pursue triple/quad-state attention if there is clear evidence that the $D\times D$ memory is the dominant bottleneck.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Katharopoulos, A. et al. <em>Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</em>. ICML (2020). <a href=https://arxiv.org/abs/2006.16236>https://arxiv.org/abs/2006.16236</a></li><li>Choromanski, K. et al. <em>Rethinking Attention with Performers</em>. ICLR (2021). <a href=https://arxiv.org/abs/2009.14794>https://arxiv.org/abs/2009.14794</a></li><li>Xiong, R. et al. <em>Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention</em>. AAAI (2021). <a href=https://arxiv.org/abs/2102.03902>https://arxiv.org/abs/2102.03902</a></li><li>Sun, Y. et al. <em>Retentive Network: A Successor to Transformer for Large Language Models</em>. arXiv (2023). <a href=https://arxiv.org/abs/2307.08621>https://arxiv.org/abs/2307.08621</a></li><li>Zhang, B., and Sennrich, R. <em>Root Mean Square Layer Normalization</em>. NeurIPS (2019). <a href=https://arxiv.org/abs/1910.07467>https://arxiv.org/abs/1910.07467</a></li><li>Tay, Y. et al. <em>Long Range Arena: A Benchmark for Efficient Transformers</em>. ICLR (2021). <a href=https://arxiv.org/abs/2011.04006>https://arxiv.org/abs/2011.04006</a></li><li>Vaswani, A. et al. <em>Attention Is All You Need</em>. NeurIPS (2017). <a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a></li><li>Kozachinskiy, A. et al. <em>Strassen Attention, Split VC Dimension and Compositionality in Transformers</em>. arXiv (2025). <a href=https://arxiv.org/abs/2501.19215>https://arxiv.org/abs/2501.19215</a></li><li>Roy, A. et al. <em>Fast and Simplex: 2-Simplicial Attention in Triton</em>. arXiv (2025). <a href=https://arxiv.org/abs/2507.02754>https://arxiv.org/abs/2507.02754</a></li><li>Qin, Z. et al. <em>The Devil in Linear Transformer</em>. arXiv (2022). <a href=https://arxiv.org/abs/2210.10340>https://arxiv.org/abs/2210.10340</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://vpuri3.github.io/>Vedant Puri</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>