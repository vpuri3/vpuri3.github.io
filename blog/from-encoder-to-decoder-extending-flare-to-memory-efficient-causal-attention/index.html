<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention | Vedant Puri</title><meta name=keywords content><meta name=description content="How FLARE evolves from bidirectional encoder attention to scalable causal decoder-only training and inference."><meta name=author content><link rel=canonical href=https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/><link crossorigin=anonymous href=/assets/css/stylesheet.72a547024d24c325f3bae9b2bb62cea7a569c0893d3ff900db35fa11b5fe740d.css integrity="sha256-cqVHAk0kwyXzuumyu2LOp6VpwIk9P/kA2zX6EbX+dA0=" rel="preload stylesheet" as=style><link rel=icon href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://github.githubassets.com/favicons/favicon.png><link rel=apple-touch-icon href=https://github.com/vpuri3.png><link rel=mask-icon href=https://github.githubassets.com/favicons/favicon.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/"><meta property="og:site_name" content="Vedant Puri"><meta property="og:title" content="From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention"><meta property="og:description" content="How FLARE evolves from bidirectional encoder attention to scalable causal decoder-only training and inference."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-18T00:00:00-05:00"><meta property="article:modified_time" content="2026-02-18T00:00:00-05:00"><meta property="og:image" content="https://github.com/vpuri3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/vpuri3.png"><meta name=twitter:title content="From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention"><meta name=twitter:description content="How FLARE evolves from bidirectional encoder attention to scalable causal decoder-only training and inference."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://vpuri3.github.io/blog/"},{"@type":"ListItem","position":2,"name":"From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention","item":"https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention","name":"From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention","description":"How FLARE evolves from bidirectional encoder attention to scalable causal decoder-only training and inference.","keywords":[],"articleBody":"Motivation FLARE was originally developed as an encoder-style global mixing primitive: learned latent queries gather information from many tokens, then scatter it back. The decoder setting is harder because causality changes algorithmic dependencies, numerical stability constraints, and what efficiency means in training versus inference.\nThis post summarizes a practical path to causal FLARE for long-context language modeling. See also the dissertation proposal talk for broader context.\nWhat changes from encoder to decoder? Encoder attention is bidirectional: token $t$ can depend on any token $\\tau$. Decoder attention is causal: token $t$ may depend only on $\\tau \\le t$.\nThis implies three requirements:\nNo future-token leakage. Efficient prefill for long contexts. Fast incremental decode with cached state. Recap: encoder FLARE as gather-scatter Let latent queries be $Q_L \\in \\mathbb{R}^{M \\times D}$, and token keys/values be $K,V \\in \\mathbb{R}^{N \\times D}$.\n$$ Z = \\mathrm{SDPA}(Q_L, K, V), \\qquad Y = \\mathrm{SDPA}(K, Q_L, Z). $$Interpretation:\nGather: latents pool from tokens. Scatter: tokens read from latents. Causal softmax attention baseline $$ y_t = \\sum_{\\tau \\le t} \\frac{\\exp(q_t^\\top k_\\tau)}{\\sum_{u \\le t}\\exp(q_t^\\top k_u)}v_\\tau $$or\n$$ Y = \\mathrm{softmax}\\!\\big((QK^\\top) \\odot M_{\\mathrm{causal}}\\big)V, $$where $M_{\\mathrm{causal}}$ masks the strict upper triangle.\nWarm-up: causal linear attention as a state update $$ y_t = \\frac{S_t q_t}{q_t^\\top z_t}, \\qquad S_t = \\sum_{\\tau \\le t} v_\\tau k_\\tau^\\top, \\qquad z_t = \\sum_{\\tau \\le t} k_\\tau. $$This shows the decoder-friendly pattern: maintain prefix state, update incrementally, and compute $y_t$ from state plus $q_t$.\nCausal FLARE definition A causal latent update can be written as:\n$$ z_m^t = \\sum_{\\tau \\le t} \\frac{\\exp(q_m^\\top k_\\tau)}{\\sum_{u \\le t}\\exp(q_m^\\top k_u)}v_\\tau. $$Then token output at step $t$:\n$$ y_t = \\sum_{m=1}^M \\frac{\\exp(k_t^\\top q_m)}{\\sum_{m'=1}^M \\exp(k_t^\\top q_{m'})} z_m^t. $$So each step produces an updated latent set $Z_t = [z_1^t,\\ldots,z_M^t]$, then token $t$ reads from it.\nAlgorithm 1: streaming recurrent causal FLARE (decode-friendly) Each latent maintains running online-softmax statistics across the token stream: a running max $\\mu_{t,m}$, a normalizing denominator $d_{t,m}$, and a numerator accumulator $U_{t,m,:}$. The update at each step is:\nInitialize: $U_0 \\in \\mathbb{R}^{M \\times D} \\leftarrow 0$ $d_0 \\in \\mathbb{R}^{M} \\leftarrow 0$ $\\mu_0 \\in \\mathbb{R}^{M} \\leftarrow -\\infty$ For each token $t=1,\\ldots,T$, given $(k_t, v_t)$: $s_t \\leftarrow (Qk_t)s$ $\\mu_t \\leftarrow \\max(\\mu_{t-1}, s_t)$ $\\gamma \\leftarrow \\exp(\\mu_{t-1}-\\mu_t)$ $\\eta \\leftarrow \\exp(s_t-\\mu_t)$ $d_t \\leftarrow d_{t-1}\\odot\\gamma + \\eta$ $U_t \\leftarrow U_{t-1}\\odot\\gamma + \\eta\\,v_t^\\top$ $Z_t \\leftarrow U_t \\oslash d_t$ $\\alpha_t \\leftarrow \\mathrm{softmax}(s_t)$ $y_t \\leftarrow \\alpha_t^\\top Z_t$ Return $\\{y_t\\}_{t=1}^T$. This recurrent form is ideal for autoregressive decode — cached state is updated in $O(M)$ work per token. However, it is throughput-inefficient for training because the backward pass must store all prefix statistics naively. The next two algorithms address the prefill and training setting.\nAlgorithm 2: dense causal FLARE (prefill-oriented) Define scores:\n$$ S_{t,m} = s\\,k_t^\\top q_m, \\qquad A_{t,m} = \\exp(S_{t,m}), \\qquad P_{t,m} = \\mathrm{softmax}_m(S_{t,:}). $$Prefix denominator per latent:\n$$ D_{t,m} = \\sum_{u \\le t} A_{u,m}. $$Then\n$$ C_{t,m} = \\frac{P_{t,m}}{D_{t,m}+\\varepsilon}, \\qquad W = C A^\\top, \\qquad Y = (W \\odot M_{\\mathrm{causal}})V. $$This dense form is matmul-friendly and efficient for training, but computing $\\exp(S)$ directly is numerically unstable for long contexts in mixed precision — exponents of large scores overflow in BF16. Algorithm 3 fixes this.\nAlgorithm 3: stable dense causal FLARE Use online-softmax style prefix statistics for each latent:\n$R_{t,m}$: running prefix max of $S_{t,m}$ $L_{t,m}$: stable prefix sum in normalized frame $$ R_{t,m} = \\max(R_{t-1,m}, S_{t,m}), $$$$ L_{t,m} = L_{t-1,m}\\exp(R_{t-1,m}-R_{t,m}) + \\exp(S_{t,m}-R_{t,m}). $$Then\n$$ C_{t,m} = \\frac{P_{t,m}}{L_{t,m}+\\varepsilon}, \\qquad W_{t,\\tau} = \\sum_{m=1}^M C_{t,m}\\exp(S_{\\tau,m}-R_{t,m}), $$$$ W \\leftarrow W \\odot M_{\\mathrm{causal}}, \\qquad Y = WV. $$This keeps the prefill path numerically stable while preserving dense-kernel structure.\nCompute score and latent decode probabilities: $S \\leftarrow s(KQ^\\top)$, so $S_{t,m}=s\\,k_t^\\top q_m$ $P \\leftarrow \\mathrm{softmax}_m(S)$ Compute stable prefix statistics for each latent: Initialize $R_{0,m}\\leftarrow -\\infty,\\;L_{0,m}\\leftarrow 0$ For $t=1,\\ldots,T$: $R_{t,m}\\leftarrow \\max(R_{t-1,m}, S_{t,m})$ $L_{t,m}\\leftarrow L_{t-1,m}\\exp(R_{t-1,m}-R_{t,m})+\\exp(S_{t,m}-R_{t,m})$ Build dense causal mixer: $C_{t,m}\\leftarrow \\dfrac{P_{t,m}}{L_{t,m}+\\varepsilon}$ $W_{t,\\tau}\\leftarrow \\sum_{m=1}^M C_{t,m}\\exp(S_{\\tau,m}-R_{t,m})$ $W \\leftarrow W \\odot M_{\\mathrm{causal}}$ Output: $Y \\leftarrow WV$. Train vs prefill vs decode Causal FLARE supports three operational regimes, each with different priorities.\nTeacher-forced training processes the full sequence in parallel and is throughput-oriented. Algorithm 3 is the right choice: stable prefix statistics, chunking over time to avoid materializing $T \\times T$, and fused kernels for arithmetic intensity.\nInference prefill is algorithmically identical to training but with a different blocking profile. Prefill is latency-sensitive and may benefit from different tile sizes and more aggressive kernel fusion than the training path.\nAutoregressive decode is latency-critical and processes one token at a time. Algorithm 1 is ideal: update the cached latent state with each new $(k_t, v_t)$, then read $y_t$ from the updated latents. No attention matrix is ever constructed, and the state size $M$ is the only memory overhead beyond the KV cache.\nAdaptive attention state size A practical FLARE advantage is controllable latent/state budget:\nLarger state in prefill for fidelity Smaller state in decode for throughput This exposes a direct compute-memory-accuracy knob.\nSystems notes A few implementation details matter disproportionately.\nFP32 prefix accumulators. The running max and sum statistics accumulate error across many tokens. Accumulating in FP32 prevents catastrophic cancellation even when inputs are in BF16 or FP16.\nTime chunking. Processing time in chunks avoids materializing the full $T \\times T$ intermediate — which is precisely the goal. Chunk size trades register pressure against memory traffic and should be tuned per GPU.\nSeparate kernels per regime. Training, prefill, and decode have different access patterns and arithmetic intensities. A single fused kernel cannot be optimally tiled for all three; separate kernels let you autotune each independently.\nMemory bandwidth first. At long contexts, causal FLARE is memory-bandwidth-bound rather than compute-bound. Optimizing cache layout and minimizing global memory traffic matters more than maximizing FLOPs/s.\nReferences Puri, V. et al. FLARE: Fast Low-rank Attention Routing Engine. arXiv (2025). https://arxiv.org/abs/2508.12594 Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Dao, T. et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS (2022). https://arxiv.org/abs/2205.14135 Qin, Z. et al. The Devil in Linear Transformer. arXiv (2022). https://arxiv.org/abs/2210.10340 ","wordCount":"980","inLanguage":"en","image":"https://github.com/vpuri3.png","datePublished":"2026-02-18T00:00:00-05:00","dateModified":"2026-02-18T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/"},"publisher":{"@type":"Organization","name":"Vedant Puri","logo":{"@type":"ImageObject","url":"https://github.githubassets.com/favicons/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://vpuri3.github.io/ accesskey=h title="Vedant Puri (Alt + H)">Vedant Puri</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://vpuri3.github.io/#featured-work title=Featured><span>Featured</span></a></li><li><a href=https://vpuri3.github.io/#research-themes title=Research><span>Research</span></a></li><li><a href=https://vpuri3.github.io/#open-source title="Open Source"><span>Open Source</span></a></li><li><a href=https://vpuri3.github.io/work/ title=Work><span>Work</span></a></li><li><a href=https://vpuri3.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://vpuri3.github.io/photography/ title=Photography><span>Photography</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://vpuri3.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://vpuri3.github.io/blog/>Blog</a></div><h1 class="post-title entry-hint-parent">From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention</h1><div class=post-description>How FLARE evolves from bidirectional encoder attention to scalable causal decoder-only training and inference.</div><div class=post-meta><span title='2026-02-18 00:00:00 -0500 -0500'>February 18, 2026</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#what-changes-from-encoder-to-decoder aria-label="What changes from encoder to decoder?">What changes from encoder to decoder?</a></li><li><a href=#recap-encoder-flare-as-gather-scatter aria-label="Recap: encoder FLARE as gather-scatter">Recap: encoder FLARE as gather-scatter</a></li><li><a href=#causal-softmax-attention-baseline aria-label="Causal softmax attention baseline">Causal softmax attention baseline</a></li><li><a href=#warm-up-causal-linear-attention-as-a-state-update aria-label="Warm-up: causal linear attention as a state update">Warm-up: causal linear attention as a state update</a></li><li><a href=#causal-flare-definition aria-label="Causal FLARE definition">Causal FLARE definition</a></li><li><a href=#algorithm-1-streaming-recurrent-causal-flare-decode-friendly aria-label="Algorithm 1: streaming recurrent causal FLARE (decode-friendly)">Algorithm 1: streaming recurrent causal FLARE (decode-friendly)</a></li><li><a href=#algorithm-2-dense-causal-flare-prefill-oriented aria-label="Algorithm 2: dense causal FLARE (prefill-oriented)">Algorithm 2: dense causal FLARE (prefill-oriented)</a></li><li><a href=#algorithm-3-stable-dense-causal-flare aria-label="Algorithm 3: stable dense causal FLARE">Algorithm 3: stable dense causal FLARE</a></li><li><a href=#train-vs-prefill-vs-decode aria-label="Train vs prefill vs decode">Train vs prefill vs decode</a></li><li><a href=#adaptive-attention-state-size aria-label="Adaptive attention state size">Adaptive attention state size</a></li><li><a href=#systems-notes aria-label="Systems notes">Systems notes</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>FLARE was originally developed as an encoder-style global mixing primitive: learned latent queries gather information from many tokens, then scatter it back. The decoder setting is harder because causality changes algorithmic dependencies, numerical stability constraints, and what efficiency means in training versus inference.</p><p>This post summarizes a practical path to causal FLARE for long-context language modeling. See also the <a href="https://www.youtube.com/watch?v=8h9EXJqQUi0">dissertation proposal talk</a> for broader context.</p><hr><h2 id=what-changes-from-encoder-to-decoder>What changes from encoder to decoder?<a hidden class=anchor aria-hidden=true href=#what-changes-from-encoder-to-decoder>#</a></h2><p>Encoder attention is bidirectional: token $t$ can depend on any token $\tau$. Decoder attention is causal: token $t$ may depend only on $\tau \le t$.</p><p>This implies three requirements:</p><ol><li>No future-token leakage.</li><li>Efficient prefill for long contexts.</li><li>Fast incremental decode with cached state.</li></ol><hr><h2 id=recap-encoder-flare-as-gather-scatter>Recap: encoder FLARE as gather-scatter<a hidden class=anchor aria-hidden=true href=#recap-encoder-flare-as-gather-scatter>#</a></h2><p>Let latent queries be $Q_L \in \mathbb{R}^{M \times D}$, and token keys/values be $K,V \in \mathbb{R}^{N \times D}$.</p>$$
Z = \mathrm{SDPA}(Q_L, K, V), \qquad
Y = \mathrm{SDPA}(K, Q_L, Z).
$$<p>Interpretation:</p><ul><li>Gather: latents pool from tokens.</li><li>Scatter: tokens read from latents.</li></ul><hr><h2 id=causal-softmax-attention-baseline>Causal softmax attention baseline<a hidden class=anchor aria-hidden=true href=#causal-softmax-attention-baseline>#</a></h2>$$
y_t
= \sum_{\tau \le t}
\frac{\exp(q_t^\top k_\tau)}{\sum_{u \le t}\exp(q_t^\top k_u)}v_\tau
$$<p>or</p>$$
Y = \mathrm{softmax}\!\big((QK^\top) \odot M_{\mathrm{causal}}\big)V,
$$<p>where $M_{\mathrm{causal}}$ masks the strict upper triangle.</p><hr><h2 id=warm-up-causal-linear-attention-as-a-state-update>Warm-up: causal linear attention as a state update<a hidden class=anchor aria-hidden=true href=#warm-up-causal-linear-attention-as-a-state-update>#</a></h2>$$
y_t = \frac{S_t q_t}{q_t^\top z_t},
\qquad
S_t = \sum_{\tau \le t} v_\tau k_\tau^\top,
\qquad
z_t = \sum_{\tau \le t} k_\tau.
$$<p>This shows the decoder-friendly pattern: maintain prefix state, update incrementally, and compute $y_t$ from state plus $q_t$.</p><hr><h2 id=causal-flare-definition>Causal FLARE definition<a hidden class=anchor aria-hidden=true href=#causal-flare-definition>#</a></h2><p>A causal latent update can be written as:</p>$$
z_m^t =
\sum_{\tau \le t}
\frac{\exp(q_m^\top k_\tau)}{\sum_{u \le t}\exp(q_m^\top k_u)}v_\tau.
$$<p>Then token output at step $t$:</p>$$
y_t = \sum_{m=1}^M
\frac{\exp(k_t^\top q_m)}{\sum_{m'=1}^M \exp(k_t^\top q_{m'})}
z_m^t.
$$<p>So each step produces an updated latent set $Z_t = [z_1^t,\ldots,z_M^t]$, then token $t$ reads from it.</p><hr><h2 id=algorithm-1-streaming-recurrent-causal-flare-decode-friendly>Algorithm 1: streaming recurrent causal FLARE (decode-friendly)<a hidden class=anchor aria-hidden=true href=#algorithm-1-streaming-recurrent-causal-flare-decode-friendly>#</a></h2><p>Each latent maintains running online-softmax statistics across the token stream: a running max $\mu_{t,m}$, a normalizing denominator $d_{t,m}$, and a numerator accumulator $U_{t,m,:}$. The update at each step is:</p><ol><li>Initialize:<ul><li>$U_0 \in \mathbb{R}^{M \times D} \leftarrow 0$</li><li>$d_0 \in \mathbb{R}^{M} \leftarrow 0$</li><li>$\mu_0 \in \mathbb{R}^{M} \leftarrow -\infty$</li></ul></li><li>For each token $t=1,\ldots,T$, given $(k_t, v_t)$:<ul><li>$s_t \leftarrow (Qk_t)s$</li><li>$\mu_t \leftarrow \max(\mu_{t-1}, s_t)$</li><li>$\gamma \leftarrow \exp(\mu_{t-1}-\mu_t)$</li><li>$\eta \leftarrow \exp(s_t-\mu_t)$</li><li>$d_t \leftarrow d_{t-1}\odot\gamma + \eta$</li><li>$U_t \leftarrow U_{t-1}\odot\gamma + \eta\,v_t^\top$</li><li>$Z_t \leftarrow U_t \oslash d_t$</li><li>$\alpha_t \leftarrow \mathrm{softmax}(s_t)$</li><li>$y_t \leftarrow \alpha_t^\top Z_t$</li></ul></li><li>Return $\{y_t\}_{t=1}^T$.</li></ol><p>This recurrent form is ideal for autoregressive decode — cached state is updated in $O(M)$ work per token. However, it is throughput-inefficient for training because the backward pass must store all prefix statistics naively. The next two algorithms address the prefill and training setting.</p><hr><h2 id=algorithm-2-dense-causal-flare-prefill-oriented>Algorithm 2: dense causal FLARE (prefill-oriented)<a hidden class=anchor aria-hidden=true href=#algorithm-2-dense-causal-flare-prefill-oriented>#</a></h2><p>Define scores:</p>$$
S_{t,m} = s\,k_t^\top q_m,
\qquad
A_{t,m} = \exp(S_{t,m}),
\qquad
P_{t,m} = \mathrm{softmax}_m(S_{t,:}).
$$<p>Prefix denominator per latent:</p>$$
D_{t,m} = \sum_{u \le t} A_{u,m}.
$$<p>Then</p>$$
C_{t,m} = \frac{P_{t,m}}{D_{t,m}+\varepsilon},
\qquad
W = C A^\top,
\qquad
Y = (W \odot M_{\mathrm{causal}})V.
$$<p>This dense form is matmul-friendly and efficient for training, but computing $\exp(S)$ directly is numerically unstable for long contexts in mixed precision — exponents of large scores overflow in BF16. Algorithm 3 fixes this.</p><hr><h2 id=algorithm-3-stable-dense-causal-flare>Algorithm 3: stable dense causal FLARE<a hidden class=anchor aria-hidden=true href=#algorithm-3-stable-dense-causal-flare>#</a></h2><p>Use online-softmax style prefix statistics for each latent:</p><ul><li>$R_{t,m}$: running prefix max of $S_{t,m}$</li><li>$L_{t,m}$: stable prefix sum in normalized frame</li></ul>$$
R_{t,m} = \max(R_{t-1,m}, S_{t,m}),
$$$$
L_{t,m} = L_{t-1,m}\exp(R_{t-1,m}-R_{t,m}) + \exp(S_{t,m}-R_{t,m}).
$$<p>Then</p>$$
C_{t,m} = \frac{P_{t,m}}{L_{t,m}+\varepsilon},
\qquad
W_{t,\tau} = \sum_{m=1}^M C_{t,m}\exp(S_{\tau,m}-R_{t,m}),
$$$$
W \leftarrow W \odot M_{\mathrm{causal}},
\qquad
Y = WV.
$$<p>This keeps the prefill path numerically stable while preserving dense-kernel structure.</p><ol><li>Compute score and latent decode probabilities:<ul><li>$S \leftarrow s(KQ^\top)$, so $S_{t,m}=s\,k_t^\top q_m$</li><li>$P \leftarrow \mathrm{softmax}_m(S)$</li></ul></li><li>Compute stable prefix statistics for each latent:<ul><li>Initialize $R_{0,m}\leftarrow -\infty,\;L_{0,m}\leftarrow 0$</li><li>For $t=1,\ldots,T$:<ul><li>$R_{t,m}\leftarrow \max(R_{t-1,m}, S_{t,m})$</li><li>$L_{t,m}\leftarrow L_{t-1,m}\exp(R_{t-1,m}-R_{t,m})+\exp(S_{t,m}-R_{t,m})$</li></ul></li></ul></li><li>Build dense causal mixer:<ul><li>$C_{t,m}\leftarrow \dfrac{P_{t,m}}{L_{t,m}+\varepsilon}$</li><li>$W_{t,\tau}\leftarrow \sum_{m=1}^M C_{t,m}\exp(S_{\tau,m}-R_{t,m})$</li><li>$W \leftarrow W \odot M_{\mathrm{causal}}$</li></ul></li><li>Output:<ul><li>$Y \leftarrow WV$.</li></ul></li></ol><hr><h2 id=train-vs-prefill-vs-decode>Train vs prefill vs decode<a hidden class=anchor aria-hidden=true href=#train-vs-prefill-vs-decode>#</a></h2><p>Causal FLARE supports three operational regimes, each with different priorities.</p><p><strong>Teacher-forced training</strong> processes the full sequence in parallel and is throughput-oriented. Algorithm 3 is the right choice: stable prefix statistics, chunking over time to avoid materializing $T \times T$, and fused kernels for arithmetic intensity.</p><p><strong>Inference prefill</strong> is algorithmically identical to training but with a different blocking profile. Prefill is latency-sensitive and may benefit from different tile sizes and more aggressive kernel fusion than the training path.</p><p><strong>Autoregressive decode</strong> is latency-critical and processes one token at a time. Algorithm 1 is ideal: update the cached latent state with each new $(k_t, v_t)$, then read $y_t$ from the updated latents. No attention matrix is ever constructed, and the state size $M$ is the only memory overhead beyond the KV cache.</p><hr><h2 id=adaptive-attention-state-size>Adaptive attention state size<a hidden class=anchor aria-hidden=true href=#adaptive-attention-state-size>#</a></h2><p>A practical FLARE advantage is controllable latent/state budget:</p><ul><li>Larger state in prefill for fidelity</li><li>Smaller state in decode for throughput</li></ul><p>This exposes a direct compute-memory-accuracy knob.</p><hr><h2 id=systems-notes>Systems notes<a hidden class=anchor aria-hidden=true href=#systems-notes>#</a></h2><p>A few implementation details matter disproportionately.</p><p><strong>FP32 prefix accumulators.</strong> The running max and sum statistics accumulate error across many tokens. Accumulating in FP32 prevents catastrophic cancellation even when inputs are in BF16 or FP16.</p><p><strong>Time chunking.</strong> Processing time in chunks avoids materializing the full $T \times T$ intermediate — which is precisely the goal. Chunk size trades register pressure against memory traffic and should be tuned per GPU.</p><p><strong>Separate kernels per regime.</strong> Training, prefill, and decode have different access patterns and arithmetic intensities. A single fused kernel cannot be optimally tiled for all three; separate kernels let you autotune each independently.</p><p><strong>Memory bandwidth first.</strong> At long contexts, causal FLARE is memory-bandwidth-bound rather than compute-bound. Optimizing cache layout and minimizing global memory traffic matters more than maximizing FLOPs/s.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Puri, V. et al. <em>FLARE: Fast Low-rank Attention Routing Engine</em>. arXiv (2025). <a href=https://arxiv.org/abs/2508.12594>https://arxiv.org/abs/2508.12594</a></li><li>Vaswani, A. et al. <em>Attention Is All You Need</em>. NeurIPS (2017). <a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a></li><li>Dao, T. et al. <em>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</em>. NeurIPS (2022). <a href=https://arxiv.org/abs/2205.14135>https://arxiv.org/abs/2205.14135</a></li><li>Qin, Z. et al. <em>The Devil in Linear Transformer</em>. arXiv (2022). <a href=https://arxiv.org/abs/2210.10340>https://arxiv.org/abs/2210.10340</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://vpuri3.github.io/>Vedant Puri</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>