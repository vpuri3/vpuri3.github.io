<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>From POD to neural manifolds: a modern take on reduced order modeling | Vedant Puri</title><meta name=keywords content><meta name=description content="From classical projection methods to smooth neural field ROMs"><meta name=author content><link rel=canonical href=https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/><link crossorigin=anonymous href=/assets/css/stylesheet.72a547024d24c325f3bae9b2bb62cea7a569c0893d3ff900db35fa11b5fe740d.css integrity="sha256-cqVHAk0kwyXzuumyu2LOp6VpwIk9P/kA2zX6EbX+dA0=" rel="preload stylesheet" as=style><link rel=icon href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://github.githubassets.com/favicons/favicon.png><link rel=apple-touch-icon href=https://github.com/vpuri3.png><link rel=mask-icon href=https://github.githubassets.com/favicons/favicon.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/"><meta property="og:site_name" content="Vedant Puri"><meta property="og:title" content="From POD to neural manifolds: a modern take on reduced order modeling"><meta property="og:description" content="From classical projection methods to smooth neural field ROMs"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-15T00:00:00-05:00"><meta property="article:modified_time" content="2026-02-15T00:00:00-05:00"><meta property="og:image" content="https://github.com/vpuri3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/vpuri3.png"><meta name=twitter:title content="From POD to neural manifolds: a modern take on reduced order modeling"><meta name=twitter:description content="From classical projection methods to smooth neural field ROMs"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://vpuri3.github.io/blog/"},{"@type":"ListItem","position":2,"name":"From POD to neural manifolds: a modern take on reduced order modeling","item":"https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"From POD to neural manifolds: a modern take on reduced order modeling","name":"From POD to neural manifolds: a modern take on reduced order modeling","description":"From classical projection methods to smooth neural field ROMs","keywords":[],"articleBody":"From POD to neural manifolds Reduced-order modeling (ROM) is ultimately about preserving dominant system behavior with far fewer degrees of freedom. Classical projection methods, convolutional autoencoder ROMs, and modern neural-field formulations all target this same objective, but they make different assumptions about (i) how we represent the state and (ii) how we evolve it in time.\nThis post walks through that progression and focuses on Smooth Neural Field ROM (SNF-ROM): a projection-based nonlinear ROM that uses continuous neural fields as the state representation and explicitly supports physics-based differentiation and time integration during deployment.\nPreprint: arXiv:2405.14890\nJ. Comp. Phys. paper: https://doi.org/10.1016/j.jcp.2025.113957\nSee my dissertation proposal talk on this topic!\nWhy ROM matters for engineering workflows High-fidelity simulation is now central to modern engineering pipelines, from fluid mechanics to control and design. However, high accuracy typically requires resolving many spatial and temporal scales, which makes PDE solves expensive. These costs become the bottleneck in workflows that require repeated evaluations, such as design-space exploration, optimization, and uncertainty quantification.\nMachine learning offers a complementary approach to accelerating simulation. Instead of solving the full PDE from scratch every time, we can learn a surrogate model from data generated by high-fidelity simulations. Training such models can be computationally expensive, since they must observe many examples across parameter settings, geometries, and time horizons. But this cost is paid once offline. At deployment time, the learned model can evaluate new scenarios orders of magnitude faster than a traditional solver, making it attractive for design optimization, uncertainty quantification, control, and real-time decision-making workflows. In this sense, machine learning amortizes the cost of expensive PDE solves across many downstream queries.\nA particularly important class of surrogates is reduced-order models (ROMs). The central idea behind ROMs is that although PDE states live in extremely high-dimensional spaces (millions of degrees of freedom in realistic simulations), the effective dynamics often evolve on a much lower-dimensional structure. ROMs aim to discover this structure and use it to simulate the system using far fewer variables. Traditionally, this was done with linear subspace methods such as Proper Orthogonal Decomposition (POD) combined with Galerkin projection. Modern approaches extend this idea using machine learning to learn nonlinear representations that can better capture complex dynamics.\nIn this post, we focus on neural reduced-order modeling: learn a low-dimensional representation directly from data, then evolve that representation using physics-aware structure. Instead of predicting the full state everywhere, the model predicts a compact latent state that encodes the dominant behavior of the system, and the full field can be reconstructed from this latent representation. This separates the problem into two parts: representing the state efficiently and modeling how that representation evolves in time.\nBefore diving into SNF-ROM, it is helpful to recall the manifold learning viewpoint that underlies most nonlinear ROM methods. The core assumption is that although simulation states are extremely high-dimensional, they often lie close to a smooth, low-dimensional manifold embedded in the ambient space. For example, fluid flow snapshots over time may evolve primarily along a small number of coherent structures or modes. Learning this manifold provides a natural coordinate system in which the dynamics can be expressed compactly, and the role of a ROM is to learn both the geometry of this manifold and the dynamics that govern motion along it.\nThis perspective is illustrated by contrasting the full-order model (FOM) with linear and nonlinear ROMs. The governing PDE can be written abstractly as\n$$ \\frac{\\partial u}{\\partial t} = \\mathcal{L}(x,t,u;\\mu), $$\nwhere $u(x,t;\\mu)$ is the high-dimensional state, $x$ is spatial position, $t$ is time, and $\\mu$ denotes parameters. In a full discretization, the state is represented as\n$$ u(x,t;\\mu) \\approx g_{\\mathrm{FOM}}(x,\\bar u(t;\\mu)) = \\Phi(x),\\bar u(t;\\mu), $$\nwhere $\\Phi(x)$ is a spatial basis and $\\bar u \\in \\mathbb{R}^{N_{\\mathrm{FOM}}}$ are high-dimensional coefficients. A linear POD-ROM assumes the dynamics evolve in a low-dimensional linear subspace,\n$$ \\bar u(t;\\mu) \\approx \\bar u_0 + \\mathbf{P},\\tilde u(t;\\mu), $$\nwhere $\\mathbf{P}$ is a projection matrix and $\\tilde u \\in \\mathbb{R}^{N_{\\mathrm{Lin\\text{-}ROM}}}$ are reduced coordinates. A nonlinear ROM instead assumes the state lies on a curved manifold and uses a learned decoder,\n$$ u(x,t;\\mu) \\approx g_{\\mathrm{ROM}}(x,\\tilde u(t;\\mu)) = \\mathrm{NN}_\\theta(x,\\tilde u(t;\\mu)), $$\nwhere $\\mathrm{NN}_\\theta$ is a neural field mapping spatial coordinates and latent variables to the state. The dimensionality relationship\n$$ N_{\\mathrm{NI\\text{-}ROM}} \\le N_{\\mathrm{Lin\\text{-}ROM}} \\ll N_{\\mathrm{FOM}} $$\ncaptures the intuition that nonlinear manifolds can often represent system behavior with fewer degrees of freedom than linear subspaces while still approximating the full system accurately.\nLimitations of current ROMs (why nonlinear + why “smooth”) It’s tempting to think ROM is “solved” once you can compress snapshots well. The hard part is what happens online, when you traverse the reduced manifold according to the governing PDE.\n1) Linear subspace ROMs can be fundamentally inefficient Classical POD-ROM assumes solutions live near a linear subspace. This works best when the Kolmogorov $n$-width decays quickly (roughly: a small number of modes captures most variation). But in advection-dominated flows and other problems with slow Kolmogorov $n$-width decay, the number of POD modes required for accuracy can become large, reducing speedups and sometimes harming stability.\n2) Nonlinear ROMs often struggle where physics enters: derivatives Neural manifold ROMs (e.g., CAE-ROM) can achieve excellent compression, but online physics requires evaluating spatial operators: gradients, Laplacians, Jacobians of the decoder map, and so on. A recurring practical issue is that neural fields are not guaranteed to interpolate smoothly, and their spatial derivatives can be noisy. If your derivative estimates are wrong, your dynamics are wrong.\nMany nonlinear ROM pipelines handle this by introducing a coarse auxiliary grid and applying low-order finite differences for derivatives. That workaround can:\nrequire maintaining a background mesh (extra memory and engineering), introduce sensitivity to differencing hyperparameters, become brittle near boundaries, shocks, or sharp features. The punchline is: if you want to do physics-based online integration, you need reliable differentiation through your representation.\nSNF-ROM is designed around this constraint.\nWhere convolutional autoencoder ROMs struggle Convolutional autoencoder ROMs can compress state fields effectively, but the compression/decompression workflow does not directly control latent trajectory quality during online integration. In practice, this mismatch can produce drift and degraded physical consistency.\nThere are two coupled issues:\nThe learned manifold may be accurate at training snapshots but poorly behaved between them. The decoder’s derivatives (needed for dynamics) can be inaccurate or unstable. If the online stage uses the PDE to evolve the reduced state, small derivative errors can compound quickly.\nSNF-ROM: physics-based dynamics with neural fields SNF-ROM is a projection-based nonlinear ROM that aims to keep the best parts of classical ROMs — Galerkin projection and classical time integrators — while replacing linear subspaces with a modern, continuous neural field representation. The key idea is not just to compress the state, but to build a representation on which the governing physics can be evaluated reliably during deployment.\nTwo design choices are central.\n1. Constrained manifold formulation (trajectory regularity) In classical ROMs, the reduced state evolves in a low-dimensional coordinate system whose geometry is fixed by the chosen basis. In nonlinear ROMs, this coordinate system becomes a learned manifold. However, if this manifold is irregular or poorly behaved between training samples, trajectories computed during online integration can drift or become numerically unstable.\nSNF-ROM addresses this by explicitly modeling the reduced state as a smooth function of parameters and time. Conceptually, instead of learning isolated latent codes for snapshots, the model learns a continuous mapping\n$$ \\tilde{u} = \\tilde{u}(t, \\mu), $$\nwhere $\\tilde{u} \\in \\mathbb{R}^r$ is the reduced state. This encourages trajectories that vary smoothly as time evolves or parameters change. From a numerical perspective, this matters because online deployment involves stepping along this manifold using the governing PDE. If the manifold has sharp curvature or local irregularities, small integration errors can amplify. Enforcing trajectory regularity therefore improves stability and allows larger time steps during reduced-order integration.\n2. Neural field regularization (derivative fidelity) The second design choice focuses on the representation of the physical state itself. SNF-ROM uses a neural field decoder\n$$ u(x,t;\\mu) \\approx f_\\theta(x, \\tilde{u}(t,\\mu)), $$\nwhich maps spatial coordinates and reduced variables to field values. Because the governing PDE involves spatial operators (gradients, Laplacians, fluxes), the accuracy of these derivatives is critical. Standard neural decoders can produce visually accurate reconstructions while still having noisy or oscillatory derivatives, which leads to incorrect physics when evaluating operators.\nSNF-ROM therefore explicitly regularizes the neural field to encourage smoothness and suppress high-frequency artifacts. This improves derivative fidelity so that operators like\n$$ \\nabla u, \\quad \\Delta u, \\quad \\mathcal{L}(u) $$\ncan be computed using automatic differentiation. The key benefit is that PDE operators are evaluated directly on the learned representation without relying on auxiliary grids or finite-difference approximations. This makes the reduced model both grid-free and numerically stable.\nTogether, these two components are aimed at one goal: do physics on the learned manifold, not just reconstruction. The model is trained so that both the geometry of the manifold and the operators acting on it are well behaved during online simulation.\nA key technical point is differentiability quality. If derivatives of the learned field are noisy, PDE-consistent inference degrades because the projected dynamics depend directly on these derivatives. By enforcing smooth neural fields, SNF-ROM ensures that gradients and higher-order operators remain accurate and stable during integration.\nWhat SNF-ROM is doing (high level) SNF-ROM uses a neural field decoder that maps coordinates and a low-dimensional latent to a field value:\nNeural field (continuous decoder):\n$$u(\\mathbf{x}, t; \\boldsymbol{\\mu}) \\approx f_\\theta(\\mathbf{x}, \\tilde{u}(t,\\boldsymbol{\\mu}))$$\nReduced state: $\\tilde{u}(t,\\boldsymbol{\\mu}) \\in \\mathbb{R}^r$ is the low-dimensional ROM state.\nDuring the online stage, SNF-ROM evolves $\\tilde{u}(t)$ using projection-based dynamics, while evaluating the PDE operators through AD on the neural field. This is the “physics with neural fields” part: you can differentiate the representation to form operators needed for the reduced dynamics, and then integrate the resulting reduced ODE with standard time-stepping.\nSmoothness: two regularized variants The paper presents two regularized variants designed to suppress high-frequency artifacts in derivatives:\nSNFL-ROM: Lipschitz-style regularization to encourage smooth neural mappings. SNFW-ROM: weight regularization motivated by controlling oscillations that appear in derivative expressions (particularly relevant with sinusoidal activations). Both are designed to produce smooth neural fields whose derivatives match the underlying signal more closely, which improves robustness during deployment-time dynamics evaluation.\nResults 1D viscous Burgers SNF-ROM variants maintain latent trajectory agreement under online solves where baseline CAE-ROM trajectories deviate.\n1D Kuramoto–Sivashinsky SNF-ROM maintains low relative error, including at larger time steps where baselines degrade.\n2D viscous Burgers The method achieves strong accuracy–speed tradeoffs with substantial state-space compression and high practical speed-up.\nSpeedup via hyper-reduction A major practical benefit of SNF-ROM is the ability to dramatically reduce the cost of evaluating the governing equations through hyper-reduction. In a full-order model, evaluating the PDE residual requires computing operators over every spatial degree of freedom. For large simulations, this can involve hundreds of thousands or millions of points.\nIn SNF-ROM, the state is represented in a low-dimensional latent space, and projection allows the governing dynamics to be evaluated in this reduced coordinate system. Hyper-reduction further accelerates computation by evaluating nonlinear operators at a carefully selected subset of spatial locations rather than the full grid, while still preserving accuracy in the projected dynamics.\nThis combination leads to substantial speedups. In the reported experiment, a problem with roughly 500,000 spatial degrees of freedom is reduced to a latent system with just two variables, and the resulting reduced model achieves approximately 199× speedup compared to evaluating the full-order model. The key insight is that once the dominant dynamics are captured in a low-dimensional manifold, the cost of simulation is governed by the reduced dimension rather than the original spatial resolution, enabling large performance gains while maintaining accuracy.\nCore contributions (what to remember) SNF-ROM is best understood as a ROM framework that explicitly supports doing physics on a learned nonlinear manifold:\nA constrained manifold formulation that encourages smoothly varying reduced trajectories, which improves online stability and allows larger time steps in practice. Smooth neural field regularization that makes the representation reliably differentiable, enabling accurate spatial derivatives via automatic differentiation. Nonintrusive, grid-free dynamics evaluation that works directly with point-cloud data, avoiding auxiliary meshes and brittle finite-difference derivative hacks. Projection + classical integration: dynamics are evaluated in a physics-consistent way and integrated using standard time integrators, rather than relying purely on black-box latent dynamics predictors. Takeaways ROM quality depends not just on compression, but on controllable and stable online dynamics. If your ROM uses the PDE online, derivative fidelity is not optional. Smoothness of the representation matters. SNF-ROM shows that regularizing neural manifolds for differentiation + integration can materially improve robustness over standard autoencoder-based ROM pipelines. ","wordCount":"2087","inLanguage":"en","image":"https://github.com/vpuri3.png","datePublished":"2026-02-15T00:00:00-05:00","dateModified":"2026-02-15T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/"},"publisher":{"@type":"Organization","name":"Vedant Puri","logo":{"@type":"ImageObject","url":"https://github.githubassets.com/favicons/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://vpuri3.github.io/ accesskey=h title="Vedant Puri (Alt + H)">Vedant Puri</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://vpuri3.github.io/#featured-work title=Featured><span>Featured</span></a></li><li><a href=https://vpuri3.github.io/#research-themes title=Research><span>Research</span></a></li><li><a href=https://vpuri3.github.io/#open-source title="Open Source"><span>Open Source</span></a></li><li><a href=https://vpuri3.github.io/work/ title=Work><span>Work</span></a></li><li><a href=https://vpuri3.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://vpuri3.github.io/photography/ title=Photography><span>Photography</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://vpuri3.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://vpuri3.github.io/blog/>Blog</a></div><h1 class="post-title entry-hint-parent">From POD to neural manifolds: a modern take on reduced order modeling</h1><div class=post-description>From classical projection methods to smooth neural field ROMs</div><div class=post-meta><span title='2026-02-15 00:00:00 -0500 -0500'>February 15, 2026</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#from-pod-to-neural-manifolds aria-label="From POD to neural manifolds">From POD to neural manifolds</a></li><li><a href=#why-rom-matters-for-engineering-workflows aria-label="Why ROM matters for engineering workflows">Why ROM matters for engineering workflows</a></li><li><a href=#limitations-of-current-roms-why-nonlinear--why-smooth aria-label="Limitations of current ROMs (why nonlinear + why &ldquo;smooth&rdquo;)">Limitations of current ROMs (why nonlinear + why &ldquo;smooth&rdquo;)</a><ul><li><a href=#1-linear-subspace-roms-can-be-fundamentally-inefficient aria-label="1) Linear subspace ROMs can be fundamentally inefficient">1) Linear subspace ROMs can be fundamentally inefficient</a></li><li><a href=#2-nonlinear-roms-often-struggle-where-physics-enters-derivatives aria-label="2) Nonlinear ROMs often struggle where physics enters: derivatives">2) Nonlinear ROMs often struggle where physics enters: derivatives</a></li></ul></li><li><a href=#where-convolutional-autoencoder-roms-struggle aria-label="Where convolutional autoencoder ROMs struggle">Where convolutional autoencoder ROMs struggle</a></li><li><a href=#snf-rom-physics-based-dynamics-with-neural-fields aria-label="SNF-ROM: physics-based dynamics with neural fields">SNF-ROM: physics-based dynamics with neural fields</a><ul><li><a href=#1-constrained-manifold-formulation-trajectory-regularity aria-label="1. Constrained manifold formulation (trajectory regularity)">1. Constrained manifold formulation (trajectory regularity)</a></li><li><a href=#2-neural-field-regularization-derivative-fidelity aria-label="2. Neural field regularization (derivative fidelity)">2. Neural field regularization (derivative fidelity)</a></li></ul></li><li><a href=#what-snf-rom-is-doing-high-level aria-label="What SNF-ROM is doing (high level)">What SNF-ROM is doing (high level)</a></li><li><a href=#smoothness-two-regularized-variants aria-label="Smoothness: two regularized variants">Smoothness: two regularized variants</a></li><li><a href=#results aria-label=Results>Results</a><ul><li><a href=#1d-viscous-burgers aria-label="1D viscous Burgers">1D viscous Burgers</a></li><li><a href=#1d-kuramotosivashinsky aria-label="1D Kuramoto–Sivashinsky">1D Kuramoto–Sivashinsky</a></li><li><a href=#2d-viscous-burgers aria-label="2D viscous Burgers">2D viscous Burgers</a></li><li><a href=#speedup-via-hyper-reduction aria-label="Speedup via hyper-reduction">Speedup via hyper-reduction</a></li></ul></li><li><a href=#core-contributions-what-to-remember aria-label="Core contributions (what to remember)">Core contributions (what to remember)</a></li><li><a href=#takeaways aria-label=Takeaways>Takeaways</a></li></ul></div></details></div><div class=post-content><h2 id=from-pod-to-neural-manifolds>From POD to neural manifolds<a hidden class=anchor aria-hidden=true href=#from-pod-to-neural-manifolds>#</a></h2><p>Reduced-order modeling (ROM) is ultimately about preserving dominant system behavior with far fewer degrees of freedom. Classical projection methods, convolutional autoencoder ROMs, and modern neural-field formulations all target this same objective, but they make different assumptions about (i) how we represent the state and (ii) how we evolve it in time.</p><p>This post walks through that progression and focuses on <strong>Smooth Neural Field ROM (SNF-ROM)</strong>: a projection-based nonlinear ROM that uses <strong>continuous neural fields</strong> as the state representation and explicitly supports <strong>physics-based differentiation and time integration</strong> during deployment.</p><p>Preprint: <a href=https://arxiv.org/pdf/2405.14890>arXiv:2405.14890</a></p><p>J. Comp. Phys. paper: <a href=https://doi.org/10.1016/j.jcp.2025.113957>https://doi.org/10.1016/j.jcp.2025.113957</a></p><p>See my <a href="https://youtu.be/8h9EXJqQUi0?si=lweE4A-QYV-3fSV5">dissertation proposal talk</a> on this topic!</p><hr><h2 id=why-rom-matters-for-engineering-workflows>Why ROM matters for engineering workflows<a hidden class=anchor aria-hidden=true href=#why-rom-matters-for-engineering-workflows>#</a></h2><p>High-fidelity simulation is now central to modern engineering pipelines, from fluid mechanics to control and design.
However, high accuracy typically requires resolving many spatial and temporal scales, which makes PDE solves expensive. These costs become the bottleneck in workflows that require repeated evaluations, such as design-space exploration, optimization, and uncertainty quantification.</p><p><img alt="Repeated solves bottleneck downstream engineering workflows" loading=lazy src=/assets/blog/flare-post/slide-009.png></p><p>Machine learning offers a complementary approach to accelerating simulation. Instead of solving the full PDE from scratch every time, we can <strong>learn a surrogate model</strong> from data generated by high-fidelity simulations. Training such models can be computationally expensive, since they must observe many examples across parameter settings, geometries, and time horizons. But this cost is paid once offline. At deployment time, the learned model can evaluate new scenarios orders of magnitude faster than a traditional solver, making it attractive for design optimization, uncertainty quantification, control, and real-time decision-making workflows. In this sense, machine learning amortizes the cost of expensive PDE solves across many downstream queries.</p><p><img alt="ML signal representations can amortize repeated PDE solve costs" loading=lazy src=/assets/blog/flare-post/slide-014.png></p><p>A particularly important class of surrogates is <strong>reduced-order models (ROMs)</strong>. The central idea behind ROMs is that although PDE states live in extremely high-dimensional spaces (millions of degrees of freedom in realistic simulations), the <em>effective dynamics</em> often evolve on a much lower-dimensional structure. ROMs aim to discover this structure and use it to simulate the system using far fewer variables. Traditionally, this was done with linear subspace methods such as Proper Orthogonal Decomposition (POD) combined with Galerkin projection. Modern approaches extend this idea using machine learning to learn nonlinear representations that can better capture complex dynamics.</p><p>In this post, we focus on <strong>neural reduced-order modeling</strong>: learn a low-dimensional representation directly from data, then evolve that representation using physics-aware structure. Instead of predicting the full state everywhere, the model predicts a compact latent state that encodes the dominant behavior of the system, and the full field can be reconstructed from this latent representation. This separates the problem into two parts: representing the state efficiently and modeling how that representation evolves in time.</p><div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;border:1px solid var(--border);margin:1rem 0"><iframe src=https://slides.com/vedantpuri/vedant-puri-thesis-proposal/embed#/0/7/2 title="Neural ROM objective animation" frameborder=0 allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div><p>Before diving into SNF-ROM, it is helpful to recall the <strong>manifold learning viewpoint</strong> that underlies most nonlinear ROM methods. The core assumption is that although simulation states are extremely high-dimensional, they often lie close to a smooth, low-dimensional manifold embedded in the ambient space. For example, fluid flow snapshots over time may evolve primarily along a small number of coherent structures or modes. Learning this manifold provides a natural coordinate system in which the dynamics can be expressed compactly, and the role of a ROM is to learn both the geometry of this manifold and the dynamics that govern motion along it.</p><p>This perspective is illustrated by contrasting the <strong>full-order model (FOM)</strong> with linear and nonlinear ROMs. The governing PDE can be written abstractly as</p><p>$$
\frac{\partial u}{\partial t} = \mathcal{L}(x,t,u;\mu),
$$</p><p>where $u(x,t;\mu)$ is the high-dimensional state, $x$ is spatial position, $t$ is time, and $\mu$ denotes parameters. In a full discretization, the state is represented as</p><p>$$
u(x,t;\mu) \approx g_{\mathrm{FOM}}(x,\bar u(t;\mu)) = \Phi(x),\bar u(t;\mu),
$$</p><p>where $\Phi(x)$ is a spatial basis and $\bar u \in \mathbb{R}^{N_{\mathrm{FOM}}}$ are high-dimensional coefficients. A <strong>linear POD-ROM</strong> assumes the dynamics evolve in a low-dimensional linear subspace,</p><p>$$
\bar u(t;\mu) \approx \bar u_0 + \mathbf{P},\tilde u(t;\mu),
$$</p><p>where $\mathbf{P}$ is a projection matrix and $\tilde u \in \mathbb{R}^{N_{\mathrm{Lin\text{-}ROM}}}$ are reduced coordinates. A <strong>nonlinear ROM</strong> instead assumes the state lies on a curved manifold and uses a learned decoder,</p><p>$$
u(x,t;\mu) \approx g_{\mathrm{ROM}}(x,\tilde u(t;\mu)) = \mathrm{NN}_\theta(x,\tilde u(t;\mu)),
$$</p><p>where $\mathrm{NN}_\theta$ is a neural field mapping spatial coordinates and latent variables to the state. The dimensionality relationship</p><p>$$
N_{\mathrm{NI\text{-}ROM}} \le N_{\mathrm{Lin\text{-}ROM}} \ll N_{\mathrm{FOM}}
$$</p><p>captures the intuition that nonlinear manifolds can often represent system behavior with fewer degrees of freedom than linear subspaces while still approximating the full system accurately.</p><div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;border:1px solid var(--border);margin:1rem 0"><iframe src=https://slides.com/vedantpuri/vedant-puri-thesis-proposal/embed#/0/10/2 title="Background on manifold learning" frameborder=0 allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div><hr><h2 id=limitations-of-current-roms-why-nonlinear--why-smooth>Limitations of current ROMs (why nonlinear + why &ldquo;smooth&rdquo;)<a hidden class=anchor aria-hidden=true href=#limitations-of-current-roms-why-nonlinear--why-smooth>#</a></h2><p>It’s tempting to think ROM is “solved” once you can compress snapshots well. The hard part is what happens <strong>online</strong>, when you traverse the reduced manifold according to the governing PDE.</p><h3 id=1-linear-subspace-roms-can-be-fundamentally-inefficient>1) Linear subspace ROMs can be fundamentally inefficient<a hidden class=anchor aria-hidden=true href=#1-linear-subspace-roms-can-be-fundamentally-inefficient>#</a></h3><p>Classical POD-ROM assumes solutions live near a <em>linear</em> subspace. This works best when the Kolmogorov $n$-width decays quickly (roughly: a small number of modes captures most variation). But in <strong>advection-dominated flows</strong> and other problems with slow Kolmogorov $n$-width decay, the number of POD modes required for accuracy can become large, reducing speedups and sometimes harming stability.</p><h3 id=2-nonlinear-roms-often-struggle-where-physics-enters-derivatives>2) Nonlinear ROMs often struggle where physics enters: derivatives<a hidden class=anchor aria-hidden=true href=#2-nonlinear-roms-often-struggle-where-physics-enters-derivatives>#</a></h3><p>Neural manifold ROMs (e.g., CAE-ROM) can achieve excellent compression, but online physics requires evaluating spatial operators: gradients, Laplacians, Jacobians of the decoder map, and so on. A recurring practical issue is that <strong>neural fields are not guaranteed to interpolate smoothly</strong>, and their spatial derivatives can be noisy. If your derivative estimates are wrong, your dynamics are wrong.</p><p>Many nonlinear ROM pipelines handle this by introducing a coarse auxiliary grid and applying low-order finite differences for derivatives. That workaround can:</p><ul><li>require maintaining a background mesh (extra memory and engineering),</li><li>introduce sensitivity to differencing hyperparameters,</li><li>become brittle near boundaries, shocks, or sharp features.</li></ul><p>The punchline is: <strong>if you want to do physics-based online integration, you need reliable differentiation through your representation.</strong></p><p>SNF-ROM is designed around this constraint.</p><hr><h2 id=where-convolutional-autoencoder-roms-struggle>Where convolutional autoencoder ROMs struggle<a hidden class=anchor aria-hidden=true href=#where-convolutional-autoencoder-roms-struggle>#</a></h2><p>Convolutional autoencoder ROMs can compress state fields effectively, but the compression/decompression workflow does not directly control latent trajectory quality during online integration. In practice, this mismatch can produce drift and degraded physical consistency.</p><div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;border:1px solid var(--border);margin:1rem 0"><iframe src=https://slides.com/vedantpuri/vedant-puri-thesis-proposal/embed#/0/11/2 title="2D Burgers problem" frameborder=0 allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div><p>There are two coupled issues:</p><ol><li>The learned manifold may be accurate at training snapshots but poorly behaved between them.</li><li>The decoder’s derivatives (needed for dynamics) can be inaccurate or unstable.</li></ol><p>If the online stage uses the PDE to evolve the reduced state, small derivative errors can compound quickly.</p><hr><h2 id=snf-rom-physics-based-dynamics-with-neural-fields>SNF-ROM: physics-based dynamics with neural fields<a hidden class=anchor aria-hidden=true href=#snf-rom-physics-based-dynamics-with-neural-fields>#</a></h2><p>SNF-ROM is a projection-based nonlinear ROM that aims to keep the best parts of classical ROMs — Galerkin projection and classical time integrators — while replacing linear subspaces with a modern, continuous neural field representation. The key idea is not just to compress the state, but to build a representation on which the governing physics can be evaluated reliably during deployment.</p><p>Two design choices are central.</p><h3 id=1-constrained-manifold-formulation-trajectory-regularity>1. Constrained manifold formulation (trajectory regularity)<a hidden class=anchor aria-hidden=true href=#1-constrained-manifold-formulation-trajectory-regularity>#</a></h3><p>In classical ROMs, the reduced state evolves in a low-dimensional coordinate system whose geometry is fixed by the chosen basis. In nonlinear ROMs, this coordinate system becomes a learned manifold. However, if this manifold is irregular or poorly behaved between training samples, trajectories computed during online integration can drift or become numerically unstable.</p><p>SNF-ROM addresses this by explicitly modeling the reduced state as a smooth function of parameters and time. Conceptually, instead of learning isolated latent codes for snapshots, the model learns a continuous mapping</p><p>$$
\tilde{u} = \tilde{u}(t, \mu),
$$</p><p>where $\tilde{u} \in \mathbb{R}^r$ is the reduced state. This encourages trajectories that vary smoothly as time evolves or parameters change. From a numerical perspective, this matters because online deployment involves stepping along this manifold using the governing PDE. If the manifold has sharp curvature or local irregularities, small integration errors can amplify. Enforcing trajectory regularity therefore improves stability and allows larger time steps during reduced-order integration.</p><h3 id=2-neural-field-regularization-derivative-fidelity>2. Neural field regularization (derivative fidelity)<a hidden class=anchor aria-hidden=true href=#2-neural-field-regularization-derivative-fidelity>#</a></h3><p>The second design choice focuses on the representation of the physical state itself. SNF-ROM uses a neural field decoder</p><p>$$
u(x,t;\mu) \approx f_\theta(x, \tilde{u}(t,\mu)),
$$</p><p>which maps spatial coordinates and reduced variables to field values. Because the governing PDE involves spatial operators (gradients, Laplacians, fluxes), the accuracy of these derivatives is critical. Standard neural decoders can produce visually accurate reconstructions while still having noisy or oscillatory derivatives, which leads to incorrect physics when evaluating operators.</p><p>SNF-ROM therefore explicitly regularizes the neural field to encourage smoothness and suppress high-frequency artifacts. This improves derivative fidelity so that operators like</p><p>$$
\nabla u, \quad \Delta u, \quad \mathcal{L}(u)
$$</p><p>can be computed using automatic differentiation. The key benefit is that PDE operators are evaluated directly on the learned representation without relying on auxiliary grids or finite-difference approximations. This makes the reduced model both grid-free and numerically stable.</p><p>Together, these two components are aimed at one goal: <strong>do physics on the learned manifold</strong>, not just reconstruction. The model is trained so that both the geometry of the manifold and the operators acting on it are well behaved during online simulation.</p><p><img alt="SNF-ROM workflow: direct latent trajectory modeling with smooth neural fields" loading=lazy src=/assets/blog/flare-post/slide-038.png></p><p>A key technical point is differentiability quality. If derivatives of the learned field are noisy, PDE-consistent inference degrades because the projected dynamics depend directly on these derivatives. By enforcing smooth neural fields, SNF-ROM ensures that gradients and higher-order operators remain accurate and stable during integration.</p><p><img alt="Accurate derivatives require smooth neural field representations" loading=lazy src=/assets/blog/flare-post/slide-043.png></p><hr><h2 id=what-snf-rom-is-doing-high-level>What SNF-ROM is doing (high level)<a hidden class=anchor aria-hidden=true href=#what-snf-rom-is-doing-high-level>#</a></h2><p>SNF-ROM uses a neural field decoder that maps coordinates and a low-dimensional latent to a field value:</p><ul><li><p><strong>Neural field (continuous decoder):</strong><br>$$u(\mathbf{x}, t; \boldsymbol{\mu}) \approx f_\theta(\mathbf{x}, \tilde{u}(t,\boldsymbol{\mu}))$$</p></li><li><p><strong>Reduced state:</strong> $\tilde{u}(t,\boldsymbol{\mu}) \in \mathbb{R}^r$ is the low-dimensional ROM state.</p></li></ul><p>During the online stage, SNF-ROM evolves $\tilde{u}(t)$ using projection-based dynamics, while evaluating the PDE operators through <strong>AD on the neural field</strong>. This is the “physics with neural fields” part: you can differentiate the representation to form operators needed for the reduced dynamics, and then integrate the resulting reduced ODE with standard time-stepping.</p><p><img alt=paper-fig-2 loading=lazy src=/assets/blog/snf-rom-post/SNF-ROM-summary.png></p><hr><h2 id=smoothness-two-regularized-variants>Smoothness: two regularized variants<a hidden class=anchor aria-hidden=true href=#smoothness-two-regularized-variants>#</a></h2><p>The paper presents two regularized variants designed to suppress high-frequency artifacts in derivatives:</p><ul><li><strong>SNFL-ROM:</strong> Lipschitz-style regularization to encourage smooth neural mappings.</li><li><strong>SNFW-ROM:</strong> weight regularization motivated by controlling oscillations that appear in derivative expressions (particularly relevant with sinusoidal activations).</li></ul><p>Both are designed to produce smooth neural fields whose derivatives match the underlying signal more closely, which improves robustness during deployment-time dynamics evaluation.</p><hr><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><h3 id=1d-viscous-burgers>1D viscous Burgers<a hidden class=anchor aria-hidden=true href=#1d-viscous-burgers>#</a></h3><p>SNF-ROM variants maintain latent trajectory agreement under online solves where baseline CAE-ROM trajectories deviate.</p><div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;border:1px solid var(--border);margin:1rem 0"><iframe src=https://slides.com/vedantpuri/vedant-puri-thesis-proposal/embed#/0/14/0 title="1D Kuramoto Sivashinsky" frameborder=0 allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div><h3 id=1d-kuramotosivashinsky>1D Kuramoto–Sivashinsky<a hidden class=anchor aria-hidden=true href=#1d-kuramotosivashinsky>#</a></h3><p>SNF-ROM maintains low relative error, including at larger time steps where baselines degrade.</p><div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;border:1px solid var(--border);margin:1rem 0"><iframe src=https://slides.com/vedantpuri/vedant-puri-thesis-proposal/embed#/0/15/1 title="2D Burgers problem" frameborder=0 allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div><h3 id=2d-viscous-burgers>2D viscous Burgers<a hidden class=anchor aria-hidden=true href=#2d-viscous-burgers>#</a></h3><p>The method achieves strong accuracy–speed tradeoffs with substantial state-space compression and high practical speed-up.</p><div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;border:1px solid var(--border);margin:1rem 0"><iframe src=https://slides.com/vedantpuri/vedant-puri-thesis-proposal/embed#/0/16/0 title="2D Burgers problem" frameborder=0 allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div><h3 id=speedup-via-hyper-reduction>Speedup via hyper-reduction<a hidden class=anchor aria-hidden=true href=#speedup-via-hyper-reduction>#</a></h3><p>A major practical benefit of SNF-ROM is the ability to dramatically reduce the cost of evaluating the governing equations through hyper-reduction. In a full-order model, evaluating the PDE residual requires computing operators over every spatial degree of freedom. For large simulations, this can involve hundreds of thousands or millions of points.</p><p>In SNF-ROM, the state is represented in a low-dimensional latent space, and projection allows the governing dynamics to be evaluated in this reduced coordinate system. Hyper-reduction further accelerates computation by evaluating nonlinear operators at a carefully selected subset of spatial locations rather than the full grid, while still preserving accuracy in the projected dynamics.</p><p>This combination leads to substantial speedups. In the reported experiment, a problem with roughly <strong>500,000 spatial degrees of freedom</strong> is reduced to a latent system with just <strong>two variables</strong>, and the resulting reduced model achieves approximately <strong>199× speedup</strong> compared to evaluating the full-order model. The key insight is that once the dominant dynamics are captured in a low-dimensional manifold, the cost of simulation is governed by the reduced dimension rather than the original spatial resolution, enabling large performance gains while maintaining accuracy.</p><p><img alt=timings loading=lazy src=/assets/blog/snf-rom-post/timings_burgers_2D.png></p><hr><h2 id=core-contributions-what-to-remember>Core contributions (what to remember)<a hidden class=anchor aria-hidden=true href=#core-contributions-what-to-remember>#</a></h2><p>SNF-ROM is best understood as a ROM framework that explicitly supports <em>doing physics</em> on a learned nonlinear manifold:</p><ol><li><strong>A constrained manifold formulation</strong> that encourages smoothly varying reduced trajectories, which improves online stability and allows larger time steps in practice.</li><li><strong>Smooth neural field regularization</strong> that makes the representation reliably differentiable, enabling accurate spatial derivatives via automatic differentiation.</li><li><strong>Nonintrusive, grid-free dynamics evaluation</strong> that works directly with point-cloud data, avoiding auxiliary meshes and brittle finite-difference derivative hacks.</li><li><strong>Projection + classical integration</strong>: dynamics are evaluated in a physics-consistent way and integrated using standard time integrators, rather than relying purely on black-box latent dynamics predictors.</li></ol><hr><h2 id=takeaways>Takeaways<a hidden class=anchor aria-hidden=true href=#takeaways>#</a></h2><ol><li>ROM quality depends not just on compression, but on <strong>controllable and stable online dynamics</strong>.</li><li>If your ROM uses the PDE online, <strong>derivative fidelity is not optional</strong>. Smoothness of the representation matters.</li><li>SNF-ROM shows that regularizing neural manifolds for <strong>differentiation + integration</strong> can materially improve robustness over standard autoencoder-based ROM pipelines.</li></ol><hr></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://vpuri3.github.io/>Vedant Puri</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>