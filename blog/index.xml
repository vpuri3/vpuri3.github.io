<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Blog on Vedant Puri</title><link>https://vpuri3.github.io/blog/</link><description>Recent content in Blog on Vedant Puri</description><image><title>Vedant Puri</title><url>https://github.com/vpuri3.png</url><link>https://github.com/vpuri3.png</link></image><generator>Hugo -- 0.155.3</generator><language>en-us</language><lastBuildDate>Wed, 18 Feb 2026 00:00:00 -0500</lastBuildDate><atom:link href="https://vpuri3.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention</title><link>https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/</link><pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate><guid>https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/</guid><description>How FLARE evolves from bidirectional encoder attention to scalable causal decoder-only training and inference.</description></item><item><title>Triple Attention in Triton: Building a Third-Order Memory in Linear Time</title><link>https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/</link><pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate><guid>https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/</guid><description>Designing and implementing third-order attention with Triton kernels and linear-time sequence scaling.</description></item><item><title>Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing</title><link>https://vpuri3.github.io/blog/adventures-in-high-order-attention/</link><pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate><guid>https://vpuri3.github.io/blog/adventures-in-high-order-attention/</guid><description>A practical tour of linear attention, its bottlenecks, and why multilinear / simplex-style attention might help</description></item><item><title>Scaling attention to 1M tokens on a single GPU</title><link>https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/</link><pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate><guid>https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/</guid><description>The story of FLARE: Fast Low-rank Attention Routing Engine</description></item><item><title>From POD to neural manifolds: a modern take on reduced order modeling</title><link>https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/</link><pubDate>Sun, 15 Feb 2026 00:00:00 -0500</pubDate><guid>https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/</guid><description>From classical projection methods to smooth neural field ROMs</description></item><item><title>Our job as computational engineers</title><link>https://vpuri3.github.io/blog/thoughts-archive/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://vpuri3.github.io/blog/thoughts-archive/</guid><description>Collected long-form notes and technical reflections.</description></item></channel></rss>