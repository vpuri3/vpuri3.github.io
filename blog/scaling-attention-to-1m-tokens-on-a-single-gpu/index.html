<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scaling attention to 1M tokens on a single GPU | Vedant Puri</title><meta name=keywords content><meta name=description content="The story of FLARE: Fast Low-rank Attention Routing Engine"><meta name=author content><link rel=canonical href=https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/><link crossorigin=anonymous href=/assets/css/stylesheet.72a547024d24c325f3bae9b2bb62cea7a569c0893d3ff900db35fa11b5fe740d.css integrity="sha256-cqVHAk0kwyXzuumyu2LOp6VpwIk9P/kA2zX6EbX+dA0=" rel="preload stylesheet" as=style><link rel=icon href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://github.githubassets.com/favicons/favicon.png><link rel=apple-touch-icon href=https://github.com/vpuri3.png><link rel=mask-icon href=https://github.githubassets.com/favicons/favicon.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/"><meta property="og:site_name" content="Vedant Puri"><meta property="og:title" content="Scaling attention to 1M tokens on a single GPU"><meta property="og:description" content="The story of FLARE: Fast Low-rank Attention Routing Engine"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-16T00:00:00-05:00"><meta property="article:modified_time" content="2026-02-16T00:00:00-05:00"><meta property="og:image" content="https://github.com/vpuri3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/vpuri3.png"><meta name=twitter:title content="Scaling attention to 1M tokens on a single GPU"><meta name=twitter:description content="The story of FLARE: Fast Low-rank Attention Routing Engine"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://vpuri3.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Scaling attention to 1M tokens on a single GPU","item":"https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scaling attention to 1M tokens on a single GPU","name":"Scaling attention to 1M tokens on a single GPU","description":"The story of FLARE: Fast Low-rank Attention Routing Engine","keywords":[],"articleBody":"The story of FLARE: Fast Low-rank Attention Routing Engine Attention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.\nRather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?\nThis post complements our latest paper (arXiv:2508.12594). See my dissertation proposal talk on this topic!\nWhy this problem is hard Attention is now used across language, vision, and scientific machine learning. The scaling bottleneck appears everywhere. For a sequence of $N$ tokens, standard self-attention requires $O(N^2)$ compute and memory, which quickly becomes impractical beyond tens of thousands of tokens.\nWe are particularly motivated by partial differential equation (PDE) surrogate modeling, one of the most demanding settings for attention:\nInputs are large and geometry dependent. Meshes are often unstructured, so ordering tricks are unreliable. Accurate predictions require long-range interactions across the domain. Linear attention methods reduce cost but often struggle to match accuracy in these regimes. The challenge is not just scaling attention, but scaling it without losing expressive global communication.\nPDE surrogate modeling For a PDE\n$$ \\mathcal{L}(\\boldsymbol{x}, t, \\boldsymbol{u}; \\boldsymbol{\\mu}), \\quad \\boldsymbol{x}\\in\\Omega, $$ we view learning as approximating a solution operator \\(\\mathcal{G}\\) that maps parameters to fields: $$ \\boldsymbol{u}(\\boldsymbol{x})=\\mathcal{G}(\\boldsymbol{\\mu})(\\boldsymbol{x}). $$ In practice, we train on many \\(\\boldsymbol{\\mu}\\to \\boldsymbol{u}\\) pairs and require generalization to unseen \\(\\boldsymbol{\\mu}\\). In the slide below, \\(\\boldsymbol{\\mu}\\) is effectively the geometry \\(\\Omega\\), and the target is a strain field on unseen domains. Token mixing in transformers A transformer block is conceptually two operations:\npointwise nonlinear transforms (MLPs), global message passing (self-attention). For PDE data, we avoid token clustering heuristics and treat each mesh point as a token. Standard attention computes, for each output token, a distribution over all inputs:\n$$ y_n = \\sum_m \\frac{\\exp\\left(q_n^T \\cdot k_l \\right)}{\\sum_l \\exp \\left(q_n^\\top \\cdot k_l \\right)} v_l $$\nThis all-to-all routing is powerful but expensive. At million-token scale, even a single forward-backward pass can take tens of seconds. The goal is to keep global communication while reducing the number of messages that must be exchanged.\nWhy global communication still matters In many physical systems, forward operators are local, but solution operators are effectively dense. A model that communicates only locally will miss long-range dependencies that determine the final field.\nHowever, physical signals are often smooth at resolved scales. Smoothness implies redundancy: nearby tokens frequently carry similar information for distant targets. This suggests we may not need to send separate messages from every token if we can aggregate similar ones.\nThis observation motivates a low-rank communication view of attention.\nAre $N^2$ messages really necessary? Physical fields are usually smooth at resolved scales. Smoothness means many high-frequency modes are weak; in a fully resolved signal, there is redundancy. So if two nearby source points carry nearly the same information for a far target point, we should be able to route one combined message instead of two separate ones.\nThe core FLARE contribution is to operationalize this physics-guided redundancy into a sequence-agnostic attention mechanism that remains general-purpose.\nFLARE How we got to FLARE The starting point was a simple systems question: what actually makes attention expensive?\nAttention can be interpreted as a routing mechanism where each token decides how to combine information from all others. The quadratic cost comes from explicitly computing all pairwise interactions, even when many of those interactions are redundant.\nFrom a physics perspective, many solution operators exhibit low effective dimensionality. Spectral decay in physical fields suggests that long-range interactions can often be captured through a smaller set of communication pathways.\nThis led to the idea of introducing a small set of latent routing tokens that act as intermediaries. Instead of every token talking to every other token directly, tokens communicate through these shared hubs. The challenge was designing this mechanism so that it remains global, interpretable, and easy to optimize.\nFLARE Method FLARE implements global communication through a two-step gather–scatter mechanism, but it does so using standard, highly optimized attention primitives.\nScaled dot-product attention (SDPA) The basic attention primitive is scaled dot-product attention. Given queries $Q \\in \\mathbb{R}^{N_q \\times d}$, keys $K \\in \\mathbb{R}^{N_k \\times d}$, and values $V \\in \\mathbb{R}^{N_k \\times d_v}$, attention is\n$$ \\mathrm{SPDA}(Q,K,V) = \\mathrm{softmax}!\\left(\\frac{Q \\cdot K^\\top}{\\sqrt{d}}\\right)V. $$\nNaively, this suggests computing the score matrix $S = Q \\cdot K^\\top \\in \\mathbb{R}^{N_q \\times N_k}$, applying softmax, and then multiplying by $V$. That approach is expensive because it materializes an $N_q \\times N_k$ matrix.\nModern frameworks provide SDPA as a fused kernel that computes the same result without materializing the full attention-weight matrix in memory. In PyTorch, torch.nn.functional.scaled_dot_product_attention is the standard entry point, and under the hood it can dispatch to fused implementations (for example FlashAttention-style kernels) that stream over blocks of $K,V$ and maintain the softmax normalization online. Practically, SDPA lets you use the standard attention math while keeping memory use close to $O((L_q+L_k)d)$ rather than $O(L_qL_k)$.\nFLARE is built to use SDPA twice: once to gather information into a small latent set, and once to scatter it back.\nStep 1: Gather (encode) Introduce $M \\ll N$ latent tokens per head. Let $Q_h \\in \\mathbb{R}^{M \\times d}$ be latent queries, and let $K_h, V_h \\in \\mathbb{R}^{N \\times d}$ be token keys/values of head $h$. The gather step pools from the $N$ tokens into $M$ latents:\n$$ Z_h = \\mathrm{SDPA}(Q_h, K_h, V_h). $$\nIn matrix notation this is\n$$ W_{\\mathrm{enc}} = \\mathrm{softmax}(Q_h \\cdot K_h^\\top) \\in \\mathbb{R}^{M \\times N}, \\qquad Z_h = W_{\\mathrm{enc}} \\cdot V_h. $$\nImportantly, although $W_{\\mathrm{enc}}$ is a useful conceptual object, SDPA computes $Z_h$ without explicitly materializing $W_{\\mathrm{enc}}$.\nStep 2: Scatter (decode) Next, the latents broadcast information back to all $N$ tokens using the reverse affinity pattern:\n$$ Y_h = \\mathrm{SDPA}(K_h, Q_h, Z_h). $$\nIn matrix form,\n$$ W_{\\mathrm{dec}} = \\mathrm{softmax}(K_h \\cdot Q_h^\\top) \\in \\mathbb{R}^{N \\times M}, \\qquad Y_h = W_{\\mathrm{dec}} \\cdot Z_h. $$\nCombining both steps gives an implicit global communication operator:\n$$ Y_h = (W_{\\mathrm{dec}} \\cdot W_{\\mathrm{enc}}) \\cdot V_h. $$\nThe product $W_{\\mathrm{dec}} \\cdot W_{\\mathrm{enc}} \\in \\mathbb{R}^{N \\times N}$ is a dense global mixing matrix, but its rank is at most $M$. FLARE therefore achieves global communication at $O(NM)$ cost per head (with $M \\ll N$), and the SDPA kernels avoid ever forming the intermediate $N \\times M$ or $M \\times N$ attention matrices in memory.\nMinimal implementation Below is the core mixer expressed directly with PyTorch SDPA. This is essentially the gather–scatter operator in a few lines.\nfrom torch.nn.functional import scaled_dot_product_attention as SDPA def flare_multihead_mixer(Q, K, V): \"\"\" Args: Q: [H, M, D] latent queries (per head) K: [B, H, N, D] token keys V: [B, H, N, D] token values Returns: Y: [B, H, N, D] mixed token outputs \"\"\" Qb = Q.unsqueeze(0) # [1, H, M, D] to match SDPA batching # Gather: tokens -\u003e latents Z = SDPA(Qb, K, V, scale=1.0) # [B, H, M, D] # Scatter: latents -\u003e tokens Y = SDPA(K, Qb, Z, scale=1.0) # [B, H, N, D] return Y The full FLARE block stacks this mixer with standard projection/MLP components: And here are the scaling results: Results FLARE enables million-token training on a single GPU while maintaining strong accuracy on PDE surrogate tasks. The key improvement is not only asymptotic scaling but a practical runtime and memory profile that allows end-to-end training.\nOn sequence benchmarks such as Long Range Arena, the operator remains competitive with other efficient transformer approaches, showing that the mechanism is broadly applicable beyond scientific data.\nOn sequence benchmarks, such as Long Range Arena, FLARE is competitive with efficient-transformer alternatives and maintains strong average accuracy, showing that the method is not restricted to PDE-only structure.\nDeep dive: why this design works Understanding why FLARE works requires looking at the structure of the gather–scatter operator and how different design choices affect learning dynamics.\nGather–scatter as communication hubs Each latent token acts as both a pooling hub and a routing hub. During encoding, it aggregates information from tokens that align with its query pattern. During decoding, it distributes that information back to tokens that match its key pattern. This creates a contraction–expansion communication structure that efficiently mixes global information.\nSymmetric operators improve stability The encode and decode steps are derived from the same query-key geometry, creating a structurally aligned operator. This symmetry reduces redundant parameterization and leads to more stable optimization compared to asymmetric variants.\nFixed queries provide a stable routing structure Latent queries are learned but input independent. This gives a consistent communication basis across inputs, making the operator easier to optimize and interpret. The reduced query dynamism is compensated by expressive key and value encoders, which adapt routing patterns to each input.\nRepeated global mixing is more effective than latent refinement Empirically, allocating compute to repeated gather–scatter operations produces better performance than stacking heavy latent self-attention blocks. The benefit comes from repeatedly mixing global information rather than refining latent representations in isolation.\nIndependent latents across heads increase diversity Allowing each head to have its own latent tokens produces more diverse communication pathways. Different heads learn complementary routing structures, improving accuracy and depth scaling.\nPractical takeaways Several design principles emerge:\nUse explicit low-rank communication rather than approximating local attention. Allocate compute to repeated global mixing. Keep routing pathways independent across heads. Use expressive key and value encoders when queries are fixed. Closing thoughts FLARE shows that scaling attention is not only about approximating softmax more efficiently. By rethinking attention as a communication operator, we can design mechanisms that preserve global information flow while dramatically reducing cost.\nThe broader lesson is that many large-scale problems have structure that can be exploited. Low-rank communication is one way to capture that structure without sacrificing flexibility, opening the door to practical million-token models on modest hardware.\nReferences Puri, V. et al. FLARE: Fast Low-rank Attention Routing Engine. arXiv (2025). https://arxiv.org/abs/2508.12594 Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Dao, T. et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS (2022). https://arxiv.org/abs/2205.14135 PyTorch Docs. torch.nn.functional.scaled_dot_product_attention. https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html Tay, Y. et al. Long Range Arena: A Benchmark for Efficient Transformers. ICLR (2021). https://arxiv.org/abs/2011.04006 ","wordCount":"1707","inLanguage":"en","image":"https://github.com/vpuri3.png","datePublished":"2026-02-16T00:00:00-05:00","dateModified":"2026-02-16T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/"},"publisher":{"@type":"Organization","name":"Vedant Puri","logo":{"@type":"ImageObject","url":"https://github.githubassets.com/favicons/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://vpuri3.github.io/ accesskey=h title="Vedant Puri (Alt + H)">Vedant Puri</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://vpuri3.github.io/#featured-work title=Featured><span>Featured</span></a></li><li><a href=https://vpuri3.github.io/#research-themes title=Research><span>Research</span></a></li><li><a href=https://vpuri3.github.io/#open-source title="Open Source"><span>Open Source</span></a></li><li><a href=https://vpuri3.github.io/work/ title=Work><span>Work</span></a></li><li><a href=https://vpuri3.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://vpuri3.github.io/photography/ title=Photography><span>Photography</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://vpuri3.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://vpuri3.github.io/blog/>Blog</a></div><h1 class="post-title entry-hint-parent">Scaling attention to 1M tokens on a single GPU</h1><div class=post-description>The story of FLARE: Fast Low-rank Attention Routing Engine</div><div class=post-meta><span title='2026-02-16 00:00:00 -0500 -0500'>February 16, 2026</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-story-of-flare-fast-low-rank-attention-routing-engine aria-label="The story of FLARE: Fast Low-rank Attention Routing Engine">The story of FLARE: Fast Low-rank Attention Routing Engine</a></li><li><a href=#why-this-problem-is-hard aria-label="Why this problem is hard">Why this problem is hard</a></li><li><a href=#pde-surrogate-modeling aria-label="PDE surrogate modeling">PDE surrogate modeling</a></li><li><a href=#token-mixing-in-transformers aria-label="Token mixing in transformers">Token mixing in transformers</a><ul><li><a href=#why-global-communication-still-matters aria-label="Why global communication still matters">Why global communication still matters</a></li><li><a href=#are-n2-messages-really-necessary aria-label="Are $N^2$ messages really necessary?">Are $N^2$ messages really necessary?</a></li></ul></li><li><a href=#flare aria-label=FLARE>FLARE</a><ul><li><a href=#how-we-got-to-flare aria-label="How we got to FLARE">How we got to FLARE</a></li><li><a href=#flare-method aria-label="FLARE Method">FLARE Method</a><ul><li><a href=#scaled-dot-product-attention-sdpa aria-label="Scaled dot-product attention (SDPA)">Scaled dot-product attention (SDPA)</a></li><li><a href=#step-1-gather-encode aria-label="Step 1: Gather (encode)">Step 1: Gather (encode)</a></li><li><a href=#step-2-scatter-decode aria-label="Step 2: Scatter (decode)">Step 2: Scatter (decode)</a></li><li><a href=#minimal-implementation aria-label="Minimal implementation">Minimal implementation</a></li></ul></li></ul></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#deep-dive-why-this-design-works aria-label="Deep dive: why this design works">Deep dive: why this design works</a><ul><li><a href=#gatherscatter-as-communication-hubs aria-label="Gather–scatter as communication hubs">Gather–scatter as communication hubs</a></li><li><a href=#symmetric-operators-improve-stability aria-label="Symmetric operators improve stability">Symmetric operators improve stability</a></li><li><a href=#fixed-queries-provide-a-stable-routing-structure aria-label="Fixed queries provide a stable routing structure">Fixed queries provide a stable routing structure</a></li><li><a href=#repeated-global-mixing-is-more-effective-than-latent-refinement aria-label="Repeated global mixing is more effective than latent refinement">Repeated global mixing is more effective than latent refinement</a></li><li><a href=#independent-latents-across-heads-increase-diversity aria-label="Independent latents across heads increase diversity">Independent latents across heads increase diversity</a></li></ul></li><li><a href=#practical-takeaways aria-label="Practical takeaways">Practical takeaways</a></li><li><a href=#closing-thoughts aria-label="Closing thoughts">Closing thoughts</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=the-story-of-flare-fast-low-rank-attention-routing-engine>The story of FLARE: Fast Low-rank Attention Routing Engine<a hidden class=anchor aria-hidden=true href=#the-story-of-flare-fast-low-rank-attention-routing-engine>#</a></h2><p>Attention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.</p><p>Rather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?</p><p>This post complements our latest paper (<a href=https://arxiv.org/abs/2508.12594>arXiv:2508.12594</a>).
See my <a href="https://youtu.be/8h9EXJqQUi0?si=lweE4A-QYV-3fSV5&amp;t=1176">dissertation proposal talk</a> on this topic!</p><p><img alt="FLARE overview" loading=lazy src=/assets/blog/flare-post/FLARE.png></p><hr><h2 id=why-this-problem-is-hard>Why this problem is hard<a hidden class=anchor aria-hidden=true href=#why-this-problem-is-hard>#</a></h2><p>Attention is now used across language, vision, and scientific machine learning. The scaling bottleneck appears everywhere. For a sequence of $N$ tokens, standard self-attention requires $O(N^2)$ compute and memory, which quickly becomes impractical beyond tens of thousands of tokens.</p><p>We are particularly motivated by partial differential equation (PDE) surrogate modeling, one of the most demanding settings for attention:</p><ul><li>Inputs are large and geometry dependent.</li><li>Meshes are often unstructured, so ordering tricks are unreliable.</li><li>Accurate predictions require long-range interactions across the domain.</li></ul><p>Linear attention methods reduce cost but often struggle to match accuracy in these regimes. The challenge is not just scaling attention, but scaling it without losing expressive global communication.</p><hr><h2 id=pde-surrogate-modeling>PDE surrogate modeling<a hidden class=anchor aria-hidden=true href=#pde-surrogate-modeling>#</a></h2><p>For a PDE</p><div>$$
\mathcal{L}(\boldsymbol{x}, t, \boldsymbol{u}; \boldsymbol{\mu}), \quad \boldsymbol{x}\in\Omega,
$$</div>we view learning as approximating a solution operator \(\mathcal{G}\) that maps parameters to fields:<div>$$
\boldsymbol{u}(\boldsymbol{x})=\mathcal{G}(\boldsymbol{\mu})(\boldsymbol{x}).
$$</div>In practice, we train on many \(\boldsymbol{\mu}\to \boldsymbol{u}\) pairs and require generalization to unseen \(\boldsymbol{\mu}\). In the slide below, \(\boldsymbol{\mu}\) is effectively the geometry \(\Omega\), and the target is a strain field on unseen domains.<p><img alt="Slide 15: PDE surrogate motivation" loading=lazy src=/assets/blog/flare-post/slide-057.png></p><h2 id=token-mixing-in-transformers>Token mixing in transformers<a hidden class=anchor aria-hidden=true href=#token-mixing-in-transformers>#</a></h2><p>A transformer block is conceptually two operations:</p><ol><li>pointwise nonlinear transforms (MLPs),</li><li>global message passing (self-attention).</li></ol><p><img alt="Slide 16: Transformer ingredients for PDEs" loading=lazy src=/assets/blog/flare-post/slide-060.png></p><p>For PDE data, we avoid token clustering heuristics and treat each mesh point as a token. Standard attention computes, for each output token, a distribution over all inputs:</p><p>$$
y_n =
\sum_m \frac{\exp\left(q_n^T \cdot k_l \right)}{\sum_l \exp \left(q_n^\top \cdot k_l \right)} v_l
$$</p><p>This all-to-all routing is powerful but expensive. At million-token scale, even a single forward-backward pass can take tens of seconds. The goal is to keep global communication while reducing the number of messages that must be exchanged.</p><p><img alt="Slide 17: Quadratic self-attention bottleneck" loading=lazy src=/assets/blog/flare-post/slide-063.png></p><h3 id=why-global-communication-still-matters>Why global communication still matters<a hidden class=anchor aria-hidden=true href=#why-global-communication-still-matters>#</a></h3><p>In many physical systems, forward operators are local, but solution operators are effectively dense. A model that communicates only locally will miss long-range dependencies that determine the final field.</p><p>However, physical signals are often smooth at resolved scales. Smoothness implies redundancy: nearby tokens frequently carry similar information for distant targets. This suggests we may not need to send separate messages from every token if we can aggregate similar ones.</p><p>This observation motivates a low-rank communication view of attention.</p><p><img alt="Slide 18: Sparse forward operator vs dense solution operator" loading=lazy src=/assets/blog/flare-post/slide-067.png></p><h3 id=are-n2-messages-really-necessary>Are $N^2$ messages really necessary?<a hidden class=anchor aria-hidden=true href=#are-n2-messages-really-necessary>#</a></h3><p>Physical fields are usually smooth at resolved scales. Smoothness means many high-frequency modes are weak; in a fully resolved signal, there is redundancy. So if two nearby source points carry nearly the same information for a far target point, we should be able to route one combined message instead of two separate ones.</p><p><img alt="Navier-Stokes spectrum" loading=lazy src=/assets/blog/flare-post/navier_spectrum.png></p><p>The core FLARE contribution is to operationalize this physics-guided redundancy into a sequence-agnostic attention mechanism that remains general-purpose.</p><hr><h2 id=flare>FLARE<a hidden class=anchor aria-hidden=true href=#flare>#</a></h2><h3 id=how-we-got-to-flare>How we got to FLARE<a hidden class=anchor aria-hidden=true href=#how-we-got-to-flare>#</a></h3><p>The starting point was a simple systems question: what actually makes attention expensive?</p><p>Attention can be interpreted as a routing mechanism where each token decides how to combine information from all others. The quadratic cost comes from explicitly computing all pairwise interactions, even when many of those interactions are redundant.</p><p>From a physics perspective, many solution operators exhibit low effective dimensionality. Spectral decay in physical fields suggests that long-range interactions can often be captured through a smaller set of communication pathways.</p><p>This led to the idea of introducing a small set of latent routing tokens that act as intermediaries. Instead of every token talking to every other token directly, tokens communicate through these shared hubs. The challenge was designing this mechanism so that it remains global, interpretable, and easy to optimize.</p><h3 id=flare-method>FLARE Method<a hidden class=anchor aria-hidden=true href=#flare-method>#</a></h3><p>FLARE implements global communication through a two-step <strong>gather–scatter</strong> mechanism, but it does so using standard, highly optimized attention primitives.</p><h4 id=scaled-dot-product-attention-sdpa>Scaled dot-product attention (SDPA)<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention-sdpa>#</a></h4><p>The basic attention primitive is <strong>scaled dot-product attention</strong>. Given queries $Q \in \mathbb{R}^{N_q \times d}$, keys $K \in \mathbb{R}^{N_k \times d}$, and values $V \in \mathbb{R}^{N_k \times d_v}$, attention is</p><p>$$ \mathrm{SPDA}(Q,K,V) = \mathrm{softmax}!\left(\frac{Q \cdot K^\top}{\sqrt{d}}\right)V.
$$</p><p>Naively, this suggests computing the score matrix $S = Q \cdot K^\top \in \mathbb{R}^{N_q \times N_k}$, applying softmax, and then multiplying by $V$. That approach is expensive because it materializes an $N_q \times N_k$ matrix.</p><p>Modern frameworks provide <strong>SDPA</strong> as a fused kernel that computes the same result without materializing the full attention-weight matrix in memory. In PyTorch, <code>torch.nn.functional.scaled_dot_product_attention</code> is the standard entry point, and under the hood it can dispatch to fused implementations (for example FlashAttention-style kernels) that stream over blocks of $K,V$ and maintain the softmax normalization online. Practically, SDPA lets you use the standard attention math while keeping memory use close to $O((L_q+L_k)d)$ rather than $O(L_qL_k)$.</p><p>FLARE is built to use <strong>SDPA twice</strong>: once to gather information into a small latent set, and once to scatter it back.</p><hr><h4 id=step-1-gather-encode>Step 1: Gather (encode)<a hidden class=anchor aria-hidden=true href=#step-1-gather-encode>#</a></h4><p>Introduce $M \ll N$ latent tokens per head. Let $Q_h \in \mathbb{R}^{M \times d}$ be latent queries, and let $K_h, V_h \in \mathbb{R}^{N \times d}$ be token keys/values of head $h$. The gather step pools from the $N$ tokens into $M$ latents:</p><p>$$
Z_h = \mathrm{SDPA}(Q_h, K_h, V_h).
$$</p><p>In matrix notation this is</p><p>$$
W_{\mathrm{enc}} = \mathrm{softmax}(Q_h \cdot K_h^\top) \in \mathbb{R}^{M \times N},
\qquad
Z_h = W_{\mathrm{enc}} \cdot V_h.
$$</p><p>Importantly, although $W_{\mathrm{enc}}$ is a useful conceptual object, SDPA computes $Z_h$ <em>without</em> explicitly materializing $W_{\mathrm{enc}}$.</p><hr><h4 id=step-2-scatter-decode>Step 2: Scatter (decode)<a hidden class=anchor aria-hidden=true href=#step-2-scatter-decode>#</a></h4><p>Next, the latents broadcast information back to all $N$ tokens using the reverse affinity pattern:</p><p>$$
Y_h = \mathrm{SDPA}(K_h, Q_h, Z_h).
$$</p><p>In matrix form,</p><p>$$
W_{\mathrm{dec}} = \mathrm{softmax}(K_h \cdot Q_h^\top) \in \mathbb{R}^{N \times M},
\qquad
Y_h = W_{\mathrm{dec}} \cdot Z_h.
$$</p><p>Combining both steps gives an implicit global communication operator:</p><p>$$
Y_h = (W_{\mathrm{dec}} \cdot W_{\mathrm{enc}}) \cdot V_h.
$$</p><p>The product $W_{\mathrm{dec}} \cdot W_{\mathrm{enc}} \in \mathbb{R}^{N \times N}$ is a dense global mixing matrix, but its rank is at most $M$. FLARE therefore achieves global communication at <strong>$O(NM)$</strong> cost per head (with $M \ll N$), and the SDPA kernels avoid ever forming the intermediate $N \times M$ or $M \times N$ attention matrices in memory.</p><p><img alt="Slide 20: FLARE in two steps" loading=lazy src=/assets/blog/flare-post/slide-074.png></p><hr><h4 id=minimal-implementation>Minimal implementation<a hidden class=anchor aria-hidden=true href=#minimal-implementation>#</a></h4><p>Below is the core mixer expressed directly with PyTorch SDPA. This is essentially the gather–scatter operator in a few lines.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.nn.functional <span style=color:#f92672>import</span> scaled_dot_product_attention <span style=color:#66d9ef>as</span> SDPA
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>flare_multihead_mixer</span>(Q, K, V):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Q: [H, M, D]          latent queries (per head)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        K: [B, H, N, D]       token keys
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        V: [B, H, N, D]       token values
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Y: [B, H, N, D]       mixed token outputs
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    Qb <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)      <span style=color:#75715e># [1, H, M, D] to match SDPA batching</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Gather: tokens -&gt; latents</span>
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> SDPA(Qb, K, V, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)   <span style=color:#75715e># [B, H, M, D]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Scatter: latents -&gt; tokens</span>
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> SDPA(K, Qb, Z, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)   <span style=color:#75715e># [B, H, N, D]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> Y
</span></span></code></pre></div><p>The full FLARE block stacks this mixer with standard projection/MLP components:
<img alt="Slide 21: Low-rank token communication" loading=lazy src=/assets/blog/flare-post/FLARE.png></p><p>And here are the scaling results:
<img alt="Slide 21: Low-rank token communication" loading=lazy src=/assets/blog/flare-post/time_memory_bwd_fp16.png></p><hr><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>FLARE enables million-token training on a single GPU while maintaining strong accuracy on PDE surrogate tasks. The key improvement is not only asymptotic scaling but a practical runtime and memory profile that allows end-to-end training.</p><p><img alt="Slide 21: Low-rank token communication" loading=lazy src=/assets/blog/flare-post/slide-082.png></p><p>On sequence benchmarks such as Long Range Arena, the operator remains competitive with other efficient transformer approaches, showing that the mechanism is broadly applicable beyond scientific data.</p><p>On sequence benchmarks, such as Long Range Arena, FLARE is competitive with efficient-transformer alternatives and maintains strong average accuracy, showing that the method is not restricted to PDE-only structure.</p><p><img alt="Slide 27: Long Range Arena results" loading=lazy src=/assets/blog/flare-post/slide-091.png></p><hr><h2 id=deep-dive-why-this-design-works>Deep dive: why this design works<a hidden class=anchor aria-hidden=true href=#deep-dive-why-this-design-works>#</a></h2><p>Understanding why FLARE works requires looking at the structure of the gather–scatter operator and how different design choices affect learning dynamics.</p><h3 id=gatherscatter-as-communication-hubs>Gather–scatter as communication hubs<a hidden class=anchor aria-hidden=true href=#gatherscatter-as-communication-hubs>#</a></h3><p>Each latent token acts as both a pooling hub and a routing hub. During encoding, it aggregates information from tokens that align with its query pattern. During decoding, it distributes that information back to tokens that match its key pattern.
This creates a contraction–expansion communication structure that efficiently mixes global information.</p><h3 id=symmetric-operators-improve-stability>Symmetric operators improve stability<a hidden class=anchor aria-hidden=true href=#symmetric-operators-improve-stability>#</a></h3><p>The encode and decode steps are derived from the same query-key geometry, creating a structurally aligned operator. This symmetry reduces redundant parameterization and leads to more stable optimization compared to asymmetric variants.</p><h3 id=fixed-queries-provide-a-stable-routing-structure>Fixed queries provide a stable routing structure<a hidden class=anchor aria-hidden=true href=#fixed-queries-provide-a-stable-routing-structure>#</a></h3><p>Latent queries are learned but input independent. This gives a consistent communication basis across inputs, making the operator easier to optimize and interpret.
The reduced query dynamism is compensated by expressive key and value encoders, which adapt routing patterns to each input.</p><h3 id=repeated-global-mixing-is-more-effective-than-latent-refinement>Repeated global mixing is more effective than latent refinement<a hidden class=anchor aria-hidden=true href=#repeated-global-mixing-is-more-effective-than-latent-refinement>#</a></h3><p>Empirically, allocating compute to repeated gather–scatter operations produces better performance than stacking heavy latent self-attention blocks. The benefit comes from repeatedly mixing global information rather than refining latent representations in isolation.</p><h3 id=independent-latents-across-heads-increase-diversity>Independent latents across heads increase diversity<a hidden class=anchor aria-hidden=true href=#independent-latents-across-heads-increase-diversity>#</a></h3><p>Allowing each head to have its own latent tokens produces more diverse communication pathways. Different heads learn complementary routing structures, improving accuracy and depth scaling.</p><hr><h2 id=practical-takeaways>Practical takeaways<a hidden class=anchor aria-hidden=true href=#practical-takeaways>#</a></h2><p>Several design principles emerge:</p><ol><li>Use explicit low-rank communication rather than approximating local attention.</li><li>Allocate compute to repeated global mixing.</li><li>Keep routing pathways independent across heads.</li><li>Use expressive key and value encoders when queries are fixed.</li></ol><hr><h2 id=closing-thoughts>Closing thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>FLARE shows that scaling attention is not only about approximating softmax more efficiently. By rethinking attention as a communication operator, we can design mechanisms that preserve global information flow while dramatically reducing cost.</p><p>The broader lesson is that many large-scale problems have structure that can be exploited. Low-rank communication is one way to capture that structure without sacrificing flexibility, opening the door to practical million-token models on modest hardware.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Puri, V. et al. <em>FLARE: Fast Low-rank Attention Routing Engine</em>. arXiv (2025). <a href=https://arxiv.org/abs/2508.12594>https://arxiv.org/abs/2508.12594</a></li><li>Vaswani, A. et al. <em>Attention Is All You Need</em>. NeurIPS (2017). <a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a></li><li>Dao, T. et al. <em>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</em>. NeurIPS (2022). <a href=https://arxiv.org/abs/2205.14135>https://arxiv.org/abs/2205.14135</a></li><li>PyTorch Docs. <code>torch.nn.functional.scaled_dot_product_attention</code>. <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html>https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a></li><li>Tay, Y. et al. <em>Long Range Arena: A Benchmark for Efficient Transformers</em>. ICLR (2021). <a href=https://arxiv.org/abs/2011.04006>https://arxiv.org/abs/2011.04006</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://vpuri3.github.io/>Vedant Puri</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>