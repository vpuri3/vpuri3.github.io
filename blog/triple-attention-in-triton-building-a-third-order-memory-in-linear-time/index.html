<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Triple Attention in Triton: Building a Third-Order Memory in Linear Time | Vedant Puri</title><meta name=keywords content><meta name=description content="Designing and implementing third-order attention with Triton kernels and linear-time sequence scaling."><meta name=author content><link rel=canonical href=https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/><link crossorigin=anonymous href=/assets/css/stylesheet.72a547024d24c325f3bae9b2bb62cea7a569c0893d3ff900db35fa11b5fe740d.css integrity="sha256-cqVHAk0kwyXzuumyu2LOp6VpwIk9P/kA2zX6EbX+dA0=" rel="preload stylesheet" as=style><link rel=icon href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://github.githubassets.com/favicons/favicon.png><link rel=apple-touch-icon href=https://github.com/vpuri3.png><link rel=mask-icon href=https://github.githubassets.com/favicons/favicon.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/"><meta property="og:site_name" content="Vedant Puri"><meta property="og:title" content="Triple Attention in Triton: Building a Third-Order Memory in Linear Time"><meta property="og:description" content="Designing and implementing third-order attention with Triton kernels and linear-time sequence scaling."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-18T00:00:00-05:00"><meta property="article:modified_time" content="2026-02-18T00:00:00-05:00"><meta property="og:image" content="https://github.com/vpuri3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/vpuri3.png"><meta name=twitter:title content="Triple Attention in Triton: Building a Third-Order Memory in Linear Time"><meta name=twitter:description content="Designing and implementing third-order attention with Triton kernels and linear-time sequence scaling."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://vpuri3.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Triple Attention in Triton: Building a Third-Order Memory in Linear Time","item":"https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Triple Attention in Triton: Building a Third-Order Memory in Linear Time","name":"Triple Attention in Triton: Building a Third-Order Memory in Linear Time","description":"Designing and implementing third-order attention with Triton kernels and linear-time sequence scaling.","keywords":[],"articleBody":"Motivation Pairwise attention is powerful, but it compresses interaction structure into second-order forms. Most efficient attention methods try to approximate or factor the $N \\times N$ attention matrix. Triple attention takes a different perspective:\nInstead of modeling pairwise token interactions, build a structured higher-order memory and let tokens read from it.\nThis post explains how triple attention works conceptually, and how we implement it in Triton using fused kernels that scale linearly in sequence length.\nWhy third-order memory? Standard linear attention builds a pooled memory $S \\in \\mathbb{R}^{D \\times D}$:\n$$ S_{ij} = \\sum_{n=1}^{N} K_{ni} V_{nj}, $$and predicts with:\n$$ Y_{nj} = \\sum_{i=1}^{D} Q_{ni} S_{ij}. $$Triple attention instead accumulates a third-order state $S \\in \\mathbb{R}^{D_q \\times D_v \\times D_q}$:\n$$ S_{ijk} = \\sum_{n=1}^{N} K^{(1)}_{ni} V_{nj} K^{(2)}_{nk}, $$with output:\n$$ Y_{nj} = \\sum_{i=1}^{D} \\sum_{k=1}^{D} Q^{(1)}_{ni} S_{ijk} Q^{(2)}_{nk}. $$The sequence reduction stays linear in $N$, while representational capacity expands from $D \\times D$ to $D \\times D \\times D$. The tradeoff: compute scales as $O(N D_q^2 D_v)$, so this mechanism suits settings where $D_q$ is fixed and $N$ is large.\nFrom quadratic attention to structured memory Standard self-attention forms $A = \\text{softmax}(QK^\\top)V$, which requires materializing or implicitly computing an $N \\times N$ interaction. Triple attention avoids this entirely. Instead of computing token-to-token interactions, we construct a third-order state tensor $S \\in \\mathbb{R}^{D_q \\times D_v \\times D_q}$ that aggregates global information from all tokens in a streaming fashion. Each token then reads from this state via two learned query projections. No $N \\times N$ matrix is ever formed.\nTensor Shapes We begin with projected tensors:\n$Q_1, Q_2, K_1, K_2 \\in \\mathbb{R}^{B \\times H \\times N \\times D_q}$ $V \\in \\mathbb{R}^{B \\times H \\times N \\times D_v}$ For kernel simplicity, batch and heads are flattened:\nQ1, Q2, K1, K2: [BH, N, Dq] V: [BH, N, Dv] We allocate:\nSTATE: [BH, Dq, Dv, Dq] (accumulated in fp32) O: [BH, N, Dv] The memory cost is independent of sequence length $N$.\nForward pass The forward pass is split into two fused Triton kernels.\nPhase 1: Streaming state construction Kernel:\ntriple_fwd_state_kernel Each instance of the kernel:\nTiles indices $(i, j, k)$ of the state tensor. Streams over tokens in chunks (CHUNK_N, e.g., 4096). Accumulates contributions into STATE using atomic adds. Conceptually:\n$$ S_{ijk} = \\sum_{n=1}^N K_{1,n,i} \\cdot V_{n,j} \\cdot K_{2,n,k} $$Key properties:\nComplexity is O(N D_q^2 D_v). No token-token matrix is constructed. Streaming over sequence makes it linear in $N$. fp32 accumulation ensures stability. This phase builds a compact global memory.\nPhase 2: Output contraction Kernel:\ntriple_fwd_out_kernel This kernel:\nTiles over tokens (BLOCK_N_OUT) Contracts $Q_1$ and $Q_2$ with the precomputed STATE Writes output blocks to O Conceptually:\n$$ O_n = \\sum_{i,j,k} Q_{1,n,i} \\cdot S_{ijk} \\cdot Q_{2,n,k} $$Interpretation:\nThe state is a global communication hub. Each token decides how to read from it via two learned projections. This is structurally similar to routing through a learned latent space, but implemented as a multilinear contraction.\nWhy this scales Standard attention scales as:\n$$ O(N^2 D) $$Triple attention scales as:\n$$ O(N D_q^2 D_v) $$If $D_q$ is held fixed as $N$ grows, this is linear in sequence length.\nMemory never grows with $N^2$.\nThis makes it viable for long-sequence workloads, including:\nPDE surrogate models Large point-cloud processing Long-context sequence modeling Numerical considerations Several implementation details are critical:\nAccumulate STATE in fp32. Use tensor cores (TF32 where available). Store outputs in fp16/bf16. Chunk over sequence dimension to fit memory. Use atomic adds carefully to avoid race conditions. The mixed-precision strategy preserves stability while keeping memory bandwidth low.\nBackward pass The backward pass mirrors the forward decomposition.\nImplemented as a custom torch.autograd.Function, it performs:\n1. Accumulate dSTATE Compute:\n$$ dS = \\sum_n dO_n \\cdot Q_{1,n} \\cdot Q_{2,n} $$Streaming over tokens in chunks.\n2. Gradients for K1, K2, V Contract dSTATE with remaining factors:\ndK1 dK2 dV Each has its own fused Triton kernel.\n3. Gradients for Q1, Q2 Contract saved STATE with dO.\nThe code includes explicit einsum expressions for gradient verification, making parity testing against a reference implementation straightforward.\nConceptual perspective Triple attention reflects a broader idea:\nGlobal communication does not require pairwise token interaction.\nInstead of asking “which tokens attend to which?”, we ask:\nCan we compress global structure into a structured tensor, and let tokens read from it?\nThis viewpoint connects to:\nLow-rank attention Latent routing methods Multilinear tensor contractions Structured operator learning It also suggests a hypothesis:\nIf a structured self-attention mechanism captures global communication efficiently, it may extend naturally to causal and autoregressive settings.\nExploring that extension is ongoing work.\nOpen problems Better approximations to reduce $D^3$ pressure Hybrid blocks combining low-rank gather–scatter with triple memory Causal decoding variants for language modeling Closing thoughts Triple attention is not just a kernel experiment — it is an exploration of structured global memory.\nBy fusing state construction and output contraction in Triton, we obtain linear scaling in sequence length, stable mixed-precision execution, and a flexible multilinear attention primitive. This kernel serves as a foundation for further experiments in structured and adaptive attention mechanisms.\nThe full implementation is available in the FLARE repository alongside the paper (arXiv:2508.12594).\nReferences Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Katharopoulos, A. et al. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML (2020). https://arxiv.org/abs/2006.16236 Kozachinskiy, A. et al. Strassen Attention, Split VC Dimension and Compositionality in Transformers. arXiv (2025). https://arxiv.org/abs/2501.19215 Roy, A. et al. Fast and Simplex: 2-Simplicial Attention in Triton. arXiv (2025). https://arxiv.org/abs/2507.02754 Qin, Z. et al. The Devil in Linear Transformer. arXiv (2022). https://arxiv.org/abs/2210.10340 ","wordCount":"918","inLanguage":"en","image":"https://github.com/vpuri3.png","datePublished":"2026-02-18T00:00:00-05:00","dateModified":"2026-02-18T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/"},"publisher":{"@type":"Organization","name":"Vedant Puri","logo":{"@type":"ImageObject","url":"https://github.githubassets.com/favicons/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://vpuri3.github.io/ accesskey=h title="Vedant Puri (Alt + H)">Vedant Puri</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://vpuri3.github.io/#featured-work title=Featured><span>Featured</span></a></li><li><a href=https://vpuri3.github.io/#research-themes title=Research><span>Research</span></a></li><li><a href=https://vpuri3.github.io/#open-source title="Open Source"><span>Open Source</span></a></li><li><a href=https://vpuri3.github.io/work/ title=Work><span>Work</span></a></li><li><a href=https://vpuri3.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://vpuri3.github.io/photography/ title=Photography><span>Photography</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://vpuri3.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://vpuri3.github.io/blog/>Blog</a></div><h1 class="post-title entry-hint-parent">Triple Attention in Triton: Building a Third-Order Memory in Linear Time</h1><div class=post-description>Designing and implementing third-order attention with Triton kernels and linear-time sequence scaling.</div><div class=post-meta><span title='2026-02-18 00:00:00 -0500 -0500'>February 18, 2026</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#why-third-order-memory aria-label="Why third-order memory?">Why third-order memory?</a></li><li><a href=#from-quadratic-attention-to-structured-memory aria-label="From quadratic attention to structured memory">From quadratic attention to structured memory</a></li><li><a href=#tensor-shapes aria-label="Tensor Shapes">Tensor Shapes</a></li><li><a href=#forward-pass aria-label="Forward pass">Forward pass</a><ul><li><a href=#phase-1-streaming-state-construction aria-label="Phase 1: Streaming state construction">Phase 1: Streaming state construction</a></li><li><a href=#phase-2-output-contraction aria-label="Phase 2: Output contraction">Phase 2: Output contraction</a></li></ul></li><li><a href=#why-this-scales aria-label="Why this scales">Why this scales</a></li><li><a href=#numerical-considerations aria-label="Numerical considerations">Numerical considerations</a></li><li><a href=#backward-pass aria-label="Backward pass">Backward pass</a><ul><li><a href=#1-accumulate-dstate aria-label="1. Accumulate dSTATE">1. Accumulate dSTATE</a></li><li><a href=#2-gradients-for-k1-k2-v aria-label="2. Gradients for K1, K2, V">2. Gradients for K1, K2, V</a></li><li><a href=#3-gradients-for-q1-q2 aria-label="3. Gradients for Q1, Q2">3. Gradients for Q1, Q2</a></li></ul></li><li><a href=#conceptual-perspective aria-label="Conceptual perspective">Conceptual perspective</a></li><li><a href=#open-problems aria-label="Open problems">Open problems</a></li><li><a href=#closing-thoughts aria-label="Closing thoughts">Closing thoughts</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>Pairwise attention is powerful, but it compresses interaction structure into second-order forms. Most efficient attention methods try to approximate or factor the $N \times N$ attention matrix. Triple attention takes a different perspective:</p><blockquote><p>Instead of modeling pairwise token interactions, build a structured higher-order memory and let tokens read from it.</p></blockquote><p>This post explains how triple attention works conceptually, and how we implement it in Triton using fused kernels that scale linearly in sequence length.</p><hr><h2 id=why-third-order-memory>Why third-order memory?<a hidden class=anchor aria-hidden=true href=#why-third-order-memory>#</a></h2><p>Standard linear attention builds a pooled memory $S \in \mathbb{R}^{D \times D}$:</p>$$ S_{ij} = \sum_{n=1}^{N} K_{ni} V_{nj}, $$<p>and predicts with:</p>$$ Y_{nj} = \sum_{i=1}^{D} Q_{ni} S_{ij}. $$<p>Triple attention instead accumulates a third-order state $S \in \mathbb{R}^{D_q \times D_v \times D_q}$:</p>$$ S_{ijk} = \sum_{n=1}^{N} K^{(1)}_{ni} V_{nj} K^{(2)}_{nk}, $$<p>with output:</p>$$ Y_{nj} = \sum_{i=1}^{D} \sum_{k=1}^{D} Q^{(1)}_{ni} S_{ijk} Q^{(2)}_{nk}. $$<p>The sequence reduction stays linear in $N$, while representational capacity expands from $D \times D$ to $D \times D \times D$. The tradeoff: compute scales as $O(N D_q^2 D_v)$, so this mechanism suits settings where $D_q$ is fixed and $N$ is large.</p><hr><h2 id=from-quadratic-attention-to-structured-memory>From quadratic attention to structured memory<a hidden class=anchor aria-hidden=true href=#from-quadratic-attention-to-structured-memory>#</a></h2><p>Standard self-attention forms $A = \text{softmax}(QK^\top)V$, which requires materializing or implicitly computing an $N \times N$ interaction. Triple attention avoids this entirely. Instead of computing token-to-token interactions, we construct a <strong>third-order state tensor</strong> $S \in \mathbb{R}^{D_q \times D_v \times D_q}$ that aggregates global information from all tokens in a streaming fashion. Each token then reads from this state via two learned query projections. No $N \times N$ matrix is ever formed.</p><hr><h2 id=tensor-shapes>Tensor Shapes<a hidden class=anchor aria-hidden=true href=#tensor-shapes>#</a></h2><p>We begin with projected tensors:</p><ul><li>$Q_1, Q_2, K_1, K_2 \in \mathbb{R}^{B \times H \times N \times D_q}$</li><li>$V \in \mathbb{R}^{B \times H \times N \times D_v}$</li></ul><p>For kernel simplicity, batch and heads are flattened:</p><pre tabindex=0><code>Q1, Q2, K1, K2: [BH, N, Dq]
V:              [BH, N, Dv]
</code></pre><p>We allocate:</p><ul><li><code>STATE: [BH, Dq, Dv, Dq]</code> (accumulated in fp32)</li><li><code>O: [BH, N, Dv]</code></li></ul><p>The memory cost is independent of sequence length $N$.</p><hr><h2 id=forward-pass>Forward pass<a hidden class=anchor aria-hidden=true href=#forward-pass>#</a></h2><p>The forward pass is split into two fused Triton kernels.</p><h3 id=phase-1-streaming-state-construction>Phase 1: Streaming state construction<a hidden class=anchor aria-hidden=true href=#phase-1-streaming-state-construction>#</a></h3><p>Kernel:</p><pre tabindex=0><code>triple_fwd_state_kernel
</code></pre><p>Each instance of the kernel:</p><ul><li>Tiles indices $(i, j, k)$ of the state tensor.</li><li>Streams over tokens in chunks (<code>CHUNK_N</code>, e.g., 4096).</li><li>Accumulates contributions into <code>STATE</code> using atomic adds.</li></ul><p>Conceptually:</p>$$
S_{ijk} = \sum_{n=1}^N K_{1,n,i} \cdot V_{n,j} \cdot K_{2,n,k}
$$<p>Key properties:</p><ul><li>Complexity is <strong>O(N D_q^2 D_v)</strong>.</li><li>No token-token matrix is constructed.</li><li>Streaming over sequence makes it linear in $N$.</li><li>fp32 accumulation ensures stability.</li></ul><p>This phase builds a compact global memory.</p><hr><h3 id=phase-2-output-contraction>Phase 2: Output contraction<a hidden class=anchor aria-hidden=true href=#phase-2-output-contraction>#</a></h3><p>Kernel:</p><pre tabindex=0><code>triple_fwd_out_kernel
</code></pre><p>This kernel:</p><ul><li>Tiles over tokens (<code>BLOCK_N_OUT</code>)</li><li>Contracts $Q_1$ and $Q_2$ with the precomputed <code>STATE</code></li><li>Writes output blocks to <code>O</code></li></ul><p>Conceptually:</p>$$
O_n = \sum_{i,j,k} Q_{1,n,i} \cdot S_{ijk} \cdot Q_{2,n,k}
$$<p>Interpretation:</p><ul><li>The state is a global communication hub.</li><li>Each token decides how to read from it via two learned projections.</li></ul><p>This is structurally similar to routing through a learned latent space, but implemented as a multilinear contraction.</p><hr><h2 id=why-this-scales>Why this scales<a hidden class=anchor aria-hidden=true href=#why-this-scales>#</a></h2><p>Standard attention scales as:</p>$$
O(N^2 D)
$$<p>Triple attention scales as:</p>$$
O(N D_q^2 D_v)
$$<p>If $D_q$ is held fixed as $N$ grows, this is linear in sequence length.</p><p>Memory never grows with $N^2$.</p><p>This makes it viable for long-sequence workloads, including:</p><ul><li>PDE surrogate models</li><li>Large point-cloud processing</li><li>Long-context sequence modeling</li></ul><hr><h2 id=numerical-considerations>Numerical considerations<a hidden class=anchor aria-hidden=true href=#numerical-considerations>#</a></h2><p>Several implementation details are critical:</p><ul><li>Accumulate <code>STATE</code> in <strong>fp32</strong>.</li><li>Use tensor cores (TF32 where available).</li><li>Store outputs in fp16/bf16.</li><li>Chunk over sequence dimension to fit memory.</li><li>Use atomic adds carefully to avoid race conditions.</li></ul><p>The mixed-precision strategy preserves stability while keeping memory bandwidth low.</p><hr><h2 id=backward-pass>Backward pass<a hidden class=anchor aria-hidden=true href=#backward-pass>#</a></h2><p>The backward pass mirrors the forward decomposition.</p><p>Implemented as a custom <code>torch.autograd.Function</code>, it performs:</p><h3 id=1-accumulate-dstate>1. Accumulate dSTATE<a hidden class=anchor aria-hidden=true href=#1-accumulate-dstate>#</a></h3><p>Compute:</p>$$
dS = \sum_n dO_n \cdot Q_{1,n} \cdot Q_{2,n}
$$<p>Streaming over tokens in chunks.</p><h3 id=2-gradients-for-k1-k2-v>2. Gradients for K1, K2, V<a hidden class=anchor aria-hidden=true href=#2-gradients-for-k1-k2-v>#</a></h3><p>Contract <code>dSTATE</code> with remaining factors:</p><ul><li><code>dK1</code></li><li><code>dK2</code></li><li><code>dV</code></li></ul><p>Each has its own fused Triton kernel.</p><h3 id=3-gradients-for-q1-q2>3. Gradients for Q1, Q2<a hidden class=anchor aria-hidden=true href=#3-gradients-for-q1-q2>#</a></h3><p>Contract saved <code>STATE</code> with <code>dO</code>.</p><p>The code includes explicit einsum expressions for gradient verification, making parity testing against a reference implementation straightforward.</p><hr><h2 id=conceptual-perspective>Conceptual perspective<a hidden class=anchor aria-hidden=true href=#conceptual-perspective>#</a></h2><p>Triple attention reflects a broader idea:</p><blockquote><p>Global communication does not require pairwise token interaction.</p></blockquote><p>Instead of asking “which tokens attend to which?”, we ask:</p><blockquote><p>Can we compress global structure into a structured tensor, and let tokens read from it?</p></blockquote><p>This viewpoint connects to:</p><ul><li>Low-rank attention</li><li>Latent routing methods</li><li>Multilinear tensor contractions</li><li>Structured operator learning</li></ul><p>It also suggests a hypothesis:</p><blockquote><p>If a structured self-attention mechanism captures global communication efficiently, it may extend naturally to causal and autoregressive settings.</p></blockquote><p>Exploring that extension is ongoing work.</p><hr><h2 id=open-problems>Open problems<a hidden class=anchor aria-hidden=true href=#open-problems>#</a></h2><ul><li>Better approximations to reduce $D^3$ pressure</li><li>Hybrid blocks combining low-rank gather–scatter with triple memory</li><li>Causal decoding variants for language modeling</li></ul><hr><h2 id=closing-thoughts>Closing thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>Triple attention is not just a kernel experiment — it is an exploration of structured global memory.</p><p>By fusing state construction and output contraction in Triton, we obtain linear scaling in sequence length, stable mixed-precision execution, and a flexible multilinear attention primitive. This kernel serves as a foundation for further experiments in structured and adaptive attention mechanisms.</p><p>The full implementation is available in the FLARE repository alongside the paper (<a href=https://arxiv.org/abs/2508.12594>arXiv:2508.12594</a>).</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Vaswani, A. et al. <em>Attention Is All You Need</em>. NeurIPS (2017). <a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a></li><li>Katharopoulos, A. et al. <em>Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</em>. ICML (2020). <a href=https://arxiv.org/abs/2006.16236>https://arxiv.org/abs/2006.16236</a></li><li>Kozachinskiy, A. et al. <em>Strassen Attention, Split VC Dimension and Compositionality in Transformers</em>. arXiv (2025). <a href=https://arxiv.org/abs/2501.19215>https://arxiv.org/abs/2501.19215</a></li><li>Roy, A. et al. <em>Fast and Simplex: 2-Simplicial Attention in Triton</em>. arXiv (2025). <a href=https://arxiv.org/abs/2507.02754>https://arxiv.org/abs/2507.02754</a></li><li>Qin, Z. et al. <em>The Devil in Linear Transformer</em>. arXiv (2022). <a href=https://arxiv.org/abs/2210.10340>https://arxiv.org/abs/2210.10340</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://vpuri3.github.io/>Vedant Puri</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>