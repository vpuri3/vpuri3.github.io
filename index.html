<!doctype html><html lang=en dir=auto data-theme=dark><head><meta name=generator content="Hugo 0.155.3"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vedant Puri</title><meta name=description content><meta name=author content><link rel=canonical href=https://vpuri3.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.72a547024d24c325f3bae9b2bb62cea7a569c0893d3ff900db35fa11b5fe740d.css integrity="sha256-cqVHAk0kwyXzuumyu2LOp6VpwIk9P/kA2zX6EbX+dA0=" rel="preload stylesheet" as=style><link rel=icon href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://github.githubassets.com/favicons/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://github.githubassets.com/favicons/favicon.png><link rel=apple-touch-icon href=https://github.com/vpuri3.png><link rel=mask-icon href=https://github.githubassets.com/favicons/favicon.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://vpuri3.github.io/index.xml title=rss><link rel=alternate type=application/json href=https://vpuri3.github.io/index.json title=json><link rel=alternate hreflang=en href=https://vpuri3.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><meta property="og:url" content="https://vpuri3.github.io/"><meta property="og:site_name" content="Vedant Puri"><meta property="og:title" content="Home"><meta property="og:description" content=" Vedant Puri PhD Candidate, Carnegie Mellon University
Efficient Transformer Architectures | Scientific Machine Learning
I design transformer architectures with explicit attention to scaling and memory efficiency. My recent work, FLARE, enables million-token regimes on a single GPU. I implement new architectures directly in PyTorch and Triton. My background spans high-performance computing, numerical analysis, and computational fluid dynamics. LinkedIn | GitHub | Google Scholar | vedantpuri@cmu.edu
Research Interests Efficient attention architectures Numerical methods for ML and for PDEs Scientific machine learning Featured Work FLARE - Fast Low-rank Attention Routing Engine Derived a flexible low-rank reformulation of self-attention via latent routing. Reduced quadratic complexity of global communication in self-attention to linear complexity. Demonstrated scaling to 1M tokens on a single H100 GPU, attaining over 200x speedup over vanilla self-attention. Implemented attention modules in PyTorch and Triton with reproducible scaling experiments. Evaluated across PDE surrogate modeling, NLP, and vision benchmarks. "><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta property="og:image" content="https://github.com/vpuri3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/vpuri3.png"><meta name=twitter:title content="Home"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Vedant Puri","url":"https://vpuri3.github.io/","description":"","logo":"https://github.githubassets.com/favicons/favicon.png","sameAs":["https://github.com/vpuri3","https://www.linkedin.com/in/vpuri3/","mailto:vedantpuri@cmu.edu"]}</script></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://vpuri3.github.io/ accesskey=h title="Vedant Puri (Alt + H)">Vedant Puri</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://vpuri3.github.io/#featured-work title=Featured><span>Featured</span></a></li><li><a href=https://vpuri3.github.io/#research-themes title=Research><span>Research</span></a></li><li><a href=https://vpuri3.github.io/#open-source title="Open Source"><span>Open Source</span></a></li><li><a href=https://vpuri3.github.io/work/ title=Work><span>Work</span></a></li><li><a href=https://vpuri3.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://vpuri3.github.io/photography/ title=Photography><span>Photography</span></a></li></ul></nav></header><main class=main><div class=post-content><div class=hero-card><div class=hero-top><img class=hero-photo src=/assets/profile-headshot.jpg alt="Vedant Puri profile photo"><div class=hero-copy><h1>Vedant Puri</h1><p class=hero-subtitle>PhD Candidate, Carnegie Mellon University</p><p class=hero-tagline>Efficient Transformer Architectures | Scientific Machine Learning</p></div></div><p>I design transformer architectures with explicit attention to scaling and memory efficiency.
My recent work, FLARE, enables million-token regimes on a single GPU.
I implement new architectures directly in PyTorch and Triton.
My background spans high-performance computing, numerical analysis, and computational fluid dynamics.</p><p><a href=https://www.linkedin.com/in/vpuri3/>LinkedIn</a> | <a href=https://github.com/vpuri3>GitHub</a> | <a href="https://scholar.google.com/citations?user=YzzTeUAAAAAJ&hl=en&oi=ao">Google Scholar</a> | <a href=mailto:vedantpuri@cmu.edu><code>vedantpuri@cmu.edu</code></a></p></div><div class=section-card id=research-interests><h2 id=research-interests>Research Interests<a hidden class=anchor aria-hidden=true href=#research-interests>#</a></h2><ul><li>Efficient attention architectures</li><li>Numerical methods for ML and for PDEs</li><li>Scientific machine learning</li></ul></div><div class=section-card id=featured-work><h2 id=featured-work>Featured Work<a hidden class=anchor aria-hidden=true href=#featured-work>#</a></h2><div class=project-card><h3 id=flare---fast-low-rank-attention-routing-engine>FLARE - Fast Low-rank Attention Routing Engine<a hidden class=anchor aria-hidden=true href=#flare---fast-low-rank-attention-routing-engine>#</a></h3><ul><li>Derived a flexible low-rank reformulation of self-attention via latent routing.</li><li>Reduced quadratic complexity of global communication in self-attention to linear complexity.</li><li>Demonstrated scaling to 1M tokens on a single H100 GPU, attaining over 200x speedup over vanilla self-attention.</li><li>Implemented attention modules in PyTorch and Triton with reproducible scaling experiments.</li><li>Evaluated across PDE surrogate modeling, NLP, and vision benchmarks.</li></ul><p><a href=https://github.com/vpuri3/FLARE.py><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a>
<a href=https://github.com/vpuri3/FLARE.py/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/FLARE.py?style=social"></a>
<a href=https://huggingface.co/papers/2508.12594><img alt="Hugging Face Paper" loading=lazy src="https://img.shields.io/badge/Hugging%20Face-Paper-FFD21E?logo=huggingface&logoColor=black"></a>
<a href=https://arxiv.org/abs/2508.12594><img alt=arXiv loading=lazy src="https://img.shields.io/badge/arXiv-2508.12594-B31B1B?logo=arxiv&logoColor=white"></a></p><p><img alt="FLARE architecture overview" loading=lazy src=/assets/flare-overview.png></p></div><div class=project-card><h3 id=flare-for-language-modeling-ongoing-dissertation-work>FLARE for Language Modeling (Ongoing dissertation work)<a hidden class=anchor aria-hidden=true href=#flare-for-language-modeling-ongoing-dissertation-work>#</a></h3><p><em>Decoder-only architectures | 2025&ndash;Present</em></p><ul><li>Extending FLARE to decoder-only next-token prediction with causal attention.</li><li>Enabling adaptive attention state size to control memory and compute during training and inference.</li><li>Implementing fused Triton kernels for causal training, prefill, and decode.</li></ul><p><a href="https://www.youtube.com/watch?v=8h9EXJqQUi0"><img alt=YouTube loading=lazy src="https://img.shields.io/badge/YouTube-Thesis%20Proposal-FF0000?logo=youtube&logoColor=white"></a></p></div><div class=project-card><h3 id=hybrid-equation-based--data-driven-pde-modeling-framework>Hybrid equation-based + data-driven PDE modeling framework<a hidden class=anchor aria-hidden=true href=#hybrid-equation-based--data-driven-pde-modeling-framework>#</a></h3><ul><li>Introduced smooth neural fields as nonlinear spatial ansatz functions in equation-based reduced-order modeling.</li><li>Retained physics-based Galerkin time evolution while learning expressive low-dimensional representations.</li><li>Attained 200x speedup over full-order simulations in transport-dominated regimes.</li></ul><p><a href=https://vpuri3.github.io/NeuralROMs.jl/dev/>Project page</a> | <a href=https://www.sciencedirect.com/science/article/pii/S0021999125002402>JCP paper</a> | <a href=https://arxiv.org/abs/2405.14890><img alt=arXiv loading=lazy src="https://img.shields.io/badge/arXiv-2405.14890-B31B1B?logo=arxiv&logoColor=white"></a>
<a href=https://github.com/vpuri3/NeuralROMs.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a>
<a href=https://github.com/vpuri3/NeuralROMs.jl/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/NeuralROMs.jl?style=social"></a>
<a href="https://youtu.be/zio-_89DJ0g?si=sDVE1c0xJqzVi8bm"><img alt="YouTube Talk" loading=lazy src="https://img.shields.io/badge/YouTube-Talk-FF0000?logo=youtube&logoColor=white"></a></p><p><img alt="SNF-ROM online stage architecture" loading=lazy src=/assets/snfrom-online-stage.png></p></div></div><div class=section-card id=previous-work><h2 id=previous-work-computational-fluid-dynamics-on-hpc-systems>Previous Work: Computational fluid dynamics on HPC systems<a hidden class=anchor aria-hidden=true href=#previous-work-computational-fluid-dynamics-on-hpc-systems>#</a></h2><div class=project-card><p>I previously worked on turbulence simulation and analysis workflows in high-performance computing settings, with emphasis on spectral element methods and large-scale post-processing.
This background in numerical methods and PDE solvers informs how I design stable and efficient transformer architectures for scientific ML.</p><p><img alt="Velocity magnitude for flow past wall-mounted cube" loading=lazy src=/assets/wall-mounted-cube-velocity.jpg>
<em>Velocity magnitude for flow past wall-mounted cube case at Reynolds Number 3900 with respect to cube height. Computation performed using spectral element code NEK5000 at Argonne Leadership Computing Facility.</em></p></div></div><div class=section-card id=photography><h2 id=not-work>Not Work<a hidden class=anchor aria-hidden=true href=#not-work>#</a></h2><div class=project-card><h3 id=not-so-up-to-date-photography-portfolio>Not So Up-to-Date Photography Portfolio<a hidden class=anchor aria-hidden=true href=#not-so-up-to-date-photography-portfolio>#</a></h3><p>For the past decade, I have used a Canon DSLR as an excuse to walk around and photograph people, geometry, and city texture.</p><p><a href=/photography/>Open portfolio page</a> | <a href=https://www.flickr.com/photos/128280868@N05/>Flickr</a></p></div><div class=project-card><h3 id=hobbies>Hobbies<a hidden class=anchor aria-hidden=true href=#hobbies>#</a></h3><ul><li>Sports: squash, golf, crossfit</li></ul></div></div><div class=section-card id=open-source><h2 id=open-source>Open Source<a hidden class=anchor aria-hidden=true href=#open-source>#</a></h2><div class=project-card><h3 id=flare-github-github-stars>FLARE <a href=https://github.com/vpuri3/FLARE.py><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/vpuri3/FLARE.py/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/FLARE.py?style=social"></a><a hidden class=anchor aria-hidden=true href=#flare-github-github-stars>#</a></h3><p>Fast Low-rank Attention Routing Engine for scalable transformer attention.</p></div><div class=project-card><h3 id=mlutilspy-github-github-stars>mlutils.py <a href=https://github.com/vpuri3/mlutils.py><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/vpuri3/mlutils.py/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/mlutils.py?style=social"></a><a hidden class=anchor aria-hidden=true href=#mlutilspy-github-github-stars>#</a></h3><p>Lightweight PyTorch project template and utility toolkit for ML experiments.</p></div><div class=project-card><h3 id=julia-open-source-tools>Julia Open Source Tools<a hidden class=anchor aria-hidden=true href=#julia-open-source-tools>#</a></h3><h4 id=scimloperatorsjl-scimloperatorsjl-scimloperators-stars><a href=https://github.com/SciML/SciMLOperators.jl>SciMLOperators.jl</a> <a href=https://github.com/SciML/SciMLOperators.jl><img alt=SciMLOperators.jl loading=lazy src="https://img.shields.io/badge/GitHub-SciMLOperators.jl-181717?logo=github"></a> <a href=https://github.com/SciML/SciMLOperators.jl/stargazers><img alt="SciMLOperators stars" loading=lazy src="https://img.shields.io/github/stars/SciML/SciMLOperators.jl?style=social"></a><a hidden class=anchor aria-hidden=true href=#scimloperatorsjl-scimloperatorsjl-scimloperators-stars>#</a></h4><p>Operator abstractions for SciML and PDE workflows</p><h4 id=linearsolvejl-linearsolvejl-linearsolve-stars><a href=https://github.com/SciML/LinearSolve.jl>LinearSolve.jl</a> <a href=https://github.com/SciML/LinearSolve.jl><img alt=LinearSolve.jl loading=lazy src="https://img.shields.io/badge/GitHub-LinearSolve.jl-181717?logo=github"></a> <a href=https://github.com/SciML/LinearSolve.jl/stargazers><img alt="LinearSolve stars" loading=lazy src="https://img.shields.io/github/stars/SciML/LinearSolve.jl?style=social"></a><a hidden class=anchor aria-hidden=true href=#linearsolvejl-linearsolvejl-linearsolve-stars>#</a></h4><p>Linear solver interface for scientific machine learning</p><p>Below is a nonexhaustive list of Julia projects that I have contributed to.</p><ul><li><a href=https://github.com/SciML/OrdinaryDiffEq.jl>OrdinaryDiffEq.jl</a> <a href=https://github.com/SciML/OrdinaryDiffEq.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/OrdinaryDiffEq.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/OrdinaryDiffEq.jl?style=social"></a></li><li><a href=https://github.com/SciML/NonlinearSolve.jl>NonlinearSolve.jl</a> <a href=https://github.com/SciML/NonlinearSolve.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/NonlinearSolve.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/NonlinearSolve.jl?style=social"></a></li><li><a href=https://github.com/SciML/Optimization.jl>Optimization.jl</a> <a href=https://github.com/SciML/Optimization.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/Optimization.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/Optimization.jl?style=social"></a></li><li><a href=https://github.com/SciML/SciMLBase.jl>SciMLBase.jl</a> <a href=https://github.com/SciML/SciMLBase.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/SciMLBase.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/SciMLBase.jl?style=social"></a></li><li><a href=https://github.com/SciML/SciMLSensitivity.jl>SciMLSensitivity.jl</a> <a href=https://github.com/SciML/SciMLSensitivity.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/SciMLSensitivity.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/SciMLSensitivity.jl?style=social"></a></li><li><a href=https://github.com/SciML/DiffEqFlux.jl>DiffEqFlux.jl</a> <a href=https://github.com/SciML/DiffEqFlux.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/DiffEqFlux.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/DiffEqFlux.jl?style=social"></a></li><li><a href=https://github.com/SciML/StochasticDiffEq.jl>StochasticDiffEq.jl</a> <a href=https://github.com/SciML/StochasticDiffEq.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/StochasticDiffEq.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/StochasticDiffEq.jl?style=social"></a></li><li><a href=https://github.com/SciML/DiffEqBase.jl>DiffEqBase.jl</a> <a href=https://github.com/SciML/DiffEqBase.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/SciML/DiffEqBase.jl/stargazers><img alt=Stars loading=lazy src="https://img.shields.io/github/stars/SciML/DiffEqBase.jl?style=social"></a></li></ul></div><div class=project-card><h3 id=kolmogorovarnoldjl-github-github-stars>KolmogorovArnold.jl <a href=https://github.com/vpuri3/KolmogorovArnold.jl><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/vpuri3/KolmogorovArnold.jl/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/KolmogorovArnold.jl?style=social"></a><a hidden class=anchor aria-hidden=true href=#kolmogorovarnoldjl-github-github-stars>#</a></h3><p>Julia implementation of Kolmogorov-Arnold Networks with custom gradients for faster training.</p></div><div class=project-card><h3 id=fastdiffusionpy-github-github-stars>FastDiffusion.py <a href=https://github.com/vpuri3/FastDiffusion.py><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/vpuri3/FastDiffusion.py/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/FastDiffusion.py?style=social"></a><a hidden class=anchor aria-hidden=true href=#fastdiffusionpy-github-github-stars>#</a></h3><p>Experiment with trigonometric noise schedule in context of few step diffusion.</p></div><div class=project-card><h3 id=nektools-github-github-stars>NekTools <a href=https://github.com/vpuri3/NekTools><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github"></a> <a href=https://github.com/vpuri3/NekTools/stargazers><img alt="GitHub stars" loading=lazy src="https://img.shields.io/github/stars/vpuri3/NekTools?style=social"></a><a hidden class=anchor aria-hidden=true href=#nektools-github-github-stars>#</a></h3><p>FORTRAN 77 utilities for turbulence statistics and post-processing in NEK5000.</p></div></div></div><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention</h2></header><div class=entry-content><p>Motivation FLARE was originally developed as an encoder-style global mixing primitive: learned latent queries gather information from many tokens, then scatter it back. The decoder setting is harder because causality changes algorithmic dependencies, numerical stability constraints, and what efficiency means in training versus inference.
This post summarizes a practical path to causal FLARE for long-context language modeling. See also the dissertation proposal talk for broader context.
What changes from encoder to decoder? Encoder attention is bidirectional: token $t$ can depend on any token $\tau$. Decoder attention is causal: token $t$ may depend only on $\tau \le t$.
...</p></div><footer class=entry-footer><span title='2026-02-18 00:00:00 -0500 -0500'>February 18, 2026</span></footer><a class=entry-link aria-label="post link to From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention" href=https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Triple Attention in Triton: Building a Third-Order Memory in Linear Time</h2></header><div class=entry-content><p>Motivation Pairwise attention is powerful, but it compresses interaction structure into second-order forms. Most efficient attention methods try to approximate or factor the $N \times N$ attention matrix. Triple attention takes a different perspective:
Instead of modeling pairwise token interactions, build a structured higher-order memory and let tokens read from it.
This post explains how triple attention works conceptually, and how we implement it in Triton using fused kernels that scale linearly in sequence length.
...</p></div><footer class=entry-footer><span title='2026-02-18 00:00:00 -0500 -0500'>February 18, 2026</span></footer><a class=entry-link aria-label="post link to Triple Attention in Triton: Building a Third-Order Memory in Linear Time" href=https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing</h2></header><div class=entry-content><p>Beyond pairwise attention Softmax attention is an extremely expressive token-mixing primitive, but it is expensive. For a sequence of length $N$, the core interaction matrix $Q \cdot K^\top$ is $N \times N$, which drives both runtime and memory. Linear transformers try to keep global context while avoiding the quadratic scaling. The catch is that the simplest linear attention formulations often lose a key ingredient that makes softmax attention work so well: token-specific routing.
...</p></div><footer class=entry-footer><span title='2026-02-16 00:00:00 -0500 -0500'>February 16, 2026</span></footer><a class=entry-link aria-label="post link to Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing" href=https://vpuri3.github.io/blog/adventures-in-high-order-attention/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Scaling attention to 1M tokens on a single GPU</h2></header><div class=entry-content><p>Attention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.
Rather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?
...</p></div><footer class=entry-footer><span title='2026-02-16 00:00:00 -0500 -0500'>February 16, 2026</span></footer><a class=entry-link aria-label="post link to Scaling attention to 1M tokens on a single GPU" href=https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>From POD to neural manifolds: a modern take on reduced order modeling</h2></header><div class=entry-content><p>From POD to neural manifolds Reduced-order modeling (ROM) is ultimately about preserving dominant system behavior with far fewer degrees of freedom. Classical projection methods, convolutional autoencoder ROMs, and modern neural-field formulations all target this same objective, but they make different assumptions about (i) how we represent the state and (ii) how we evolve it in time.
This post walks through that progression and focuses on Smooth Neural Field ROM (SNF-ROM): a projection-based nonlinear ROM that uses continuous neural fields as the state representation and explicitly supports physics-based differentiation and time integration during deployment.
...</p></div><footer class=entry-footer><span title='2026-02-15 00:00:00 -0500 -0500'>February 15, 2026</span></footer><a class=entry-link aria-label="post link to From POD to neural manifolds: a modern take on reduced order modeling" href=https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Our job as computational engineers</h2></header><div class=entry-content><p>Why math is interesting The foundation of my interest in math was laid in my freshman year when I was appointed course staff to a sophomore engineering course. As I taught students to model mechanical interactions using forces and moments, I learned not only to communicate my ideas to newcomers in engineering but also how to challenge my own preconceived notions. Explaining why a certain unseen force has to exist to maintain equilibrium is like talking in a different language, in the sense that certain things are obvious (evident enough not to require proving) to me but not to a student. I have to explain that concentrated point forces, reaction moments and all these unseen entities are fictitious objects made by engineers and scientists to model and approximate reality.
...</p></div><footer class=entry-footer><span title='2022-01-01 00:00:00 +0000 UTC'>January 1, 2022</span></footer><a class=entry-link aria-label="post link to Our job as computational engineers" href=https://vpuri3.github.io/blog/thoughts-archive/></a></article></main><footer class=footer><span>&copy; 2026 <a href=https://vpuri3.github.io/>Vedant Puri</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>