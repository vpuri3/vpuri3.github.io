[{"content":"Motivation FLARE was originally developed as an encoder-style global mixing primitive: learned latent queries gather information from many tokens, then scatter it back. The decoder setting is harder because causality changes algorithmic dependencies, numerical stability constraints, and what efficiency means in training versus inference.\nThis post summarizes a practical path to causal FLARE for long-context language modeling. See also the dissertation proposal talk for broader context.\nWhat changes from encoder to decoder? Encoder attention is bidirectional: token $t$ can depend on any token $\\tau$. Decoder attention is causal: token $t$ may depend only on $\\tau \\le t$.\nThis implies three requirements:\nNo future-token leakage. Efficient prefill for long contexts. Fast incremental decode with cached state. Recap: encoder FLARE as gather-scatter Let latent queries be $Q_L \\in \\mathbb{R}^{M \\times D}$, and token keys/values be $K,V \\in \\mathbb{R}^{N \\times D}$.\n$$ Z = \\mathrm{SDPA}(Q_L, K, V), \\qquad Y = \\mathrm{SDPA}(K, Q_L, Z). $$Interpretation:\nGather: latents pool from tokens. Scatter: tokens read from latents. Causal softmax attention baseline $$ y_t = \\sum_{\\tau \\le t} \\frac{\\exp(q_t^\\top k_\\tau)}{\\sum_{u \\le t}\\exp(q_t^\\top k_u)}v_\\tau $$or\n$$ Y = \\mathrm{softmax}\\!\\big((QK^\\top) \\odot M_{\\mathrm{causal}}\\big)V, $$where $M_{\\mathrm{causal}}$ masks the strict upper triangle.\nWarm-up: causal linear attention as a state update $$ y_t = \\frac{S_t q_t}{q_t^\\top z_t}, \\qquad S_t = \\sum_{\\tau \\le t} v_\\tau k_\\tau^\\top, \\qquad z_t = \\sum_{\\tau \\le t} k_\\tau. $$This shows the decoder-friendly pattern: maintain prefix state, update incrementally, and compute $y_t$ from state plus $q_t$.\nCausal FLARE definition A causal latent update can be written as:\n$$ z_m^t = \\sum_{\\tau \\le t} \\frac{\\exp(q_m^\\top k_\\tau)}{\\sum_{u \\le t}\\exp(q_m^\\top k_u)}v_\\tau. $$Then token output at step $t$:\n$$ y_t = \\sum_{m=1}^M \\frac{\\exp(k_t^\\top q_m)}{\\sum_{m'=1}^M \\exp(k_t^\\top q_{m'})} z_m^t. $$So each step produces an updated latent set $Z_t = [z_1^t,\\ldots,z_M^t]$, then token $t$ reads from it.\nAlgorithm 1: streaming recurrent causal FLARE (decode-friendly) Each latent maintains running online-softmax statistics across the token stream: a running max $\\mu_{t,m}$, a normalizing denominator $d_{t,m}$, and a numerator accumulator $U_{t,m,:}$. The update at each step is:\nInitialize: $U_0 \\in \\mathbb{R}^{M \\times D} \\leftarrow 0$ $d_0 \\in \\mathbb{R}^{M} \\leftarrow 0$ $\\mu_0 \\in \\mathbb{R}^{M} \\leftarrow -\\infty$ For each token $t=1,\\ldots,T$, given $(k_t, v_t)$: $s_t \\leftarrow (Qk_t)s$ $\\mu_t \\leftarrow \\max(\\mu_{t-1}, s_t)$ $\\gamma \\leftarrow \\exp(\\mu_{t-1}-\\mu_t)$ $\\eta \\leftarrow \\exp(s_t-\\mu_t)$ $d_t \\leftarrow d_{t-1}\\odot\\gamma + \\eta$ $U_t \\leftarrow U_{t-1}\\odot\\gamma + \\eta\\,v_t^\\top$ $Z_t \\leftarrow U_t \\oslash d_t$ $\\alpha_t \\leftarrow \\mathrm{softmax}(s_t)$ $y_t \\leftarrow \\alpha_t^\\top Z_t$ Return $\\{y_t\\}_{t=1}^T$. This recurrent form is ideal for autoregressive decode — cached state is updated in $O(M)$ work per token. However, it is throughput-inefficient for training because the backward pass must store all prefix statistics naively. The next two algorithms address the prefill and training setting.\nimport torch import torch.nn.functional as F def causal_flare_decode_step(q, k_t, v_t, U, d, mu, scale=1.0): \u0026#34;\u0026#34;\u0026#34; Algorithm 1: single recurrent step for autoregressive decode. Updates the cached latent state with the new token and reads the output. Args: q: [H, M, D] latent queries (learned, fixed across steps) k_t: [B, H, D] key for the current token v_t: [B, H, D] value for the current token U: [B, H, M, D] running numerator accumulator (fp32) d: [B, H, M] running denominator (fp32) mu: [B, H, M] running prefix max (fp32) scale: float key-query scale factor (default 1.0) Returns: y_t: [B, H, D] output for the current token U: [B, H, M, D] updated numerator accumulator d: [B, H, M] updated denominator mu: [B, H, M] updated prefix max \u0026#34;\u0026#34;\u0026#34; # s_t[b, h, m] = scale * dot(q[h, m], k_t[b, h]) s_t = scale * torch.einsum(\u0026#39;hmd,bhd-\u0026gt;bhm\u0026#39;, q, k_t.float()) # [B, H, M] # Online-softmax update for each latent mu_new = torch.maximum(mu, s_t) # [B, H, M] gamma = torch.exp(mu - mu_new) # rescale old stats eta = torch.exp(s_t - mu_new) # weight for new token d = d * gamma + eta # [B, H, M] U = U * gamma.unsqueeze(-1) + eta.unsqueeze(-1) * v_t.float().unsqueeze(2) # [B, H, M, D] mu = mu_new Z_t = U / (d.unsqueeze(-1) + 1e-6) # [B, H, M, D] # Scatter: token reads from updated latents via softmax over M alpha_t = F.softmax(s_t, dim=-1) # [B, H, M] y_t = torch.einsum(\u0026#39;bhm,bhmd-\u0026gt;bhd\u0026#39;, alpha_t, Z_t) # [B, H, D] return y_t.to(v_t.dtype), U, d, mu Algorithm 2: dense causal FLARE (prefill-oriented) Define scores:\n$$ S_{t,m} = s\\,k_t^\\top q_m, \\qquad A_{t,m} = \\exp(S_{t,m}), \\qquad P_{t,m} = \\mathrm{softmax}_m(S_{t,:}). $$Prefix denominator per latent:\n$$ D_{t,m} = \\sum_{u \\le t} A_{u,m}. $$Then\n$$ C_{t,m} = \\frac{P_{t,m}}{D_{t,m}+\\varepsilon}, \\qquad W = C A^\\top, \\qquad Y = (W \\odot M_{\\mathrm{causal}})V. $$This dense form is matmul-friendly and efficient for training, but computing $\\exp(S)$ directly is numerically unstable for long contexts in mixed precision — exponents of large scores overflow in BF16. Algorithm 3 fixes this.\nAlgorithm 3: stable dense causal FLARE Use online-softmax style prefix statistics for each latent:\n$R_{t,m}$: running prefix max of $S_{t,m}$ $L_{t,m}$: stable prefix sum in normalized frame $$ R_{t,m} = \\max(R_{t-1,m}, S_{t,m}), $$$$ L_{t,m} = L_{t-1,m}\\exp(R_{t-1,m}-R_{t,m}) + \\exp(S_{t,m}-R_{t,m}). $$Then\n$$ C_{t,m} = \\frac{P_{t,m}}{L_{t,m}+\\varepsilon}, \\qquad W_{t,\\tau} = \\sum_{m=1}^M C_{t,m}\\exp(S_{\\tau,m}-R_{t,m}), $$$$ W \\leftarrow W \\odot M_{\\mathrm{causal}}, \\qquad Y = WV. $$This keeps the prefill path numerically stable while preserving dense-kernel structure.\nCompute score and latent decode probabilities: $S \\leftarrow s(KQ^\\top)$, so $S_{t,m}=s\\,k_t^\\top q_m$ $P \\leftarrow \\mathrm{softmax}_m(S)$ Compute stable prefix statistics for each latent: Initialize $R_{0,m}\\leftarrow -\\infty,\\;L_{0,m}\\leftarrow 0$ For $t=1,\\ldots,T$: $R_{t,m}\\leftarrow \\max(R_{t-1,m}, S_{t,m})$ $L_{t,m}\\leftarrow L_{t-1,m}\\exp(R_{t-1,m}-R_{t,m})+\\exp(S_{t,m}-R_{t,m})$ Build dense causal mixer: $C_{t,m}\\leftarrow \\dfrac{P_{t,m}}{L_{t,m}+\\varepsilon}$ $W_{t,\\tau}\\leftarrow \\sum_{m=1}^M C_{t,m}\\exp(S_{\\tau,m}-R_{t,m})$ $W \\leftarrow W \\odot M_{\\mathrm{causal}}$ Output: $Y \\leftarrow WV$. import torch import torch.nn.functional as F def causal_flare_prefill(q, k, v, scale=1.0): \u0026#34;\u0026#34;\u0026#34; Algorithm 3: stable dense causal FLARE for training and prefill. Computes the full causal output in parallel using online-softmax prefix statistics to avoid numerical overflow in mixed precision. Args: q: [H, M, D] latent queries (learned, shared across batch) k: [B, H, T, D] token keys v: [B, H, T, D] token values scale: float key-query scale factor (default 1.0) Returns: Y: [B, H, T, D] token outputs \u0026#34;\u0026#34;\u0026#34; B, H, T, D = k.shape M = q.shape[1] # S[b, h, t, m] = scale * dot(k[b, h, t], q[h, m]) S = scale * torch.einsum(\u0026#39;bhtd,hmd-\u0026gt;bhtm\u0026#39;, k.float(), q.float()) # [B, H, T, M] # P[b, h, t, m] = softmax over M (scatter weights for token t) P = F.softmax(S, dim=-1) # [B, H, T, M] # Stable prefix statistics: running max R and normalized prefix sum L R = torch.full((B, H, 1, M), float(\u0026#39;-inf\u0026#39;), dtype=torch.float32, device=k.device) L = torch.zeros(B, H, 1, M, dtype=torch.float32, device=k.device) R_all = torch.zeros(B, H, T, M, dtype=torch.float32, device=k.device) L_all = torch.zeros(B, H, T, M, dtype=torch.float32, device=k.device) for t in range(T): s_t = S[:, :, t:t+1, :] # [B, H, 1, M] R_new = torch.maximum(R, s_t) L = L * torch.exp(R - R_new) + torch.exp(s_t - R_new) R = R_new R_all[:, :, t:t+1, :] = R L_all[:, :, t:t+1, :] = L # C[b, h, t, m] = P[t, m] / (L[t, m] + eps) — gather weight for latent m at step t C = P / (L_all + 1e-6) # [B, H, T, M] # W[b, h, t, tau] = sum_m C[t, m] * exp(S[tau, m] - R[t, m]) # exp(S[tau, m] - R[t, m]): [B, H, T, M] x [B, H, T, M] -\u0026gt; broadcast over tau exp_S = torch.exp(S.unsqueeze(2) - R_all.unsqueeze(3)) # [B, H, T, T, M] W = torch.einsum(\u0026#39;bhtm,bhtsm-\u0026gt;bhts\u0026#39;, C, exp_S) # [B, H, T, T] # Apply causal mask (token t may only attend to tau \u0026lt;= t) causal_mask = torch.tril(torch.ones(T, T, device=k.device, dtype=torch.bool)) W = W.masked_fill(~causal_mask, 0.0) Y = torch.einsum(\u0026#39;bhts,bhsd-\u0026gt;bhtd\u0026#39;, W, v.float()) # [B, H, T, D] return Y.to(v.dtype) Train vs prefill vs decode Causal FLARE supports three operational regimes, each with different priorities.\nTeacher-forced training processes the full sequence in parallel and is throughput-oriented. Algorithm 3 is the right choice: stable prefix statistics, chunking over time to avoid materializing $T \\times T$, and fused kernels for arithmetic intensity.\nInference prefill is algorithmically identical to training but with a different blocking profile. Prefill is latency-sensitive and may benefit from different tile sizes and more aggressive kernel fusion than the training path.\nAutoregressive decode is latency-critical and processes one token at a time. Algorithm 1 is ideal: update the cached latent state with each new $(k_t, v_t)$, then read $y_t$ from the updated latents. No attention matrix is ever constructed, and the state size $M$ is the only memory overhead beyond the KV cache.\nAdaptive attention state size A practical FLARE advantage is controllable latent/state budget:\nLarger state in prefill for fidelity Smaller state in decode for throughput This exposes a direct compute-memory-accuracy knob.\nSystems notes A few implementation details matter disproportionately.\nFP32 prefix accumulators. The running max and sum statistics accumulate error across many tokens. Accumulating in FP32 prevents catastrophic cancellation even when inputs are in BF16 or FP16.\nTime chunking. Processing time in chunks avoids materializing the full $T \\times T$ intermediate — which is precisely the goal. Chunk size trades register pressure against memory traffic and should be tuned per GPU.\nSeparate kernels per regime. Training, prefill, and decode have different access patterns and arithmetic intensities. A single fused kernel cannot be optimally tiled for all three; separate kernels let you autotune each independently.\nMemory bandwidth first. At long contexts, causal FLARE is memory-bandwidth-bound rather than compute-bound. Optimizing cache layout and minimizing global memory traffic matters more than maximizing FLOPs/s.\nReferences Puri, V. et al. FLARE: Fast Low-rank Attention Routing Engine. arXiv (2025). https://arxiv.org/abs/2508.12594 Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Dao, T. et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS (2022). https://arxiv.org/abs/2205.14135 Qin, Z. et al. The Devil in Linear Transformer. arXiv (2022). https://arxiv.org/abs/2210.10340 ","permalink":"https://vpuri3.github.io/blog/from-encoder-to-decoder-extending-flare-to-memory-efficient-causal-attention/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eFLARE was originally developed as an encoder-style global mixing primitive: learned latent queries gather information from many tokens, then scatter it back. The decoder setting is harder because causality changes algorithmic dependencies, numerical stability constraints, and what efficiency means in training versus inference.\u003c/p\u003e\n\u003cp\u003eThis post summarizes a practical path to causal FLARE for long-context language modeling. See also the \u003ca href=\"https://www.youtube.com/watch?v=8h9EXJqQUi0\"\u003edissertation proposal talk\u003c/a\u003e for broader context.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-changes-from-encoder-to-decoder\"\u003eWhat changes from encoder to decoder?\u003c/h2\u003e\n\u003cp\u003eEncoder attention is bidirectional: token $t$ can depend on any token $\\tau$. Decoder attention is causal: token $t$ may depend only on $\\tau \\le t$.\u003c/p\u003e","title":"From Encoder to Decoder: Extending FLARE to Memory-Efficient Causal Attention"},{"content":"Motivation Pairwise attention is powerful, but it compresses interaction structure into second-order forms. Most efficient attention methods try to approximate or factor the $N \\times N$ attention matrix. Triple attention takes a different perspective:\nInstead of modeling pairwise token interactions, build a structured higher-order memory and let tokens read from it.\nThis post explains how triple attention works conceptually, and how we implement it in Triton using fused kernels that scale linearly in sequence length.\nWhy third-order memory? Standard linear attention builds a pooled memory $S \\in \\mathbb{R}^{D \\times D}$:\n$$ S_{ij} = \\sum_{n=1}^{N} K_{ni} V_{nj}, $$and predicts with:\n$$ Y_{nj} = \\sum_{i=1}^{D} Q_{ni} S_{ij}. $$Triple attention instead accumulates a third-order state $S \\in \\mathbb{R}^{D_q \\times D_v \\times D_q}$:\n$$ S_{ijk} = \\sum_{n=1}^{N} K^{(1)}_{ni} V_{nj} K^{(2)}_{nk}, $$with output:\n$$ Y_{nj} = \\sum_{i=1}^{D} \\sum_{k=1}^{D} Q^{(1)}_{ni} S_{ijk} Q^{(2)}_{nk}. $$The sequence reduction stays linear in $N$, while representational capacity expands from $D \\times D$ to $D \\times D \\times D$. The tradeoff: compute scales as $O(N D_q^2 D_v)$, so this mechanism suits settings where $D_q$ is fixed and $N$ is large.\nFrom quadratic attention to structured memory Standard self-attention forms $A = \\text{softmax}(QK^\\top)V$, which requires materializing or implicitly computing an $N \\times N$ interaction. Triple attention avoids this entirely. Instead of computing token-to-token interactions, we construct a third-order state tensor $S \\in \\mathbb{R}^{D_q \\times D_v \\times D_q}$ that aggregates global information from all tokens in a streaming fashion. Each token then reads from this state via two learned query projections. No $N \\times N$ matrix is ever formed.\nTensor Shapes We begin with projected tensors:\n$Q_1, Q_2, K_1, K_2 \\in \\mathbb{R}^{B \\times H \\times N \\times D_q}$ $V \\in \\mathbb{R}^{B \\times H \\times N \\times D_v}$ For kernel simplicity, batch and heads are flattened:\nQ1, Q2, K1, K2: [BH, N, Dq] V: [BH, N, Dv] We allocate:\nSTATE: [BH, Dq, Dv, Dq] (accumulated in fp32) O: [BH, N, Dv] The memory cost is independent of sequence length $N$.\nForward pass The forward pass is split into two phases, each corresponding to a fused Triton kernel.\nPhase 1: Streaming state construction Each kernel instance tiles the indices $(i, j, k)$ of the state tensor, streams over tokens in chunks (e.g. 4096 at a time), and accumulates contributions into STATE using atomic adds in fp32.\n$$ S_{ijk} = \\sum_{n=1}^N K_{1,n,i} \\cdot V_{n,j} \\cdot K_{2,n,k} $$No token-token matrix is constructed; streaming over the sequence makes this linear in $N$. As a reference:\nimport torch def build_triple_state(k1, k2, v): \u0026#34;\u0026#34;\u0026#34; Phase 1: accumulate the third-order global memory. Args: k1: [B, H, N, Dq] first key projection k2: [B, H, N, Dq] second key projection v: [B, H, N, Dv] value projection Returns: state: [B, H, Dq, Dv, Dq] third-order state (fp32) \u0026#34;\u0026#34;\u0026#34; # S[b,h,i,j,k] = sum_n k1[n,i] * v[n,j] * k2[n,k] return torch.einsum(\u0026#39;bhni,bhnj,bhnk-\u0026gt;bhijk\u0026#39;, k1.float(), v.float(), k2.float()) Complexity: O(N D_q^2 D_v) — linear in sequence length. The Triton kernel tiles over $(i,j,k)$ and streams over $n$ to avoid materializing the full intermediate.\nPhase 2: Output contraction Each token reads from the precomputed STATE via two learned query projections:\n$$ O_n = \\sum_{i,j,k} Q_{1,n,i} \\cdot S_{ijk} \\cdot Q_{2,n,k} $$The state acts as a global communication hub — each token independently decides how to read from it via $Q_1$ and $Q_2$.\ndef read_triple_state(q1, q2, state): \u0026#34;\u0026#34;\u0026#34; Phase 2: contract each token\u0026#39;s queries against the global state. Args: q1: [B, H, N, Dq] first query projection q2: [B, H, N, Dq] second query projection state: [B, H, Dq, Dv, Dq] precomputed third-order memory Returns: y: [B, H, N, Dv] token outputs \u0026#34;\u0026#34;\u0026#34; # y[b,h,n,j] = sum_{i,k} q1[n,i] * state[i,j,k] * q2[n,k] return torch.einsum(\u0026#39;bhni,bhijk,bhnk-\u0026gt;bhnj\u0026#39;, q1.float(), state, q2.float()) This is structurally similar to routing through a learned latent space, but realized as a multilinear contraction.\nFull reference import torch def triple_attn(q1, q2, k1, k2, v): \u0026#34;\u0026#34;\u0026#34; Triple attention: linear-time third-order global memory. Args: q1: [B, H, N, Dq] first query projection q2: [B, H, N, Dq] second query projection k1: [B, H, N, Dq] first key projection k2: [B, H, N, Dq] second key projection v: [B, H, N, Dv] value projection Returns: y: [B, H, N, Dv] token outputs \u0026#34;\u0026#34;\u0026#34; state = build_triple_state(k1, k2, v) # [B, H, Dq, Dv, Dq] y = read_triple_state(q1, q2, state) # [B, H, N, Dv] return y.to(v.dtype) Why this scales Standard attention scales as:\n$$ O(N^2 D) $$Triple attention scales as:\n$$ O(N D_q^2 D_v) $$If $D_q$ is held fixed as $N$ grows, this is linear in sequence length.\nMemory never grows with $N^2$.\nThis makes it viable for long-sequence workloads, including:\nPDE surrogate models Large point-cloud processing Long-context sequence modeling Numerical considerations Several implementation details are critical:\nAccumulate STATE in fp32. Use tensor cores (TF32 where available). Store outputs in fp16/bf16. Chunk over sequence dimension to fit memory. Use atomic adds carefully to avoid race conditions. The mixed-precision strategy preserves stability while keeping memory bandwidth low.\nBackward pass The backward pass mirrors the forward decomposition.\nImplemented as a custom torch.autograd.Function, it performs:\n1. Accumulate dSTATE Compute:\n$$ dS = \\sum_n dO_n \\cdot Q_{1,n} \\cdot Q_{2,n} $$Streaming over tokens in chunks.\n2. Gradients for K1, K2, V Contract dSTATE with remaining factors:\ndK1 dK2 dV Each has its own fused Triton kernel.\n3. Gradients for Q1, Q2 Contract saved STATE with dO.\nThe code includes explicit einsum expressions for gradient verification, making parity testing against a reference implementation straightforward.\nConceptual perspective Triple attention reflects a broader idea:\nGlobal communication does not require pairwise token interaction.\nInstead of asking “which tokens attend to which?”, we ask:\nCan we compress global structure into a structured tensor, and let tokens read from it?\nThis viewpoint connects to:\nLow-rank attention Latent routing methods Multilinear tensor contractions Structured operator learning It also suggests a hypothesis:\nIf a structured self-attention mechanism captures global communication efficiently, it may extend naturally to causal and autoregressive settings.\nExploring that extension is ongoing work.\nOpen problems Better approximations to reduce $D^3$ pressure Hybrid blocks combining low-rank gather–scatter with triple memory Causal decoding variants for language modeling Closing thoughts Triple attention is not just a kernel experiment — it is an exploration of structured global memory.\nBy fusing state construction and output contraction in Triton, we obtain linear scaling in sequence length, stable mixed-precision execution, and a flexible multilinear attention primitive. This kernel serves as a foundation for further experiments in structured and adaptive attention mechanisms.\nThe full implementation is available in the FLARE repository alongside the paper (arXiv:2508.12594).\nReferences Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Katharopoulos, A. et al. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML (2020). https://arxiv.org/abs/2006.16236 Kozachinskiy, A. et al. Strassen Attention, Split VC Dimension and Compositionality in Transformers. arXiv (2025). https://arxiv.org/abs/2501.19215 Roy, A. et al. Fast and Simplex: 2-Simplicial Attention in Triton. arXiv (2025). https://arxiv.org/abs/2507.02754 Qin, Z. et al. The Devil in Linear Transformer. arXiv (2022). https://arxiv.org/abs/2210.10340 ","permalink":"https://vpuri3.github.io/blog/triple-attention-in-triton-building-a-third-order-memory-in-linear-time/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003ePairwise attention is powerful, but it compresses interaction structure into second-order forms. Most efficient attention methods try to approximate or factor the $N \\times N$ attention matrix. Triple attention takes a different perspective:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eInstead of modeling pairwise token interactions, build a structured higher-order memory and let tokens read from it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis post explains how triple attention works conceptually, and how we implement it in Triton using fused kernels that scale linearly in sequence length.\u003c/p\u003e","title":"Triple Attention in Triton: Building a Third-Order Memory in Linear Time"},{"content":"Beyond pairwise attention Softmax attention is an extremely expressive token-mixing primitive, but it is expensive. For a sequence of length $N$, the core interaction matrix $Q \\cdot K^\\top$ is $N \\times N$, which drives both runtime and memory. Linear transformers try to keep global context while avoiding the quadratic scaling. The catch is that the simplest linear attention formulations often lose a key ingredient that makes softmax attention work so well: token-specific routing.\nThis post has two goals. First, I want to explain why vanilla linear attention underperforms in many settings, not as a matter of “bad approximation,” but as a structural consequence of how the computation is arranged. Second, I want to sketch a set of experimental ideas I’ve been exploring around enhanced attention mechanisms. To be explicit: these are research notes. Some of these ideas are promising on paper, but they have not reliably panned out in my experiments so far. I’m writing them up anyway because the framing has been useful for me, and it may help others reason about the design space.\nOne additional constraint threads through the post: permutation equivariance. Many strong long-sequence models exploit 1D sequence structure (chunking, convolution, hierarchical pooling, scan recurrences). That is great for language, but it is brittle for unstructured grids and point sets where there is no canonical ordering and where I would like the model to be insensitive to permutations.\nPreliminaries Vanilla softmax attention (SDPA) Let $X \\in \\mathbb{R}^{N \\times C}$ be a sequence of $N$ tokens with $C$ features. We form queries/keys/values by linear projection:\n$$Q = XW^q,\\quad K = XW^k,\\quad V = XW^v$$where $W^q,W^k,W^v \\in \\mathbb{R}^{C \\times C}$. In multi-head attention we split features into $H$ heads of dimension $d = C/H$, writing\n$$Q=[Q_1,\\dots,Q_H],\\quad K=[K_1,\\dots,K_H],\\quad V=[V_1,\\dots,V_H].$$Scaled dot-product attention (SDPA) in head $h$ is\n$$Y_h = \\mathrm{SDPA}(Q_h,K_h,V_h;s) = \\mathrm{softmax}\\!\\left(\\frac{Q_hK_h^\\top}{s}\\right)V_h$$with $s \\approx \\sqrt{d}$. In vector form,\n$$y_i = \\sum_{j=1}^N \\frac{\\exp(q_i^\\top k_j)}{\\sum_{\\ell=1}^N \\exp(q_i^\\top k_\\ell)}\\cdot v_j.$$The quadratic cost comes from the pairwise interaction matrix $QK^\\top \\in \\mathbb{R}^{N\\times N}$. A useful mental model is: each query $q_i$ produces its own distribution over keys, so it can route information in a token-specific way. That “personalized routing” is what many linear-time mechanisms struggle to preserve.\nimport torch def sdpa_naive(q, k, v): \u0026#34;\u0026#34;\u0026#34; Scaled dot-product attention. Args: q: [B, H, N, D] queries (already split across heads) k: [B, H, N, D] keys v: [B, H, N, D] values scale: float attention scale (default 1.0) Returns: y: [B, H, N, D] output \u0026#34;\u0026#34;\u0026#34; scale = q.size(-1) ** -0.5 S = (q @ k.mT) * scale # [B, H, N, N] attn logits A = S.softmax(dim=-1) # [B, H, N, N] attn weights y = A @ v # [B, H, N, D] return y Linear attention The basic idea Linear attention replaces the softmax kernel with a feature map $\\phi(\\cdot)$ (often constrained to be positive) so that similarities become dot products in feature space. Define\n$$\\tilde{Q}=\\phi(Q),\\quad \\tilde{K}=\\phi(K).$$A typical (row-normalized) update is\n$$Y = \\mathrm{rownorm}(\\tilde Q\\tilde K^\\top)V,$$which in vector form is\n$$y_i = \\sum_{j=1}^N \\frac{(\\tilde q_i^\\top \\tilde k_j)}{\\sum_{\\ell=1}^N \\tilde q_i^\\top \\tilde k_\\ell}\\cdot v_j.$$The efficiency comes from associativity:\n$$y_i = \\frac{\\tilde q_i^\\top\\left(\\sum_{j=1}^N \\tilde k_j v_j^\\top\\right)}{\\tilde q_i^\\top\\left(\\sum_{\\ell=1}^N \\tilde k_\\ell\\right)}.$$If we define a pooled “memory” state $S=\\tilde K^\\top V \\in \\mathbb{R}^{D\\times D}$ and $z=\\tilde K^\\top\\mathbf{1}\\in\\mathbb{R}^D$, we can write $Y = \\dfrac{\\tilde Q \\cdot S}{\\tilde Q \\cdot z}$. This is linear in $N$ (up to constants) because the expensive terms are reductions over the sequence. At a systems level, linear attention is appealing: you can stream over tokens, maintain a running state, and avoid materializing $N\\times N$ matrices.\nimport torch def linear_attn(q, k, v, kernel=None): \u0026#34;\u0026#34;\u0026#34; Linear attention: linear-time global mixing via associative state reduction. Args: q: [B, H, N, D] queries k: [B, H, N, D] keys v: [B, H, N, D] values kernel: callable feature map φ applied to q and k (e.g. elu + 1) Returns: y: [B, H, N, D] output \u0026#34;\u0026#34;\u0026#34; if kernel is not None: q = kernel(q) k = kernel(k) state = k.mT @ v # [B, H, D, D] k_sum = k.sum(dim=2).unsqueeze(-1) # [B, H, D, 1] num = q @ state den = q @ k_sum return num / (den + 1e-6) Why vanilla linear attention often underperforms It helps to look at the structural form of the update and ignore normalization for intuition. Linear attention can be written as\n$$y_i = \\tilde q_i^\\top\\left(\\sum_{j=1}^N \\tilde k_j v_j^\\top\\right),$$i.e., $y_i = \\tilde q_i^\\top S$ where $S\\in\\mathbb{R}^{D\\times D}$.\nThe critical observation is simple: $\\tilde q_i$ changes with $i$, but the memory $S$ is shared across all tokens. Every token is querying the same pooled summary of key/value content. That makes the model efficient, but it also changes the nature of token mixing.\nToken-specific routing is weakened. In softmax attention, each query produces its own distribution over keys. In basic linear attention, each query projects against the same global state. You can still get token-dependent outputs because $\\tilde q_i$ varies, but the mechanism no longer has “choose a few values and ignore the rest” behavior in the same way.\nSmoothing is hard to avoid. The associativity trick forces compression before interaction. Fine-grained information has to survive the bottleneck $S$ to influence outputs. This tends to smear distinctions unless the feature map and projections work very hard to preserve them.\nThe bottleneck lives in feature space. Softmax attention carries $O(N^2)$ interaction capacity through an $N\\times N$ attention map. Linear attention collapses this into a feature-space quadratic form: $S \\in \\mathbb{R}^{D\\times D}$. Unless $\\phi$ is carefully chosen (e.g., kernelized approximations of $\\exp$), this can be structurally less expressive, not merely a “worse approximation.”\nThis is why the linear-transformer literature has so many “patches”: better feature maps (Performer/FAVOR+ style), gating and recurrence (RetNet/RWKV-like ideas), low-rank bottlenecks (Nyström, latent tokens), and various normalization tricks (including the practical observation that removing row-normalization and stabilizing with RMSNorm at the end can sometimes help).\nHigher-order attention: why pairwise may not be enough Softmax attention is pairwise: it scores $(q_i,k_j)$ and uses that to weight $v_j$. Many tasks can be solved with pairwise interactions plus depth, but there are reasons to explore explicit multi-way mixing.\nOn algorithmic tasks, it is natural to point at interactions that look like (digit1, digit2, carry). On PDE-like data, nonlinearities often couple multiple factors, and it is tempting to ask whether explicitly modeling multi-way interactions could reduce the burden on depth or help linear-time models recover some selectivity.\nOne concrete formalization is 2-simplex (3-way) attention, which replaces bilinear dot products with trilinear forms. I do not view this as “the answer,” but it is a clean starting point for thinking about higher-order token mixing.\n2-simplex attention (3-way interactions) Standard attention is built on the bilinear form\n$$\\langle x_1,x_2\\rangle = \\sum_{d=1}^D x_1[d]x_2[d].$$2-simplex attention generalizes this to a trilinear form\n$$\\langle x_1,x_2,x_3\\rangle = \\sum_{d=1}^D x_1[d]x_2[d]x_3[d].$$A naive 3-way attention mechanism defines a score tensor over triples. Let\n$$Q,K^1,K^2,V^1,V^2 \\in \\mathbb{R}^{N\\times D},$$and define\n$$S_{ijk}=\\sum_{d'=1}^D Q_{id'}K^1_{jd'}K^2_{kd'}.$$Then $A=\\mathrm{softmax}(S)$ (softmax over $(j,k)$) and\n$$Y_{id}=\\sum_{j,k=1}^N A_{ijk}V^1_{jd}V^2_{kd}.$$This is expressive, but it requires forming $N\\times N\\times N$ tensors, which is intractable for large $N$. So the question that keeps coming up is: can we capture some higher-order behavior without paying cubic cost?\nProposal: multilinear kernelized attention (linear-time simplex-style mixing) This is where the “research notes” part begins. The idea below is not a claim that I have a working model. It is a proposal that falls out of the same associativity trick that makes linear attention fast. It is conceptually clean, but in my experiments it has been finicky to stabilize and has not consistently beaten simpler baselines.\nThe key trick in linear attention is: avoid instantiating $N\\times N$ by collapsing into a $D\\times D$ state. We can do something similar for 2-simplex attention by dropping softmax and restructuring the computation so the triple interaction factorizes into feature-space memories.\nStart from the unnormalized 3-way update:\n$$A_{ijk}=\\sum_{d'=1}^D Q_{id'}K^1_{jd'}K^2_{kd'},$$and\n$$Y_{id}=\\sum_{j,k=1}^N A_{ijk}V^1_{jd}V^2_{kd}.$$Rearranging sums gives\n$$Y_{id}=\\sum_{d'=1}^D Q_{id'}\\left(\\sum_{j=1}^N K^1_{jd'}V^1_{jd}\\right)\\left(\\sum_{k=1}^N K^2_{kd'}V^2_{kd}\\right).$$Define feature-space memories\n$$S^1=(K^1)^\\top V^1 \\in \\mathbb{R}^{D\\times D}$$and\n$$S^2=(K^2)^\\top V^2 \\in \\mathbb{R}^{D\\times D}.$$Then the output becomes $Y = Q\\left(S^1 \\odot S^2\\right)$ where $\\odot$ is the elementwise Hadamard product. This is linear in $N$ because forming each $S^\\ell$ is a single reduction pass over tokens.\nGeneralization to L-way multilinear attention With $L$ key/value pairs\n$$\\{(K^\\ell,V^\\ell)\\}_{\\ell=1}^L,$$define\n$$S^\\ell=(K^\\ell)^\\top V^\\ell \\in \\mathbb{R}^{D\\times D}$$and compute\n$$Y = Q\\left(S^1 \\odot S^2 \\odot \\cdots \\odot S^L\\right).$$You can view this as a generalized linear-time $L$-simplicial attention.\nOne motivation I find useful: “mixture of memories” mechanisms tend to combine multiple memories additively (a router picks or mixes). This proposal combines them multiplicatively, which behaves like feature-wise gating: all memories have to agree, and features can be amplified or suppressed by products.\nIn practice, the open questions dominate:\nnormalization (row-norm vs no row-norm + RMSNorm, or explicit gating), whether RoPE-like positional structure still helps or becomes awkward under multiplicative state composition, and whether the mechanism learns something meaningfully different than “more projections + gating.” So far, my results have not been clean enough to recommend this as a drop-in replacement. The main value for me has been as a lens: if the bottleneck is the shared memory $S$, one way to increase expressivity is to increase the structure of the memory interaction without blowing up the $N$ dependence.\nimport torch def multilinear_attn(q, ks, vs): \u0026#34;\u0026#34;\u0026#34; Multilinear attention: elementwise product of L feature-space memories. Each (ks[l], vs[l]) pair contributes one D×D memory state; the states are multiplied elementwise before contracting with q. Args: q: [B, H, N, D] queries ks: [L, B, H, N, D] L key projections vs: [L, B, H, N, D] L value projections Returns: y: [B, H, N, D] output \u0026#34;\u0026#34;\u0026#34; states = ks.mT @ vs # [L, B, H, D, D] state = states.prod(dim=0) # [B, H, D, D] return q @ state Strassen-style linearized mixing can be viewed as another structured memory composition:\nimport torch def strassen_linear_attn(q, k1, v1, k2, v2, g1, g2, g3, g4, scale=None): \u0026#34;\u0026#34;\u0026#34; Strassen-style linearized mixing: structured combination of two memories. Args: q: [B, H, N, D] queries k1: [B, H, N, D] first key projection v1: [B, H, N, D] first value projection k2: [B, H, N, D] second key projection v2: [B, H, N, D] second value projection g1-g4: [B, H, N, D] learned gate tensors scale: float normalization scale Returns: y: [B, H, N, D] output \u0026#34;\u0026#34;\u0026#34; if scale is None: N = q.size(-2) scale = N ** 05 S1 = (k1.mT / scale) @ (v1 / scale) # [B, H, D, D] S2 = (k2.mT / scale) @ (v2 / scale) # [B, H, D, D] v1_sum = v1.sum(dim=2, keepdim=True) # [B, H, 1, D] v2_sum = v2.sum(dim=2, keepdim=True) # [B, H, 1, D] y1 = (q @ S1) * v2_sum y2 = (S1 * S2).sum(dim=-2, keepdim=True).expand_as(q) y3 = (q @ S2) * v1_sum y4 = q @ (S1 * S2) return y1 * g1 + y2 * g2 + y3 * g3 + y4 * g4 Proposal: higher-order memory states (triple / quad attention) A second idea is to increase the capacity of the pooled state itself. Standard linear attention compresses everything into $S\\in\\mathbb{R}^{D\\times D}$. If you believe tasks are bottlenecked by this memory, you can increase its order.\nTriple attention uses a $D\\times D\\times D$ state:\n$$S_{ijk} = \\sum_{n=1}^N K^1_{ni}V_{nj}K^2_{nk},$$and\n$$Y_{nj} = \\sum_{i,k=1}^D Q^1_{ni}S_{ijk}Q^2_{nk}.$$Quad attention similarly uses a $D\\times D\\times D\\times D$ state:\n$$S_{ijk\\ell}=\\sum_{n=1}^N K^1_{ni}V_{nj}K^2_{nk}K^3_{n\\ell},$$and\n$$Y_{nj}=\\sum_{i,k,\\ell=1}^D Q^1_{ni}S_{ijk\\ell}Q^2_{nk}Q^3_{n\\ell}.$$These preserve linear scaling in $N$ but increase polynomial cost in $D$. That makes them plausible only when $D$ is small and kernels are highly optimized. In my own experiments, stability and memory traffic become the main hurdles quickly, so I view these as “maybe useful for specific bottlenecked settings,” not a general recipe.\nimport torch def triple_attn(q1, q2, k1, k2, v): \u0026#34;\u0026#34;\u0026#34; Triple attention: third-order feature-space memory, linear in N. Args: q1: [B, H, N, D] first query projection q2: [B, H, N, D] second query projection k1: [B, H, N, D] first key projection k2: [B, H, N, D] second key projection v: [B, H, N, D] values Returns: y: [B, H, N, D] output \u0026#34;\u0026#34;\u0026#34; state = torch.einsum(\u0026#39;bhni,bhnj,bhnk-\u0026gt;bhijk\u0026#39;, k1, v, k2) # [B, H, D, D, D] return torch.einsum(\u0026#39;bhni,bhijk,bhnk-\u0026gt;bhnj\u0026#39;, q1, state, q2) def quad_attn(q1, q2, q3, k1, k2, k3, v): \u0026#34;\u0026#34;\u0026#34; Quad attention: fourth-order feature-space memory, linear in N. Args: q1-q3: [B, H, N, D] query projections k1-k3: [B, H, N, D] key projections v: [B, H, N, D] values Returns: y: [B, H, N, D] output \u0026#34;\u0026#34;\u0026#34; state = torch.einsum(\u0026#39;bhni,bhnj,bhnk,bhnl-\u0026gt;bhijkl\u0026#39;, k1, v, k2, k3) # [B, H, D, D, D, D] return torch.einsum(\u0026#39;bhni,bhijkl,bhnk,bhnl-\u0026gt;bhnj\u0026#39;, q1, state, q2, q3) Where low-rank bottlenecks fit (and why linearizing FLARE collapses) Low-rank methods like FLARE reduce cost by routing through an intermediate set of latent tokens ($M\\ll N$). A subtle point is that they rely on a nonlinearity (softmax) that prevents full associativity collapse. If you fully linearize a two-stage gather–scatter mechanism, you typically recover something like $Y \\approx K\\,(Q^\\top Q)\\,(K^\\top V)$, which is still governed by a feature-space state and no longer depends on $M$ in an interesting way.\nI like keeping this sanity check in mind: if a design’s efficiency comes purely from associativity, then without an additional mechanism (nonlinearity, gating, recurrence, or structured bottlenecks) it tends to inherit the same shared-memory limitations.\nFLARE gather-scatter:\nimport torch.nn.functional as F def flare_mixer(q, k, v, scale=1.0): \u0026#34;\u0026#34;\u0026#34; FLARE gather–scatter: low-rank global mixing via two SDPA calls. Args: q: [H, M, D] latent queries (per head, shared across batch) k: [B, H, N, D] token keys v: [B, H, N, D] token values scale: float attention scale Returns: y: [B, H, N, D] mixed token outputs \u0026#34;\u0026#34;\u0026#34; qb = q.unsqueeze(0) # [1, H, M, D] z = F.scaled_dot_product_attention(qb, k, v, scale=scale) # [B, H, M, D] y = F.scaled_dot_product_attention(k, qb, z, scale=scale) # [B, H, N, D] return y Practical notes (from my experiments) A few things that have mattered more than I expected:\nRMSNorm has been more stable than LayerNorm in mixed precision for these variants. Normalization placement can dominate behavior. A recurring trick in linear attention is to simplify or remove row-normalization and stabilize later (e.g., RMSNorm after the mixer). Hybrid stacks are worth trying. Interleaving low-rank blocks with multilinear or higher-order blocks could be a reasonable way to trade off routing flexibility and stability, even if the standalone higher-order block is finicky. Benchmarks I care about I’m mainly interested in whether these mechanisms work beyond toy settings:\nPDE surrogates (steady and transient), where permutation equivariance matters and token ordering tricks are brittle. Long Range Arena, as a stress test for long-context sequence modeling. Comprehensive Attention Benchmark, to compare operators across a broader task suite. Takeaways Vanilla linear attention is efficient because of associativity, but that same compression creates a shared-memory bottleneck that weakens token-specific routing. Higher-order attention is a principled direction if you believe pairwise mixing is the wrong inductive bias for certain tasks, but naive formulations are intractable. Multilinear “memories” provide a clean linear-time way to inject higher-order structure, but in my experiments so far they have been hard to stabilize and have not consistently outperformed simpler baselines. If there is a practical path forward, I suspect it will involve careful normalization and gating, and likely hybrid designs that combine low-rank routing with higher-order feature-space interactions. If I were building the next round of experiments, my default plan would be:\nstart with a strong $L=1$ linear baseline (with modern normalization), then test $L=2$ multilinear as the cheapest higher-order extension, and only pursue triple/quad-state attention if there is clear evidence that the $D\\times D$ memory is the dominant bottleneck. References Katharopoulos, A. et al. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML (2020). https://arxiv.org/abs/2006.16236 Choromanski, K. et al. Rethinking Attention with Performers. ICLR (2021). https://arxiv.org/abs/2009.14794 Xiong, R. et al. Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention. AAAI (2021). https://arxiv.org/abs/2102.03902 Sun, Y. et al. Retentive Network: A Successor to Transformer for Large Language Models. arXiv (2023). https://arxiv.org/abs/2307.08621 Zhang, B., and Sennrich, R. Root Mean Square Layer Normalization. NeurIPS (2019). https://arxiv.org/abs/1910.07467 Tay, Y. et al. Long Range Arena: A Benchmark for Efficient Transformers. ICLR (2021). https://arxiv.org/abs/2011.04006 Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Kozachinskiy, A. et al. Strassen Attention, Split VC Dimension and Compositionality in Transformers. arXiv (2025). https://arxiv.org/abs/2501.19215 Roy, A. et al. Fast and Simplex: 2-Simplicial Attention in Triton. arXiv (2025). https://arxiv.org/abs/2507.02754 Qin, Z. et al. The Devil in Linear Transformer. arXiv (2022). https://arxiv.org/abs/2210.10340 ","permalink":"https://vpuri3.github.io/blog/adventures-in-high-order-attention/","summary":"\u003ch2 id=\"beyond-pairwise-attention\"\u003eBeyond pairwise attention\u003c/h2\u003e\n\u003cp\u003eSoftmax attention is an extremely expressive token-mixing primitive, but it is expensive. For a sequence of length $N$, the core interaction matrix $Q \\cdot K^\\top$ is $N \\times N$, which drives both runtime and memory. Linear transformers try to keep global context while avoiding the quadratic scaling. The catch is that the simplest linear attention formulations often lose a key ingredient that makes softmax attention work so well: \u003cstrong\u003etoken-specific routing\u003c/strong\u003e.\u003c/p\u003e","title":"Higher-Order Attention in Linear Time: Multilinear Memories and Simplex Mixing"},{"content":"Attention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.\nRather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?\nThis post complements our latest paper (arXiv:2508.12594). See my dissertation proposal talk on this topic!\nWhy this problem is hard Attention is now used across language, vision, and scientific machine learning. The scaling bottleneck appears everywhere. For a sequence of $N$ tokens, standard self-attention requires $O(N^2)$ compute and memory, which quickly becomes impractical beyond tens of thousands of tokens.\nWe are particularly motivated by partial differential equation (PDE) surrogate modeling, one of the most demanding settings for attention:\nInputs are large and geometry dependent. Meshes are often unstructured, so ordering tricks are unreliable. Accurate predictions require long-range interactions across the domain. Linear attention methods reduce cost but often struggle to match accuracy in these regimes. The challenge is not just scaling attention, but scaling it without losing expressive global communication.\nPDE surrogate modeling For a PDE\n$$ \\mathcal{L}(\\boldsymbol{x}, t, \\boldsymbol{u}; \\boldsymbol{\\mu}), \\quad \\boldsymbol{x}\\in\\Omega, $$ we view learning as approximating a solution operator $\\mathcal{G}$ that maps parameters to fields: $$ \\boldsymbol{u}(\\boldsymbol{x})=\\mathcal{G}(\\boldsymbol{\\mu})(\\boldsymbol{x}). $$ In practice, we train on many $\\boldsymbol{\\mu}\\to \\boldsymbol{u}$ pairs and require generalization to unseen $\\boldsymbol{\\mu}$. In the slide below, $\\boldsymbol{\\mu}$ is effectively the geometry $\\Omega$, and the target is a strain field on unseen domains. Token mixing in transformers A transformer block is conceptually two operations:\npointwise nonlinear transforms (MLPs), global message passing (self-attention). For PDE data, we avoid token clustering heuristics and treat each mesh point as a token. Standard attention computes, for each output token, a distribution over all inputs:\n$$ y_n = \\sum_m \\frac{\\exp\\left(q_n^T \\cdot k_l \\right)}{\\sum_l \\exp \\left(q_n^\\top \\cdot k_l \\right)} v_l $$This all-to-all routing is powerful but expensive. At million-token scale, even a single forward-backward pass can take tens of seconds. The goal is to keep global communication while reducing the number of messages that must be exchanged.\nWhy global communication still matters In many physical systems, forward operators are local, but solution operators are effectively dense. A model that communicates only locally will miss long-range dependencies that determine the final field.\nHowever, physical signals are often smooth at resolved scales. Smoothness means many high-frequency modes are weak; there is genuine redundancy in a fully resolved signal. So if two nearby source points carry nearly the same information for a far target point, we should be able to route one combined message instead of two separate ones — we do not need all $N^2$ interactions.\nThe core FLARE contribution is to operationalize this physics-guided redundancy into a sequence-agnostic attention mechanism that remains general-purpose.\nFLARE How we got to FLARE The starting point was a simple systems question: what actually makes attention expensive?\nAttention can be interpreted as a routing mechanism where each token decides how to combine information from all others. The quadratic cost comes from explicitly computing all pairwise interactions, even when many of those interactions are redundant.\nFrom a physics perspective, many solution operators exhibit low effective dimensionality. Spectral decay in physical fields suggests that long-range interactions can often be captured through a smaller set of communication pathways.\nThis led to the idea of introducing a small set of latent routing tokens that act as intermediaries. Instead of every token talking to every other token directly, tokens communicate through these shared hubs. The challenge was designing this mechanism so that it remains global, interpretable, and easy to optimize.\nFLARE Method FLARE implements global communication through a two-step gather–scatter mechanism, but it does so using standard, highly optimized attention primitives.\nScaled dot-product attention (SDPA) The basic attention primitive is scaled dot-product attention. Given queries $Q \\in \\mathbb{R}^{N_q \\times d}$, keys $K \\in \\mathbb{R}^{N_k \\times d}$, and values $V \\in \\mathbb{R}^{N_k \\times d_v}$, attention is\n$$ \\mathrm{SDPA}(Q,K,V) = \\mathrm{softmax}\\!\\left(\\frac{Q \\cdot K^\\top}{\\sqrt{d}}\\right)V. $$Naively, this suggests computing the score matrix $S = Q \\cdot K^\\top \\in \\mathbb{R}^{N_q \\times N_k}$, applying softmax, and then multiplying by $V$. That approach is expensive because it materializes an $N_q \\times N_k$ matrix.\nModern frameworks provide SDPA as a fused kernel that computes the same result without materializing the full attention-weight matrix in memory. In PyTorch, torch.nn.functional.scaled_dot_product_attention is the standard entry point, and under the hood it can dispatch to fused implementations (for example FlashAttention-style kernels) that stream over blocks of $K,V$ and maintain the softmax normalization online. Practically, SDPA lets you use the standard attention math while keeping memory use close to $O((L_q+L_k)d)$ rather than $O(L_qL_k)$.\nFLARE is built to use SDPA twice: once to gather information into a small latent set, and once to scatter it back.\nStep 1: Gather (encode) Introduce $M \\ll N$ latent tokens per head. Let $Q_h \\in \\mathbb{R}^{M \\times d}$ be latent queries, and let $K_h, V_h \\in \\mathbb{R}^{N \\times d}$ be token keys/values of head $h$. The gather step pools from the $N$ tokens into $M$ latents:\n$$ Z_h = \\mathrm{SDPA}(Q_h, K_h, V_h). $$In matrix notation this is\n$$ W_{\\mathrm{enc}} = \\mathrm{softmax}(Q_h \\cdot K_h^\\top) \\in \\mathbb{R}^{M \\times N}, \\qquad Z_h = W_{\\mathrm{enc}} \\cdot V_h. $$Importantly, although $W_{\\mathrm{enc}}$ is a useful conceptual object, SDPA computes $Z_h$ without explicitly materializing $W_{\\mathrm{enc}}$.\nStep 2: Scatter (decode) Next, the latents broadcast information back to all $N$ tokens using the reverse affinity pattern:\n$$ Y_h = \\mathrm{SDPA}(K_h, Q_h, Z_h). $$In matrix form,\n$$ W_{\\mathrm{dec}} = \\mathrm{softmax}(K_h \\cdot Q_h^\\top) \\in \\mathbb{R}^{N \\times M}, \\qquad Y_h = W_{\\mathrm{dec}} \\cdot Z_h. $$Combining both steps gives an implicit global communication operator:\n$$ Y_h = (W_{\\mathrm{dec}} \\cdot W_{\\mathrm{enc}}) \\cdot V_h. $$The product $W_{\\mathrm{dec}} \\cdot W_{\\mathrm{enc}} \\in \\mathbb{R}^{N \\times N}$ is a dense global mixing matrix, but its rank is at most $M$. FLARE therefore achieves global communication at $O(NM)$ cost per head (with $M \\ll N$), and the SDPA kernels avoid ever forming the intermediate $N \\times M$ or $M \\times N$ attention matrices in memory.\nMinimal implementation Below is the core mixer expressed directly with PyTorch SDPA. This is essentially the gather–scatter operator in a few lines.\nfrom torch.nn.functional import scaled_dot_product_attention as SDPA def flare_multihead_mixer(Q, K, V): \u0026#34;\u0026#34;\u0026#34; Args: Q: [H, M, D] latent queries (per head) K: [B, H, N, D] token keys V: [B, H, N, D] token values Returns: Y: [B, H, N, D] mixed token outputs \u0026#34;\u0026#34;\u0026#34; Qb = Q.unsqueeze(0) # [1, H, M, D] to match SDPA batching # Gather: tokens -\u0026gt; latents Z = SDPA(Qb, K, V, scale=1.0) # [B, H, M, D] # Scatter: latents -\u0026gt; tokens Y = SDPA(K, Qb, Z, scale=1.0) # [B, H, N, D] return Y The full FLARE block stacks this mixer with standard projection/MLP components: Compared to standard attention, FLARE reduces both time and peak memory roughly linearly with $M/N$: Results FLARE enables million-token training on a single GPU while maintaining strong accuracy on PDE surrogate tasks. The key improvement is not only asymptotic scaling but a practical runtime and memory profile that allows end-to-end training.\nOn sequence benchmarks such as Long Range Arena, FLARE is competitive with efficient-transformer alternatives and maintains strong average accuracy, showing that the mechanism is not restricted to PDE-only structure.\nDeep dive: why this design works Understanding why FLARE works requires looking at the structure of the gather–scatter operator and how different design choices affect learning dynamics.\nGather–scatter as communication hubs Each latent token acts as both a pooling hub and a routing hub. During encoding, it aggregates information from tokens that align with its query pattern. During decoding, it distributes that information back to tokens that match its key pattern.\nThis creates a contraction–expansion communication structure that efficiently mixes global information.\nSymmetric operators improve stability The encode and decode steps are derived from the same query-key geometry, creating a structurally aligned operator. This symmetry reduces redundant parameterization and leads to more stable optimization compared to asymmetric variants.\nFixed queries provide a stable routing structure Latent queries are learned but input independent. This gives a consistent communication basis across inputs, making the operator easier to optimize and interpret.\nThe reduced query dynamism is compensated by expressive key and value encoders, which adapt routing patterns to each input.\nRepeated global mixing is more effective than latent refinement Empirically, allocating compute to repeated gather–scatter operations produces better performance than stacking heavy latent self-attention blocks. The benefit comes from repeatedly mixing global information rather than refining latent representations in isolation.\nIndependent latents across heads increase diversity Allowing each head to have its own latent tokens produces more diverse communication pathways. Different heads learn complementary routing structures, improving accuracy and depth scaling.\nPractical takeaways Several design principles emerge:\nUse explicit low-rank communication rather than approximating local attention. Allocate compute to repeated global mixing. Keep routing pathways independent across heads. Use expressive key and value encoders when queries are fixed. Closing thoughts FLARE shows that scaling attention is not only about approximating softmax more efficiently. By rethinking attention as a communication operator, we can design mechanisms that preserve global information flow while dramatically reducing cost.\nThe broader lesson is that many large-scale problems have structure that can be exploited. Low-rank communication is one way to capture that structure without sacrificing flexibility, opening the door to practical million-token models on modest hardware.\nReferences Puri, V. et al. FLARE: Fast Low-rank Attention Routing Engine. arXiv (2025). https://arxiv.org/abs/2508.12594 Vaswani, A. et al. Attention Is All You Need. NeurIPS (2017). https://arxiv.org/abs/1706.03762 Dao, T. et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS (2022). https://arxiv.org/abs/2205.14135 PyTorch Docs. torch.nn.functional.scaled_dot_product_attention. https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html Tay, Y. et al. Long Range Arena: A Benchmark for Efficient Transformers. ICLR (2021). https://arxiv.org/abs/2011.04006 ","permalink":"https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/","summary":"\u003cp\u003eAttention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.\u003c/p\u003e\n\u003cp\u003eRather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?\u003c/p\u003e","title":"Scaling attention to 1M tokens on a single GPU"},{"content":"From POD to neural manifolds Reduced-order modeling (ROM) is ultimately about preserving dominant system behavior with far fewer degrees of freedom. Classical projection methods, convolutional autoencoder ROMs, and modern neural-field formulations all target this same objective, but they make different assumptions about (i) how we represent the state and (ii) how we evolve it in time.\nThis post walks through that progression and focuses on Smooth Neural Field ROM (SNF-ROM): a projection-based nonlinear ROM that uses continuous neural fields as the state representation and explicitly supports physics-based differentiation and time integration during deployment.\nPreprint: arXiv:2405.14890\nJ. Comp. Phys. paper: https://doi.org/10.1016/j.jcp.2025.113957\nSee my dissertation proposal talk on this topic!\nWhy ROM matters for engineering workflows High-fidelity simulation is now central to modern engineering pipelines, from fluid mechanics to control and design. However, high accuracy typically requires resolving many spatial and temporal scales, which makes PDE solves expensive. These costs become the bottleneck in workflows that require repeated evaluations, such as design-space exploration, optimization, and uncertainty quantification.\nMachine learning offers a complementary approach. Instead of solving the full PDE from scratch every time, we can learn a surrogate model from data generated by high-fidelity simulations. Training is expensive — the model must observe many examples across parameter settings, geometries, and time horizons — but the cost is paid once offline. At deployment time, the learned model evaluates new scenarios orders of magnitude faster than a traditional solver. In this sense, machine learning amortizes the cost of expensive PDE solves across many downstream queries.\nA particularly important class of surrogates is reduced-order models (ROMs). The central idea behind ROMs is that although PDE states live in extremely high-dimensional spaces (millions of degrees of freedom in realistic simulations), the effective dynamics often evolve on a much lower-dimensional structure. ROMs aim to discover this structure and use it to simulate the system using far fewer variables. Traditionally, this was done with linear subspace methods such as Proper Orthogonal Decomposition (POD) combined with Galerkin projection. Modern approaches extend this idea using machine learning to learn nonlinear representations that can better capture complex dynamics.\nIn this post, we focus on neural reduced-order modeling: learn a low-dimensional representation directly from data, then evolve that representation using physics-aware structure. Instead of predicting the full state everywhere, the model predicts a compact latent state that encodes the dominant behavior of the system, and the full field can be reconstructed from this latent representation. This separates the problem into two parts: representing the state efficiently and modeling how that representation evolves in time.\nBefore diving into SNF-ROM, it is helpful to recall the manifold learning viewpoint that underlies most nonlinear ROM methods. The core assumption is that although simulation states are extremely high-dimensional, they often lie close to a smooth, low-dimensional manifold embedded in the ambient space. For example, fluid flow snapshots over time may evolve primarily along a small number of coherent structures or modes. Learning this manifold provides a natural coordinate system in which the dynamics can be expressed compactly, and the role of a ROM is to learn both the geometry of this manifold and the dynamics that govern motion along it.\nThis perspective is illustrated by contrasting the full-order model (FOM) with linear and nonlinear ROMs. The governing PDE can be written abstractly as\n$$ \\frac{\\partial u}{\\partial t} = \\mathcal{L}(x,t,u;\\mu), $$where $u(x,t;\\mu)$ is the high-dimensional state, $x$ is spatial position, $t$ is time, and $\\mu$ denotes parameters. In a full discretization, the state is represented as\n$$ u(x,t;\\mu) \\approx g_{\\mathrm{FOM}}(x,\\bar u(t;\\mu)) = \\Phi(x)\\,\\bar u(t;\\mu), $$where $\\Phi(x)$ is a spatial basis and $\\bar u \\in \\mathbb{R}^{N_{\\mathrm{FOM}}}$ are high-dimensional coefficients. A linear POD-ROM assumes the dynamics evolve in a low-dimensional linear subspace,\n$$ \\bar u(t;\\mu) \\approx \\bar u_0 + \\mathbf{P}\\,\\tilde u(t;\\mu), $$where $\\mathbf{P}$ is a projection matrix and $\\tilde u \\in \\mathbb{R}^{N_{\\mathrm{Lin\\text{-}ROM}}}$ are reduced coordinates. A nonlinear ROM instead assumes the state lies on a curved manifold and uses a learned decoder,\n$$ u(x,t;\\mu) \\approx g_{\\mathrm{ROM}}(x,\\tilde u(t;\\mu)) = \\mathrm{NN}_\\theta(x,\\tilde u(t;\\mu)), $$where $\\mathrm{NN}_\\theta$ is a neural field mapping spatial coordinates and latent variables to the state. The dimensionality relationship\n$$ N_{\\mathrm{NI\\text{-}ROM}} \\le N_{\\mathrm{Lin\\text{-}ROM}} \\ll N_{\\mathrm{FOM}} $$captures the intuition that nonlinear manifolds can often represent system behavior with fewer degrees of freedom than linear subspaces while still approximating the full system accurately.\nLimitations of current ROMs (why nonlinear + why \u0026ldquo;smooth\u0026rdquo;) It’s tempting to think ROM is “solved” once you can compress snapshots well. The hard part is what happens online, when you traverse the reduced manifold according to the governing PDE.\n1) Linear subspace ROMs can be fundamentally inefficient Classical POD-ROM assumes solutions live near a linear subspace. This works best when the Kolmogorov $n$-width decays quickly (roughly: a small number of modes captures most variation). But in advection-dominated flows and other problems with slow Kolmogorov $n$-width decay, the number of POD modes required for accuracy can become large, reducing speedups and sometimes harming stability.\n2) Nonlinear ROMs often struggle where physics enters: derivatives Neural manifold ROMs (e.g., CAE-ROM) can achieve excellent compression, but online physics requires evaluating spatial operators: gradients, Laplacians, Jacobians of the decoder map, and so on. A recurring practical issue is that neural fields are not guaranteed to interpolate smoothly, and their spatial derivatives can be noisy. If your derivative estimates are wrong, your dynamics are wrong.\nMany nonlinear ROM pipelines handle this by introducing a coarse auxiliary grid and applying low-order finite differences for derivatives. That workaround can:\nrequire maintaining a background mesh (extra memory and engineering), introduce sensitivity to differencing hyperparameters, become brittle near boundaries, shocks, or sharp features. The punchline is: if you want to do physics-based online integration, you need reliable differentiation through your representation.\nSNF-ROM is designed around this constraint.\nWhere convolutional autoencoder ROMs struggle Convolutional autoencoder ROMs can compress state fields effectively, but the compression/decompression workflow does not directly control latent trajectory quality during online integration. In practice, this mismatch can produce drift and degraded physical consistency.\nThere are two coupled issues:\nThe learned manifold may be accurate at training snapshots but poorly behaved between them. The decoder’s derivatives (needed for dynamics) can be inaccurate or unstable. If the online stage uses the PDE to evolve the reduced state, small derivative errors can compound quickly.\nSNF-ROM: physics-based dynamics with neural fields SNF-ROM is a projection-based nonlinear ROM that keeps the best parts of classical ROMs — Galerkin projection and classical time integrators — while replacing linear subspaces with a continuous neural field representation. The key idea is not just to compress the state, but to build a representation on which the governing physics can be evaluated reliably during deployment.\nAt its core, SNF-ROM uses a neural field decoder that maps spatial coordinates and a low-dimensional latent to a field value:\n$$u(\\mathbf{x}, t; \\boldsymbol{\\mu}) \\approx f_\\theta(\\mathbf{x}, \\tilde{u}(t,\\boldsymbol{\\mu})),$$where $\\tilde{u}(t,\\boldsymbol{\\mu}) \\in \\mathbb{R}^r$ is the reduced state. During the online stage, SNF-ROM evolves $\\tilde{u}(t)$ using projection-based dynamics, evaluating PDE operators through automatic differentiation on the neural field. This means you can differentiate the representation to form the spatial operators needed for the reduced dynamics, then integrate the resulting reduced ODE with standard time-stepping.\nTwo design choices make this work reliably in practice.\n1. Constrained manifold formulation (trajectory regularity) In classical ROMs, the reduced state evolves in a low-dimensional coordinate system whose geometry is fixed by the chosen basis. In nonlinear ROMs, this coordinate system becomes a learned manifold. However, if this manifold is irregular or poorly behaved between training samples, trajectories computed during online integration can drift or become numerically unstable.\nSNF-ROM addresses this by explicitly modeling the reduced state as a smooth function of parameters and time. Conceptually, instead of learning isolated latent codes for snapshots, the model learns a continuous mapping\n$$ \\tilde{u} = \\tilde{u}(t, \\mu), $$where $\\tilde{u} \\in \\mathbb{R}^r$ is the reduced state. This encourages trajectories that vary smoothly as time evolves or parameters change. From a numerical perspective, this matters because online deployment involves stepping along this manifold using the governing PDE. If the manifold has sharp curvature or local irregularities, small integration errors can amplify. Enforcing trajectory regularity therefore improves stability and allows larger time steps during reduced-order integration.\n2. Neural field regularization (derivative fidelity) The second design choice focuses on the representation of the physical state itself. SNF-ROM uses a neural field decoder\n$$ u(x,t;\\mu) \\approx f_\\theta(x, \\tilde{u}(t,\\mu)), $$which maps spatial coordinates and reduced variables to field values. Because the governing PDE involves spatial operators (gradients, Laplacians, fluxes), the accuracy of these derivatives is critical. Standard neural decoders can produce visually accurate reconstructions while still having noisy or oscillatory derivatives, which leads to incorrect physics when evaluating operators.\nSNF-ROM therefore explicitly regularizes the neural field to encourage smoothness and suppress high-frequency artifacts. This improves derivative fidelity so that operators like\n$$ \\nabla u, \\quad \\Delta u, \\quad \\mathcal{L}(u) $$can be computed using automatic differentiation. The key benefit is that PDE operators are evaluated directly on the learned representation without relying on auxiliary grids or finite-difference approximations. This makes the reduced model both grid-free and numerically stable.\nTogether, these two components serve one goal: do physics on the learned manifold, not just reconstruction. The model is trained so that both the geometry of the manifold and the operators acting on it are well behaved during online simulation.\nIf derivatives of the learned field are noisy, PDE-consistent inference degrades because the projected dynamics depend directly on these derivatives. By enforcing smooth neural fields, SNF-ROM ensures that gradients and higher-order operators remain accurate and stable during integration.\nSmoothness: two regularized variants The paper presents two regularized variants designed to suppress high-frequency artifacts in derivatives:\nSNFL-ROM: Lipschitz-style regularization to encourage smooth neural mappings. SNFW-ROM: weight regularization motivated by controlling oscillations that appear in derivative expressions (particularly relevant with sinusoidal activations). Both are designed to produce smooth neural fields whose derivatives match the underlying signal more closely, which improves robustness during deployment-time dynamics evaluation.\nResults 1D viscous Burgers SNF-ROM variants maintain latent trajectory agreement under online solves where baseline CAE-ROM trajectories deviate.\n1D Kuramoto–Sivashinsky SNF-ROM maintains low relative error, including at larger time steps where baselines degrade.\n2D viscous Burgers The method achieves strong accuracy–speed tradeoffs with substantial state-space compression and high practical speed-up.\nSpeedup via hyper-reduction A major practical benefit of SNF-ROM is the ability to dramatically reduce the cost of evaluating the governing equations through hyper-reduction. In a full-order model, evaluating the PDE residual requires computing operators over every spatial degree of freedom. For large simulations, this can involve hundreds of thousands or millions of points.\nIn SNF-ROM, the state is represented in a low-dimensional latent space, and projection allows the governing dynamics to be evaluated in this reduced coordinate system. Hyper-reduction further accelerates computation by evaluating nonlinear operators at a carefully selected subset of spatial locations rather than the full grid, while still preserving accuracy in the projected dynamics.\nThis combination leads to substantial speedups. In the reported experiment, a problem with roughly 500,000 spatial degrees of freedom is reduced to a latent system with just two variables, and the resulting reduced model achieves approximately 199× speedup compared to evaluating the full-order model. The key insight is that once the dominant dynamics are captured in a low-dimensional manifold, the cost of simulation is governed by the reduced dimension rather than the original spatial resolution, enabling large performance gains while maintaining accuracy.\nCore contributions SNF-ROM is best understood as a ROM framework that explicitly supports doing physics on a learned nonlinear manifold:\nConstrained manifold formulation that encourages smoothly varying reduced trajectories, improving online stability and allowing larger time steps in practice. Smooth neural field regularization that makes the representation reliably differentiable, enabling accurate spatial derivatives via automatic differentiation rather than finite differences. Nonintrusive, grid-free dynamics evaluation working directly with point-cloud data, with no auxiliary mesh required. Projection + classical integration: dynamics evaluated in a physics-consistent way and integrated with standard time integrators, not black-box latent predictors. ROM quality depends not just on compression, but on controllable and stable online dynamics. If your ROM uses the PDE online, derivative fidelity is not optional — smoothness of the representation matters. SNF-ROM shows that regularizing neural manifolds for differentiation and integration can materially improve robustness over standard autoencoder-based pipelines.\nReferences Puri, V. et al. SNF-ROM: Projection-based nonlinear reduced order modeling with smooth neural fields. arXiv (2024). https://arxiv.org/abs/2405.14890 Puri, V. et al. SNF-ROM: Projection-based nonlinear reduced order modeling with smooth neural fields. Journal of Computational Physics (2025). DOI: 10.1016/j.jcp.2025.113957 Lee, K., and Carlberg, K. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics (2020). https://doi.org/10.1016/j.jcp.2019.108973 Lumley, J. L. The structure of inhomogeneous turbulent flows. Atmospheric Turbulence and Radio Wave Propagation (1967). https://ndlsearch.ndl.go.jp/en/books/R100000136-I1570572700589710080 Holmes, P., Lumley, J. L., Berkooz, G., and Rowley, C. W. Turbulence, Coherent Structures, Dynamical Systems and Symmetry (2nd ed.). Cambridge University Press (2012). https://doi.org/10.1017/CBO9780511919701 ","permalink":"https://vpuri3.github.io/blog/from-pod-to-neural-manifolds-a-modern-take-on-reduced-order-modeling/","summary":"\u003ch2 id=\"from-pod-to-neural-manifolds\"\u003eFrom POD to neural manifolds\u003c/h2\u003e\n\u003cp\u003eReduced-order modeling (ROM) is ultimately about preserving dominant system behavior with far fewer degrees of freedom. Classical projection methods, convolutional autoencoder ROMs, and modern neural-field formulations all target this same objective, but they make different assumptions about (i) how we represent the state and (ii) how we evolve it in time.\u003c/p\u003e\n\u003cp\u003eThis post walks through that progression and focuses on \u003cstrong\u003eSmooth Neural Field ROM (SNF-ROM)\u003c/strong\u003e: a projection-based nonlinear ROM that uses \u003cstrong\u003econtinuous neural fields\u003c/strong\u003e as the state representation and explicitly supports \u003cstrong\u003ephysics-based differentiation and time integration\u003c/strong\u003e during deployment.\u003c/p\u003e","title":"From POD to neural manifolds: a modern take on reduced order modeling"},{"content":"Why math is interesting The foundation of my interest in math was laid in my freshman year when I was appointed course staff to a sophomore engineering course. As I taught students to model mechanical interactions using forces and moments, I learned not only to communicate my ideas to newcomers in engineering but also how to challenge my own preconceived notions. Explaining why a certain unseen force has to exist to maintain equilibrium is like talking in a different language, in the sense that certain things are obvious (evident enough not to require proving) to me but not to a student. I have to explain that concentrated point forces, reaction moments and all these unseen entities are fictitious objects made by engineers and scientists to model and approximate reality.\nI could relate to students as I was in the same situation mere months ago, asking the same questions in disbelief. However, as I rolled concepts around in my head and thought things over I realized that however obscure, these seemingly unreal objects actually exist and their approximations are appropriate. In fact, their presence makes perfect physical sense. This epiphany, a shining moment of clarity is a reward in itself - one that makes you not want to stop thinking. Such daunting thought exercises attract me further towards the theoretical nature of engineering. It is truly marvelous to me that this edifice of human knowledge is derived from, and can be parsed through with, logical deductions based on a combination of observations and axiomatic thinking.\nIn teaching foundational topics, including proving the validity of the variational principles of mechanics, I articulated my own learning style. While studying discretization techniques in numerics courses, I became interested in the underlying variational formulation and the analytical treatment of PDEs. I studied Sobolev spaces, and wrote my own set of reference notes on the topic. I enjoyed the analytical rigor and saw a second degree in mathematics as means to place my engineering education in context within applied mathematics.\nWriting Proofs Part of writing proofs in mathematics is convincing yourself of the reality of the statement, say P. That given the nature of reality, P is the only way things can go, and no other way is feasible.\nA way to convince yourself is to \u0026ldquo;stare at it until you believe it is true.\u0026rdquo; The methods sounds passive but in reality is fairly involved. While staring, I run thought experiments in my head imagining all sorts of test functions, sets, and what not \u0026ndash; and wonder how they interact with with the statement P. Over time the visualizations of P in your head become intricate and you become more comfortable with the idea. The aim is to get an exhaustive idea of the nature of P and be able to say with confidence \u0026ldquo;how could it be anything but P!\u0026rdquo;\nNow that the act of self delusion is over, the next (harder) step is to attack your supposition from every angle to see how much of it is true.\nDeep learning, composition, gradient flows In my work at Carnegie Mellon University, we have been able to recover finite element function spaces using hand-crafted neural network architectures, implying that the space of functions that can be represented by neural networks encompasses the space of finite elements. The integration of gradient flows through differentiable programming therefore allow us to attack high dimensional problems nested within engineering workflows such as automated meshing, a significant bottleneck in engineering workflows.\nDeep learning derives its powers from the composition of nonlienar operations, a setup in which model parameters can be tuned via gradient descent. It\u0026rsquo;s an interesting problem: integrating gradient flows in state of the art computational technology integrations of gradient flows (good for high dimensional problems) with state of the art simulation technology (good for 2D/3D).\nNeed to understand (Barron space ideas) what function space outputs of neural networks lie in, and what are appropriate norms for analysis in these spaces. How to manipulate them to get a desired output? how to effectively do regression?\nOur job as computational engineers Computational workflows form a tall stack of abstractions that map physical quantities like velocity to distributed data structures acted upon by numerical operators. Scientific computing consolidates my interest in physics and the intricate mathematical framework that attempts to explain it. As such, developing mathematical models to accurately capture physics is a creative problem I like spending time on.\nPresent challenges in predictive modelling vary with the level of analysis applied from robust meshing of complex geometries to reliable turbulence models. With the aim of optimizing Computer Aided Engineering workflows, my focus is on developing application-specific tools for problems that advance humanity\u0026rsquo;s technological prowess.\nDifferentiable programming offer a superior way of integrating data with theory. Our job is to develop the tools that further our understanding the most. Aim is to develop problem specific architectures for PDE solving in complex geometries. The task is to develop a hybrid (sort) system where we can utilizes gradient flows (DL) and high order function approximation (FEM type) in an efficient way.\nMisc (do not read) This seems to be the nature of the universe: no matter how far you zoom in, you can still zoom in more. we\u0026rsquo;re just limited by our abilities not but a lack of detail to discover. no matter how far you zoom out, you can zoom out more. there\u0026rsquo;s always more stuff to look at. let\u0026rsquo;s call the un-computable transcendental. now by virtue of its infiniteness and by virtue of the the finiteness of human abilities of logic and to reason, the relationship one has with the nature of the universe has to be private because nobody can articulate it completely and explain it to themselves or to others. it\u0026rsquo;s like an infinitely layered story. something you can go on and on about without getting to the bottom of it. like a great book. or a great story. it\u0026rsquo;s by definition that the transcendent cannot be reduced a single or any number of thoughts or ideas or things without it automatically losing its essence. Ok I should get back to work.\nTangent on tet meshes Tetrehedral meshes are everywhere. Geometry is designed with splines and immidiately translated and shared with tet meshes. Splines rule CAD because knots give a designer granular control over the shape of their part.\nI\u0026rsquo;m very curious about implicit geometry representations, particularly high order spectral representations (neural too because that\u0026rsquo;s just a different basis to same difference) because it provides a bigger bang for your buck: you can accurately represent a more complicated function with the same number of points with chebychev distribution rather than uniform distribution (Runge phenomena). The issue with spectral geometry processing might be with patching, ie how to make different spectral pieces talk to each other. In Spectral Element Method, C0 continuity is handled via a gather-scatter operation, meaning that the adjacency matrix only needs to account for the boundary nodes.\nIt would be interesting to learn how splines preserve locality, and how they impose cotinuity conditions across \u0026ldquo;elements\u0026rdquo;.\nReferences Brenner, S. C., and Scott, L. R. The Mathematical Theory of Finite Element Methods (3rd ed.). Springer (2008). https://doi.org/10.1007/978-0-387-75934-0 Evans, L. C. Partial Differential Equations (2nd ed.). AMS (2010). https://bookstore.ams.org/gsm-19-r Adams, R. A., and Fournier, J. J. F. Sobolev Spaces (2nd ed.). Academic Press (2003). https://shop.elsevier.com/books/sobolev-spaces/adams/978-0-12-044143-3 Quarteroni, A., and Rozza, G. (eds.). Reduced Order Methods for Modeling and Computational Reduction. Springer (2014). https://doi.org/10.1007/978-3-319-02090-7 Hughes, T. J. R., Cottrell, J. A., and Bazilevs, Y. Isogeometric Analysis: CAD, Finite Elements, NURBS, Exact Geometry and Mesh Refinement. CMAME (2005). https://doi.org/10.1016/j.cma.2004.10.008 ","permalink":"https://vpuri3.github.io/blog/thoughts-archive/","summary":"\u003ch2 id=\"why-math-is-interesting\"\u003eWhy math is interesting\u003c/h2\u003e\n\u003cp\u003eThe foundation of my interest in math was laid in my freshman year when I was appointed course staff to a sophomore engineering course. As I taught students to model mechanical interactions using forces and moments, I learned not only to communicate my ideas to newcomers in engineering but also how to challenge my own preconceived notions. Explaining why a certain unseen force has to exist to maintain equilibrium is like talking in a different language, in the sense that certain things are obvious (evident enough not to require proving) to me but not to a student. I have to explain that concentrated point forces, reaction moments and all these unseen entities are fictitious objects made by engineers and scientists to model and approximate reality.\u003c/p\u003e","title":"Our job as computational engineers"},{"content":" Not So Up-to-Date Photography Portfolio For the past decade, I have used a Canon DSLR as an excuse to walk around and photograph people, geometry, and city texture.\nThis is a small archive from my Flickr stream.\nView full Flickr portfolio\nMisty Mornings Worlds apart Wizened Trials and Tribulations Simplicity, in geometry Perspective Leave the World Behind Ingenuity In Anticipation Protecting the protector Tiny elephants Illuminate Clutching Broken things The Kumbh Withering intricacies ","permalink":"https://vpuri3.github.io/photography/","summary":"\u003cdiv class=\"section-card\" id=\"photography-portfolio\"\u003e\n\u003ch2 id=\"not-so-up-to-date-photography-portfolio\"\u003eNot So Up-to-Date Photography Portfolio\u003c/h2\u003e\n\u003cp\u003eFor the past decade, I have used a Canon DSLR as an excuse to walk around and photograph people, geometry, and city texture.\u003cbr\u003e\nThis is a small archive from my Flickr stream.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.flickr.com/photos/128280868@N05/\"\u003eView full Flickr portfolio\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"photo-grid\"\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/24464384943/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-01.jpg\" alt=\"Misty Mornings\"\u003e\u003cspan\u003eMisty Mornings\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15671787505/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-02.jpg\" alt=\"Worlds apart\"\u003e\u003cspan\u003eWorlds apart\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15051818524/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-03.jpg\" alt=\"Wizened\"\u003e\u003cspan\u003eWizened\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486392498/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-04.jpg\" alt=\"Trials and Tribulations\"\u003e\u003cspan\u003eTrials and Tribulations\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15671791705/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-05.jpg\" alt=\"Simplicity, in geometry\"\u003e\u003cspan\u003eSimplicity, in geometry\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15485909309/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-06.jpg\" alt=\"Perspective\"\u003e\u003cspan\u003ePerspective\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15673379832/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-07.jpg\" alt=\"Leave the World Behind\"\u003e\u003cspan\u003eLeave the World Behind\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486584357/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-08.jpg\" alt=\"Ingenuity\"\u003e\u003cspan\u003eIngenuity\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15485913259/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-09.jpg\" alt=\"In Anticipation\"\u003e\u003cspan\u003eIn Anticipation\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15671865155/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-10.jpg\" alt=\"Protecting the protector\"\u003e\u003cspan\u003eProtecting the protector\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486993370/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-11.jpg\" alt=\"Tiny elephants\"\u003e\u003cspan\u003eTiny elephants\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486407268/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-12.jpg\" alt=\"Illuminate\"\u003e\u003cspan\u003eIlluminate\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15648074336/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-13.jpg\" alt=\"Clutching\"\u003e\u003cspan\u003eClutching\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486411598/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-14.jpg\" alt=\"Broken things\"\u003e\u003cspan\u003eBroken things\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15669895691/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-15.jpg\" alt=\"The Kumbh\"\u003e\u003cspan\u003eThe Kumbh\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486419388/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-16.jpg\" alt=\"Withering intricacies\"\u003e\u003cspan\u003eWithering intricacies\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e","title":"Not So Up-to-Date Photography Portfolio"},{"content":" Work summary This page is an extended curriculum viatte with unnecessary commentary. Check out my single page CV if you\u0026rsquo;re feeling lazy.\nBS @ University of Illinois Urbana-Champaign I attended University of Illinois Urbana-Champaign from 2015 to 2019, earning Bachelor\u0026rsquo;s degrees in Mathematics, Engineering Mechanics, and a minor in Computational Science and Engineering. While at university, I became interested in the numerical treatment of partial differential equations for physical modeling. To that effect, I made an attempt to comprehend the entire knowledge stack of computer simulations: I studied Hamiltonian mechanics to learn how physics is modeled; Sobolev spaces to learn the mathematical treatment of differential operators; and numerical algorithms for converting continuous equationsinto solvable problems.\nTo complement my classwork, I wrote spectral element codes to solve fluid-flows in interesting geometries, and interned first at the National Center for Supercomputing Applications on numerical PDEs, and then at the Argonne National Laboratory on wall-bounded turbulent flows. The work from ANL resulted in my senior thesis. I also have substantial on-campus engagement through Society for Engineering Mechanics, a student organization that I led in my senior year.\nSociety for Engineering Mechanics We engineerined mechanics. Add link to projects like chocolate 3D printer, SIIP work.\nIntroductory Statics Course Assistant At UIUC, I discovered an interest in teaching when I was selected in freshman year to be a course assistant for Introductory Statics, a mechanics course where students learn to model mechanical interactions as forces and moments. The instructors for Statics invited me to attend the Strategic Instructional Innovations Program meetings to assist with curriculum design for mechanics courses serving $2500$ students annually. I created engineering-design oriented activities for students and coordinated with the Society for Engineering Mechanics, a student organization that I led in my senior year, to manufacture instructional demonstrations for introductory courses in Statics, Dynamics, and Solid Mechanics.\nIllini Hyperloop As our capstone project at UIUC, my group implemented a passive cooling solution capable of absorbing up to 300 kJ of heat. The solution would be used for the propulsion system of the Hyperloop pod of Illini Hyperloop team participating in SpaceX Hyperloop Competition 2019. Fabrication was handled by the project sponsor, Novark Technologies, Inc. The project video, report, and summary can be found here.\nNational Center for Supercomputing Applications, 2017-18 My work at NCSA on initial data generation for gravitational wave simulations resulted in a talk that I delivered at the American Physical Society April Meeting 2018. Under Dr. Roland Haas of the Gravity Group, I solved nonlinear elliptic initial data equations for gravitational wave simulations in the cosmological framework Einstein Toolkit. I implemented the novel Scheduled Relaxation Jacobi technique which relies on precomputing a set of relaxation factors by nonlinear optimization for Laplacian-dominated PDEs. The relaxation solve was embedded within a Newton-Raphson loop to alleviate the stiffness due to nonlinearity, reducing the time-to-solution by several orders of magnitude.\nArgonne Natinoal Laboratory, 2018 In the summer of 2018, I studied wall-bounded turbulence flows at Argonne Natinoal Lab Dr. Ramesh Balakrishnan, and Dr. Aleksandr Obabko of the Mathematics and Computer Science divisio. The project was part of a larger United States Department of Energy effort on simulating airflow in open-ocean windfarms. I examined the physics of flows over curved geometries by conducting Direct Numerical Simulations.\nTo facilitate the development of wall-models for Large Eddy Simulations, I computed the terms of the tensor Reynolds Stress Transport Equation within the spectral element fluid dynamics code NEK5000. The converged Reynolds Stress budgets (third order statistics) proper resolution of the high-frequency modes in my calculation, and reveal the spatial distribution and relative magnitudes of the production, dissipation, and transport mechanisms of turbulent energy. My FORTRAN 77 routines for turbulence statistics computation and post-processing in NEK5000 can be found in this git repo.\nAs the scope of the project was too broad for one summer, I continued the work as thesis research at University of Illinois.\nArgonne Natinoal Laboratory, 2020 I returned to Argonne after graduation to continue working on turbulence modeling problems, with a focus on complex geometry, and hierarchical simulations where the output of a highly resolved but simplified calculation would be used in modelling unresolved physics in a larger more complicated simulation. I conducted Large Eddy Simulations (LES) and Reynolds Averaged Navier-Stokes (RANS) calculations of airflow near building-like geometries for the DOE’s Distributed Windproject. The overarching goal of the project is to obtain high-resolution LES and DNS data to support development of subgrid-stress models and wall-models for Detached Eddy Simulations and Reynolds Averaged Navier-Stokes simulations.\nCarnegie Mellon University, 2020 In July 2020, I connected with Professor Venkat Viswanathan at Carnegie Mellon University, who expressed interest in developing differentiable programming models for computational fluid dynamics problems. With his group, I tackled problems in scientific computing in a very broad sense, while pushing out packages in the Julia ecosystem. In our work, we have been able to recover finite element function spaces using hand-crafted neural network architectures, implying that the space of functions that can be represented by neural networks encompasses the space of finite elements.\nAs such, the Viswanathan research group\u0026rsquo;s emphasis on pushing out open source packages and ties to commercial markets would constrain and guide my research endeavors in a fruitful direction.\nStarted work on fully differentiable spectral element solver implemented in Julia.\nJulia Computing, 2021 I am working on deploying Physics Informed Neural Networks for Julia Computing\u0026rsquo;s commercial products under Dr. Chris Rackauckas. I also wrote the linear solver interface for DifferentialEquations.jl ecosystem.\n","permalink":"https://vpuri3.github.io/work/","summary":"\u003cdiv class=\"section-card\" \u003e\n\u003ch1 id=\"work-summary\"\u003eWork summary\u003c/h1\u003e\n\u003cp\u003eThis page is an extended curriculum viatte with unnecessary commentary. Check out my single page \u003ca href=\"https://github.com/vpuri3/vpCV/raw/master/vpCV.pdf\"\u003eCV\u003c/a\u003e if you\u0026rsquo;re feeling lazy.\u003c/p\u003e\n\u003cdiv class=\"project-card\"\u003e\n\u003ch2 id=\"bs--university-of-illinois-urbana-champaign\"\u003eBS @ University of Illinois Urbana-Champaign\u003c/h2\u003e\n\u003cp\u003eI attended University of Illinois Urbana-Champaign from 2015 to 2019, earning Bachelor\u0026rsquo;s degrees in \u003cem\u003eMathematics\u003c/em\u003e, \u003cem\u003eEngineering Mechanics\u003c/em\u003e, and a minor in \u003cem\u003eComputational Science and Engineering\u003c/em\u003e. While at university, I became interested in the numerical treatment of partial differential equations for physical modeling. To that effect, I made an attempt to comprehend the entire knowledge stack of computer simulations: I studied Hamiltonian mechanics to learn how physics is modeled; Sobolev spaces to learn the mathematical treatment of differential operators; and numerical algorithms for converting continuous equationsinto solvable problems.\u003c/p\u003e","title":"Work"}]