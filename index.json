[{"content":"The story of FLARE: Fast Low-rank Attention Routing Engine Attention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.\nRather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?\nThis post complements our latest paper (arXiv:2508.12594).\nWhy this problem is hard Attention is now used across language, vision, and scientific machine learning. The scaling bottleneck appears everywhere. For a sequence of $N$ tokens, standard self-attention requires $O(N^2)$ compute and memory, which quickly becomes impractical beyond tens of thousands of tokens.\nWe are particularly motivated by partial differential equation (PDE) surrogate modeling, one of the most demanding settings for attention:\nInputs are large and geometry dependent. Meshes are often unstructured, so ordering tricks are unreliable. Accurate predictions require long-range interactions across the domain. Linear attention methods reduce cost but often struggle to match accuracy in these regimes. The challenge is not just scaling attention, but scaling it without losing expressive global communication.\nPDE surrogate modeling For a PDE\n$$ \\mathcal{L}(\\boldsymbol{x}, t, \\boldsymbol{u}; \\boldsymbol{\\mu}), \\quad \\boldsymbol{x}\\in\\Omega, $$ we view learning as approximating a solution operator \\(\\mathcal{G}\\) that maps parameters to fields: $$ \\boldsymbol{u}(\\boldsymbol{x})=\\mathcal{G}(\\boldsymbol{\\mu})(\\boldsymbol{x}). $$ In practice, we train on many \\(\\boldsymbol{\\mu}\\to \\boldsymbol{u}\\) pairs and require generalization to unseen \\(\\boldsymbol{\\mu}\\). In the slide below, \\(\\boldsymbol{\\mu}\\) is effectively the geometry \\(\\Omega\\), and the target is a strain field on unseen domains. Token mixing in transformers A transformer block is conceptually two operations:\npointwise nonlinear transforms (MLPs), global message passing (self-attention). For PDE data, we avoid token clustering heuristics and treat each mesh point as a token. Standard attention computes, for each output token, a distribution over all inputs:\n$$ y_n = \\sum_m \\frac{\\exp\\left(q_n^T \\cdot k_l \\right)}{\\sum_l \\exp \\left(q_n^\\top \\cdot k_l \\right)} v_l $$\nThis all-to-all routing is powerful but expensive. At million-token scale, even a single forward-backward pass can take tens of seconds. The goal is to keep global communication while reducing the number of messages that must be exchanged.\nWhy global communication still matters In many physical systems, forward operators are local, but solution operators are effectively dense. A model that communicates only locally will miss long-range dependencies that determine the final field.\nHowever, physical signals are often smooth at resolved scales. Smoothness implies redundancy: nearby tokens frequently carry similar information for distant targets. This suggests we may not need to send separate messages from every token if we can aggregate similar ones.\nThis observation motivates a low-rank communication view of attention.\nAre $N^2$ messages really necessary? Physical fields are usually smooth at resolved scales. Smoothness means many high-frequency modes are weak; in a fully resolved signal, there is redundancy. So if two nearby source points carry nearly the same information for a far target point, we should be able to route one combined message instead of two separate ones.\nThe core FLARE contribution is to operationalize this physics-guided redundancy into a sequence-agnostic attention mechanism that remains general-purpose.\nFLARE How we got to FLARE The starting point was a simple systems question: what actually makes attention expensive?\nAttention can be interpreted as a routing mechanism where each token decides how to combine information from all others. The quadratic cost comes from explicitly computing all pairwise interactions, even when many of those interactions are redundant.\nFrom a physics perspective, many solution operators exhibit low effective dimensionality. Spectral decay in physical fields suggests that long-range interactions can often be captured through a smaller set of communication pathways.\nThis led to the idea of introducing a small set of latent routing tokens that act as intermediaries. Instead of every token talking to every other token directly, tokens communicate through these shared hubs. The challenge was designing this mechanism so that it remains global, interpretable, and easy to optimize.\nFLARE Method FLARE implements global communication through a two-step gather–scatter mechanism, but it does so using standard, highly optimized attention primitives.\nScaled dot-product attention (SDPA) The basic attention primitive is scaled dot-product attention. Given queries $Q \\in \\mathbb{R}^{N_q \\times d}$, keys $K \\in \\mathbb{R}^{N_k \\times d}$, and values $V \\in \\mathbb{R}^{N_k \\times d_v}$, attention is\n$$ \\mathrm{SPDA}(Q,K,V) = \\mathrm{softmax}!\\left(\\frac{Q \\cdot K^\\top}{\\sqrt{d}}\\right)V. $$\nNaively, this suggests computing the score matrix $S = Q \\cdot K^\\top \\in \\mathbb{R}^{N_q \\times N_k}$, applying softmax, and then multiplying by $V$. That approach is expensive because it materializes an $N_q \\times N_k$ matrix.\nModern frameworks provide SDPA as a fused kernel that computes the same result without materializing the full attention-weight matrix in memory. In PyTorch, torch.nn.functional.scaled_dot_product_attention is the standard entry point, and under the hood it can dispatch to fused implementations (for example FlashAttention-style kernels) that stream over blocks of $K,V$ and maintain the softmax normalization online. Practically, SDPA lets you use the standard attention math while keeping memory use close to $O((L_q+L_k)d)$ rather than $O(L_qL_k)$.\nFLARE is built to use SDPA twice: once to gather information into a small latent set, and once to scatter it back.\nStep 1: Gather (encode) Introduce $M \\ll N$ latent tokens per head. Let $Q_h \\in \\mathbb{R}^{M \\times d}$ be latent queries, and let $K_h, V_h \\in \\mathbb{R}^{N \\times d}$ be token keys/values of head $h$. The gather step pools from the $N$ tokens into $M$ latents:\n$$ Z_h = \\mathrm{SDPA}(Q_h, K_h, V_h). $$\nIn matrix notation this is\n$$ W_{\\mathrm{enc}} = \\mathrm{softmax}(Q_h \\cdot K_h^\\top) \\in \\mathbb{R}^{M \\times N}, \\qquad Z_h = W_{\\mathrm{enc}} \\cdot V_h. $$\nImportantly, although $W_{\\mathrm{enc}}$ is a useful conceptual object, SDPA computes $Z_h$ without explicitly materializing $W_{\\mathrm{enc}}$.\nStep 2: Scatter (decode) Next, the latents broadcast information back to all $N$ tokens using the reverse affinity pattern:\n$$ Y_h = \\mathrm{SDPA}(K_h, Q_h, Z_h). $$\nIn matrix form,\n$$ W_{\\mathrm{dec}} = \\mathrm{softmax}(K_h \\cdot Q_h^\\top) \\in \\mathbb{R}^{N \\times M}, \\qquad Y_h = W_{\\mathrm{dec}} \\cdot Z_h. $$\nCombining both steps gives an implicit global communication operator:\n$$ Y_h = (W_{\\mathrm{dec}} \\cdot W_{\\mathrm{enc}}) \\cdot V_h. $$\nThe product $W_{\\mathrm{dec}} \\cdot W_{\\mathrm{enc}} \\in \\mathbb{R}^{N \\times N}$ is a dense global mixing matrix, but its rank is at most $M$. FLARE therefore achieves global communication at $O(NM)$ cost per head (with $M \\ll N$), and the SDPA kernels avoid ever forming the intermediate $N \\times M$ or $M \\times N$ attention matrices in memory.\nMinimal implementation Below is the core mixer expressed directly with PyTorch SDPA. This is essentially the gather–scatter operator in a few lines.\nfrom torch.nn.functional import scaled_dot_product_attention as SDPA def flare_multihead_mixer(Q, K, V): \u0026#34;\u0026#34;\u0026#34; Args: Q: [H, M, D] latent queries (per head) K: [B, H, N, D] token keys V: [B, H, N, D] token values Returns: Y: [B, H, N, D] mixed token outputs \u0026#34;\u0026#34;\u0026#34; Qb = Q.unsqueeze(0) # [1, H, M, D] to match SDPA batching # Gather: tokens -\u0026gt; latents Z = SDPA(Qb, K, V, scale=1.0) # [B, H, M, D] # Scatter: latents -\u0026gt; tokens Y = SDPA(K, Qb, Z, scale=1.0) # [B, H, N, D] return Y The full FLARE block stacks this mixer with standard projection/MLP components: And here are the scaling results: Results FLARE enables million-token training on a single GPU while maintaining strong accuracy on PDE surrogate tasks. The key improvement is not only asymptotic scaling but a practical runtime and memory profile that allows end-to-end training.\nOn sequence benchmarks such as Long Range Arena, the operator remains competitive with other efficient transformer approaches, showing that the mechanism is broadly applicable beyond scientific data.\nOn sequence benchmarks, such as Long Range Arena, FLARE is competitive with efficient-transformer alternatives and maintains strong average accuracy, showing that the method is not restricted to PDE-only structure.\nDeep dive: why this design works Understanding why FLARE works requires looking at the structure of the gather–scatter operator and how different design choices affect learning dynamics.\nGather–scatter as communication hubs Each latent token acts as both a pooling hub and a routing hub. During encoding, it aggregates information from tokens that align with its query pattern. During decoding, it distributes that information back to tokens that match its key pattern.\nThis creates a contraction–expansion communication structure that efficiently mixes global information.\nSymmetric operators improve stability The encode and decode steps are derived from the same query-key geometry, creating a structurally aligned operator. This symmetry reduces redundant parameterization and leads to more stable optimization compared to asymmetric variants.\nFixed queries provide a stable routing structure Latent queries are learned but input independent. This gives a consistent communication basis across inputs, making the operator easier to optimize and interpret.\nThe reduced query dynamism is compensated by expressive key and value encoders, which adapt routing patterns to each input.\nRepeated global mixing is more effective than latent refinement Empirically, allocating compute to repeated gather–scatter operations produces better performance than stacking heavy latent self-attention blocks. The benefit comes from repeatedly mixing global information rather than refining latent representations in isolation.\nIndependent latents across heads increase diversity Allowing each head to have its own latent tokens produces more diverse communication pathways. Different heads learn complementary routing structures, improving accuracy and depth scaling.\nPractical takeaways Several design principles emerge:\nUse explicit low-rank communication rather than approximating local attention. Allocate compute to repeated global mixing. Keep routing pathways independent across heads. Use expressive key and value encoders when queries are fixed. Closing thoughts FLARE shows that scaling attention is not only about approximating softmax more efficiently. By rethinking attention as a communication operator, we can design mechanisms that preserve global information flow while dramatically reducing cost.\nThe broader lesson is that many large-scale problems have structure that can be exploited. Low-rank communication is one way to capture that structure without sacrificing flexibility, opening the door to practical million-token models on modest hardware.\n","permalink":"https://vpuri3.github.io/blog/scaling-attention-to-1m-tokens-on-a-single-gpu/","summary":"\u003ch2 id=\"the-story-of-flare-fast-low-rank-attention-routing-engine\"\u003eThe story of FLARE: Fast Low-rank Attention Routing Engine\u003c/h2\u003e\n\u003cp\u003eAttention, the core mechanism of transformers, becomes prohibitively expensive at large sequence lengths. This post explains the ideas behind FLARE, an attention operator designed to retain global communication while scaling to million-token regimes on a single GPU.\u003c/p\u003e\n\u003cp\u003eRather than focusing on architectural novelty for its own sake, the goal is to understand attention as a communication operator and ask a simple question: can we keep the benefits of global message passing without paying the full quadratic cost?\u003c/p\u003e","title":"Scaling attention to 1M tokens on a single GPU"},{"content":"Why math is interesting The foundation of my interest in math was laid in my freshman year when I was appointed course staff to a sophomore engineering course. As I taught students to model mechanical interactions using forces and moments, I learned not only to communicate my ideas to newcomers in engineering but also how to challenge my own preconceived notions. Explaining why a certain unseen force has to exist to maintain equilibrium is like talking in a different language, in the sense that certain things are obvious (evident enough not to require proving) to me but not to a student. I have to explain that concentrated point forces, reaction moments and all these unseen entities are fictitious objects made by engineers and scientists to model and approximate reality.\nI could relate to students as I was in the same situation mere months ago, asking the same questions in disbelief. However, as I rolled concepts around in my head and thought things over I realized that however obscure, these seemingly unreal objects actually exist and their approximations are appropriate. In fact, their presence makes perfect physical sense. This epiphany, a shining moment of clarity is a reward in itself - one that makes you not want to stop thinking. Such daunting thought exercises attract me further towards the theoretical nature of engineering. It is truly marvelous to me that this edifice of human knowledge is derived from, and can be parsed through with, logical deductions based on a combination of observations and axiomatic thinking.\nIn teaching foundational topics, including proving the validity of the variational principles of mechanics, I articulated my own learning style. While studying discretization techniques in numerics courses, I became interested in the underlying variational formulation and the analytical treatment of PDEs. I studied Sobolev spaces, and wrote my own set of reference notes on the topic. I enjoyed the analytical rigor and saw a second degree in mathematics as means to place my engineering education in context within applied mathematics.\nWriting Proofs Part of writing proofs in mathematics is convincing yourself of the reality of the statement, say P. That given the nature of reality, P is the only way things can go, and no other way is feasible.\nA way to convince yourself is to \u0026ldquo;stare at it until you believe it is true.\u0026rdquo; The methods sounds passive but in reality is fairly involved. While staring, I run thought experiments in my head imagining all sorts of test functions, sets, and what not \u0026ndash; and wonder how they interact with with the statement P. Over time the visualizations of P in your head become intricate and you become more comfortable with the idea. The aim is to get an exhaustive idea of the nature of P and be able to say with confidence \u0026ldquo;how could it be anything but P!\u0026rdquo;\nNow that the act of self delusion is over, the next (harder) step is to attack your supposition from every angle to see how much of it is true.\nDeep learning, composition, gradient flows In my work at Carnegie Mellon University, we have been able to recover finite element function spaces using hand-crafted neural network architectures, implying that the space of functions that can be represented by neural networks encompasses the space of finite elements. The integration of gradient flows through differentiable programming therefore allow us to attack high dimensional problems nested within engineering workflows such as automated meshing, a significant bottleneck in engineering workflows.\nDeep learning derives its powers from the composition of nonlienar operations, a setup in which model parameters can be tuned via gradient descent. It\u0026rsquo;s an interesting problem: integrating gradient flows in state of the art computational technology integrations of gradient flows (good for high dimensional problems) with state of the art simulation technology (good for 2D/3D).\nNeed to understand (Barron space ideas) what function space outputs of neural networks lie in, and what are appropriate norms for analysis in these spaces. How to manipulate them to get a desired output? how to effectively do regression?\nOur job as computational engineers Computational workflows form a tall stack of abstractions that map physical quantities like velocity to distributed data structures acted upon by numerical operators. Scientific computing consolidates my interest in physics and the intricate mathematical framework that attempts to explain it. As such, developing mathematical models to accurately capture physics is a creative problem I like spending time on.\nPresent challenges in predictive modelling vary with the level of analysis applied from robust meshing of complex geometries to reliable turbulence models. With the aim of optimizing Computer Aided Engineering workflows, my focus is on developing application-specific tools for problems that advance humanity\u0026rsquo;s technological prowess.\nDifferentiable programming offer a superior way of integrating data with theory. Our job is to develop the tools that further our understanding the most. Aim is to develop problem specific architectures for PDE solving in complex geometries. The task is to develop a hybrid (sort) system where we can utilizes gradient flows (DL) and high order function approximation (FEM type) in an efficient way.\nMisc (do not read) This seems to be the nature of the universe: no matter how far you zoom in, you can still zoom in more. we\u0026rsquo;re just limited by our abilities not but a lack of detail to discover. no matter how far you zoom out, you can zoom out more. there\u0026rsquo;s always more stuff to look at. let\u0026rsquo;s call the un-computable transcendental. now by virtue of its infiniteness and by virtue of the the finiteness of human abilities of logic and to reason, the relationship one has with the nature of the universe has to be private because nobody can articulate it completely and explain it to themselves or to others. it\u0026rsquo;s like an infinitely layered story. something you can go on and on about without getting to the bottom of it. like a great book. or a great story. it\u0026rsquo;s by definition that the transcendent cannot be reduced a single or any number of thoughts or ideas or things without it automatically losing its essence. Ok I should get back to work.\nTangent on tet meshes Tetrehedral meshes are everywhere. Geometry is designed with splines and immidiately translated and shared with tet meshes. Splines rule CAD because knots give a designer granular control over the shape of their part.\nI\u0026rsquo;m very curious about implicit geometry representations, particularly high order spectral representations (neural too because that\u0026rsquo;s just a different basis to same difference) because it provides a bigger bang for your buck: you can accurately represent a more complicated function with the same number of points with chebychev distribution rather than uniform distribution (Runge phenomena). The issue with spectral geometry processing might be with patching, ie how to make different spectral pieces talk to each other. In Spectral Element Method, C0 continuity is handled via a gather-scatter operation, meaning that the adjacency matrix only needs to account for the boundary nodes.\nIt would be interesting to learn how splines preserve locality, and how they impose cotinuity conditions across \u0026ldquo;elements\u0026rdquo;.\n","permalink":"https://vpuri3.github.io/blog/thoughts-archive/","summary":"\u003ch2 id=\"why-math-is-interesting\"\u003eWhy math is interesting\u003c/h2\u003e\n\u003cp\u003eThe foundation of my interest in math was laid in my freshman year when I was appointed course staff to a sophomore engineering course. As I taught students to model mechanical interactions using forces and moments, I learned not only to communicate my ideas to newcomers in engineering but also how to challenge my own preconceived notions. Explaining why a certain unseen force has to exist to maintain equilibrium is like talking in a different language, in the sense that certain things are obvious (evident enough not to require proving) to me but not to a student. I have to explain that concentrated point forces, reaction moments and all these unseen entities are fictitious objects made by engineers and scientists to model and approximate reality.\u003c/p\u003e","title":"Our job as computational engineers"},{"content":" Not So Up-to-Date Photography Portfolio For the past decade, I have used a Canon DSLR as an excuse to walk around and photograph people, geometry, and city texture.\nThis is a small archive from my Flickr stream.\nView full Flickr portfolio\nMisty Mornings Worlds apart Wizened Trials and Tribulations Simplicity, in geometry Perspective Leave the World Behind Ingenuity In Anticipation Protecting the protector Tiny elephants Illuminate Clutching Broken things The Kumbh Withering intricacies ","permalink":"https://vpuri3.github.io/photography/","summary":"\u003cdiv class=\"section-card\" id=\"photography-portfolio\"\u003e\n\u003ch2 id=\"not-so-up-to-date-photography-portfolio\"\u003eNot So Up-to-Date Photography Portfolio\u003c/h2\u003e\n\u003cp\u003eFor the past decade, I have used a Canon DSLR as an excuse to walk around and photograph people, geometry, and city texture.\u003cbr\u003e\nThis is a small archive from my Flickr stream.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.flickr.com/photos/128280868@N05/\"\u003eView full Flickr portfolio\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"photo-grid\"\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/24464384943/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-01.jpg\" alt=\"Misty Mornings\"\u003e\u003cspan\u003eMisty Mornings\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15671787505/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-02.jpg\" alt=\"Worlds apart\"\u003e\u003cspan\u003eWorlds apart\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15051818524/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-03.jpg\" alt=\"Wizened\"\u003e\u003cspan\u003eWizened\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486392498/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-04.jpg\" alt=\"Trials and Tribulations\"\u003e\u003cspan\u003eTrials and Tribulations\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15671791705/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-05.jpg\" alt=\"Simplicity, in geometry\"\u003e\u003cspan\u003eSimplicity, in geometry\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15485909309/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-06.jpg\" alt=\"Perspective\"\u003e\u003cspan\u003ePerspective\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15673379832/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-07.jpg\" alt=\"Leave the World Behind\"\u003e\u003cspan\u003eLeave the World Behind\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486584357/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-08.jpg\" alt=\"Ingenuity\"\u003e\u003cspan\u003eIngenuity\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15485913259/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-09.jpg\" alt=\"In Anticipation\"\u003e\u003cspan\u003eIn Anticipation\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15671865155/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-10.jpg\" alt=\"Protecting the protector\"\u003e\u003cspan\u003eProtecting the protector\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486993370/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-11.jpg\" alt=\"Tiny elephants\"\u003e\u003cspan\u003eTiny elephants\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486407268/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-12.jpg\" alt=\"Illuminate\"\u003e\u003cspan\u003eIlluminate\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15648074336/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-13.jpg\" alt=\"Clutching\"\u003e\u003cspan\u003eClutching\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486411598/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-14.jpg\" alt=\"Broken things\"\u003e\u003cspan\u003eBroken things\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15669895691/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-15.jpg\" alt=\"The Kumbh\"\u003e\u003cspan\u003eThe Kumbh\u003c/span\u003e\u003c/a\u003e\n  \u003ca class=\"photo-tile\" href=\"https://www.flickr.com/photos/128280868@N05/15486419388/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/photography/photo-16.jpg\" alt=\"Withering intricacies\"\u003e\u003cspan\u003eWithering intricacies\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e","title":"Not So Up-to-Date Photography Portfolio"},{"content":" Work summary This page is an extended curriculum viatte with unnecessary commentary. Check out my single page CV if you\u0026rsquo;re feeling lazy.\nBS @ University of Illinois Urbana-Champaign I attended University of Illinois Urbana-Champaign from 2015 to 2019, earning Bachelor\u0026rsquo;s degrees in Mathematics, Engineering Mechanics, and a minor in Computational Science and Engineering. While at university, I became interested in the numerical treatment of partial differential equations for physical modeling. To that effect, I made an attempt to comprehend the entire knowledge stack of computer simulations: I studied Hamiltonian mechanics to learn how physics is modeled; Sobolev spaces to learn the mathematical treatment of differential operators; and numerical algorithms for converting continuous equationsinto solvable problems.\nTo complement my classwork, I wrote spectral element codes to solve fluid-flows in interesting geometries, and interned first at the National Center for Supercomputing Applications on numerical PDEs, and then at the Argonne National Laboratory on wall-bounded turbulent flows. The work from ANL resulted in my senior thesis. I also have substantial on-campus engagement through Society for Engineering Mechanics, a student organization that I led in my senior year.\nSociety for Engineering Mechanics We engineerined mechanics. Add link to projects like chocolate 3D printer, SIIP work.\nIntroductory Statics Course Assistant At UIUC, I discovered an interest in teaching when I was selected in freshman year to be a course assistant for Introductory Statics, a mechanics course where students learn to model mechanical interactions as forces and moments. The instructors for Statics invited me to attend the Strategic Instructional Innovations Program meetings to assist with curriculum design for mechanics courses serving $2500$ students annually. I created engineering-design oriented activities for students and coordinated with the Society for Engineering Mechanics, a student organization that I led in my senior year, to manufacture instructional demonstrations for introductory courses in Statics, Dynamics, and Solid Mechanics.\nIllini Hyperloop As our capstone project at UIUC, my group implemented a passive cooling solution capable of absorbing up to 300 kJ of heat. The solution would be used for the propulsion system of the Hyperloop pod of Illini Hyperloop team participating in SpaceX Hyperloop Competition 2019. Fabrication was handled by the project sponsor, Novark Technologies, Inc. The project video, report, and summary can be found here.\nNational Center for Supercomputing Applications, 2017-18 My work at NCSA on initial data generation for gravitational wave simulations resulted in a talk that I delivered at the American Physical Society April Meeting 2018. Under Dr. Roland Haas of the Gravity Group, I solved nonlinear elliptic initial data equations for gravitational wave simulations in the cosmological framework Einstein Toolkit. I implemented the novel Scheduled Relaxation Jacobi technique which relies on precomputing a set of relaxation factors by nonlinear optimization for Laplacian-dominated PDEs. The relaxation solve was embedded within a Newton-Raphson loop to alleviate the stiffness due to nonlinearity, reducing the time-to-solution by several orders of magnitude.\nArgonne Natinoal Laboratory, 2018 In the summer of 2018, I studied wall-bounded turbulence flows at Argonne Natinoal Lab Dr. Ramesh Balakrishnan, and Dr. Aleksandr Obabko of the Mathematics and Computer Science divisio. The project was part of a larger United States Department of Energy effort on simulating airflow in open-ocean windfarms. I examined the physics of flows over curved geometries by conducting Direct Numerical Simulations.\nTo facilitate the development of wall-models for Large Eddy Simulations, I computed the terms of the tensor Reynolds Stress Transport Equation within the spectral element fluid dynamics code NEK5000. The converged Reynolds Stress budgets (third order statistics) proper resolution of the high-frequency modes in my calculation, and reveal the spatial distribution and relative magnitudes of the production, dissipation, and transport mechanisms of turbulent energy. My FORTRAN 77 routines for turbulence statistics computation and post-processing in NEK5000 can be found in this git repo.\nAs the scope of the project was too broad for one summer, I continued the work as thesis research at University of Illinois.\nArgonne Natinoal Laboratory, 2020 I returned to Argonne after graduation to continue working on turbulence modeling problems, with a focus on complex geometry, and hierarchical simulations where the output of a highly resolved but simplified calculation would be used in modelling unresolved physics in a larger more complicated simulation. I conducted Large Eddy Simulations (LES) and Reynolds Averaged Navier-Stokes (RANS) calculations of airflow near building-like geometries for the DOE’s Distributed Windproject. The overarching goal of the project is to obtain high-resolution LES and DNS data to support development of subgrid-stress models and wall-models for Detached Eddy Simulations and Reynolds Averaged Navier-Stokes simulations.\nCarnegie Mellon University, 2020 In July 2020, I connected with Professor Venkat Viswanathan at Carnegie Mellon University, who expressed interest in developing differentiable programming models for computational fluid dynamics problems. With his group, I tackled problems in scientific computing in a very broad sense, while pushing out packages in the Julia ecosystem. In our work, we have been able to recover finite element function spaces using hand-crafted neural network architectures, implying that the space of functions that can be represented by neural networks encompasses the space of finite elements.\nAs such, the Viswanathan research group\u0026rsquo;s emphasis on pushing out open source packages and ties to commercial markets would constrain and guide my research endeavors in a fruitful direction.\nStarted work on fully differentiable spectral element solver implemented in Julia.\nJulia Computing, 2021 I am working on deploying Physics Informed Neural Networks for Julia Computing\u0026rsquo;s commercial products under Dr. Chris Rackauckas. I also wrote the linear solver interface for DifferentialEquations.jl ecosystem.\n","permalink":"https://vpuri3.github.io/work/","summary":"\u003cdiv class=\"section-card\" \u003e\n\u003ch1 id=\"work-summary\"\u003eWork summary\u003c/h1\u003e\n\u003cp\u003eThis page is an extended curriculum viatte with unnecessary commentary. Check out my single page \u003ca href=\"https://github.com/vpuri3/vpCV/raw/master/vpCV.pdf\"\u003eCV\u003c/a\u003e if you\u0026rsquo;re feeling lazy.\u003c/p\u003e\n\u003cdiv class=\"project-card\"\u003e\n\u003ch2 id=\"bs--university-of-illinois-urbana-champaign\"\u003eBS @ University of Illinois Urbana-Champaign\u003c/h2\u003e\n\u003cp\u003eI attended University of Illinois Urbana-Champaign from 2015 to 2019, earning Bachelor\u0026rsquo;s degrees in \u003cem\u003eMathematics\u003c/em\u003e, \u003cem\u003eEngineering Mechanics\u003c/em\u003e, and a minor in \u003cem\u003eComputational Science and Engineering\u003c/em\u003e. While at university, I became interested in the numerical treatment of partial differential equations for physical modeling. To that effect, I made an attempt to comprehend the entire knowledge stack of computer simulations: I studied Hamiltonian mechanics to learn how physics is modeled; Sobolev spaces to learn the mathematical treatment of differential operators; and numerical algorithms for converting continuous equationsinto solvable problems.\u003c/p\u003e","title":"Work"}]